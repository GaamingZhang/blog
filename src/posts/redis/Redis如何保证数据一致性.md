---
date: 2026-01-06
author: Gaaming Zhang
isOriginal: false
article: true
category:
  - Redis
tag:
  - Redis
---

# Redis 的数据一致性：从主从复制到集群

## 一致性的挑战

Redis 作为内存数据库，面临独特的一致性挑战：

**问题1：内存易失**
- 数据存在内存，断电即失
- 需要持久化机制（RDB/AOF）

**问题2：主从延迟**
- 主节点写入后，从节点需要时间同步
- 读从节点可能读到旧数据

**问题3：故障转移**
- 主节点宕机时，如何选出新主节点？
- 切换过程中如何避免数据丢失？

**问题4：集群分区**
- 数据分散在多个节点
- 网络分区时如何保证一致性？

## 主从复制的核心机制

### 全量复制：首次同步

从节点第一次连接主节点时，需要全量复制：

```
步骤1: 从节点发送 PSYNC ? -1
    (表示首次复制，没有复制偏移量)

步骤2: 主节点执行 bgsave
    fork子进程，生成RDB快照
    主进程继续处理写请求

步骤3: 主节点发送RDB文件
    通过网络传输给从节点
    同时，新的写操作进入"复制缓冲区"

步骤4: 从节点加载RDB
    清空旧数据
    加载RDB文件到内存

步骤5: 主节点发送复制缓冲区
    将RDB生成期间的写操作发送给从节点
    从节点执行这些操作

步骤6: 进入命令传播阶段
    主节点每个写操作都实时发送给从节点
```

**全量复制的代价**：
- 主节点需要fork子进程（可能阻塞）
- 生成和传输RDB消耗CPU、内存、网络
- 从节点加载RDB时无法提供服务

### 部分复制：断线重连优化

如果主从连接短暂中断，不需要全量复制：

```
主节点维护：
    复制积压缓冲区（Replication Backlog）
    ┌─────────────────────────────┐
    │ offset 1000: SET key1 val1  │
    │ offset 1020: SET key2 val2  │
    │ offset 1040: DEL key3       │
    │ ...                         │
    │ offset 2000: SET key9 val9  │  ← write_pos
    └─────────────────────────────┘
    默认大小：1MB（循环覆盖）

从节点断线重连：
    发送: PSYNC <master_run_id> 1040
    (我的复制偏移量是1040，从这里继续)

主节点判断：
    IF 1040 在复制积压缓冲区内:
        发送1040之后的所有操作
        → 部分复制，快速恢复
    ELSE:
        缓冲区已被覆盖
        → 回退到全量复制
```

**关键参数**：`repl-backlog-size`
- 太小：频繁全量复制
- 太大：占用过多内存
- 建议：根据网络中断时长和写入速率设置（如10MB）

### 主从复制的延迟

**异步复制的本质**：
```
时刻    主节点                      从节点
T1     接收写请求: SET x 1
T2     写入内存，返回OK
       (客户端收到成功响应)
T3     将命令发送给从节点      →
T4                                接收命令
T5                                执行: SET x 1

延迟窗口：T2-T5
    主节点已返回成功
    但从节点还没同步
    此时读从节点会读到旧值
```

**延迟的影响**：
- 从节点可能返回过期数据
- 主节点宕机可能丢失未同步的数据

**min-slaves-to-write 机制**：
```
配置: min-slaves-to-write 2
      min-slaves-max-lag 10

含义：
    只有当至少2个从节点的同步延迟<10秒时
    主节点才接受写请求
    否则拒绝写入，返回错误

权衡：
    ✓ 减少数据丢失风险
    ✗ 降低可用性（从节点不足时拒写）
```

## 哨兵：自动故障转移

### 哨兵的工作机制

哨兵（Sentinel）是独立的监控进程，负责：
- 监控主从节点健康状态
- 检测主节点故障
- 自动选举新主节点
- 通知客户端新主节点地址

```
典型部署：
    ┌──────────┐     ┌──────────┐     ┌──────────┐
    │Sentinel 1│     │Sentinel 2│     │Sentinel 3│
    └────┬─────┘     └────┬─────┘     └────┬─────┘
         └────────────────┼────────────────┘
                          │ 监控
         ┌────────────────┼────────────────┐
         │                │                │
    ┌────▼────┐     ┌────▼────┐     ┌────▼────┐
    │ Master  │────>│ Slave 1 │     │ Slave 2 │
    └─────────┘     └─────────┘     └─────────┘
```

### 故障检测：主观下线 vs 客观下线

**主观下线（SDOWN）**：
```
单个Sentinel的判断：
    每秒发送PING给主节点
    如果超过down-after-milliseconds没收到PONG
    → 该Sentinel认为主节点"主观下线"
```

**客观下线（ODOWN）**：
```
多数Sentinel达成共识：
    Sentinel 1发现主观下线后
    询问其他Sentinel：你们觉得主节点是否下线？

    IF 超过quorum个Sentinel认为下线:
        → 认定为"客观下线"
        → 触发故障转移

quorum配置示例：
    3个Sentinel，quorum=2
    → 至少2个Sentinel同意才认定故障
```

### 故障转移流程

```
步骤1: 选举领导Sentinel
    多个Sentinel通过Raft算法选举
    选出一个领导Sentinel执行故障转移

步骤2: 选择新主节点
    领导Sentinel从所有从节点中选择：
    过滤条件：
        - 排除已下线的从节点
        - 排除5秒内没响应INFO的从节点
        - 排除与旧主节点断连超过10*down-after的从节点

    选择优先级：
        1. slave-priority最高的（配置的优先级）
        2. 复制偏移量最大的（数据最新）
        3. run_id最小的（选择启动最早的）

步骤3: 提升新主节点
    向选中的从节点发送: SLAVEOF NO ONE
    → 该从节点变为主节点

步骤4: 更新其他从节点
    向其他从节点发送: SLAVEOF <新主节点IP> <端口>
    → 它们开始从新主节点复制

步骤5: 通知客户端
    Sentinel向订阅者发布新主节点信息
    客户端更新连接到新主节点
```

### 数据丢失的窗口

**场景1：异步复制导致的数据丢失**
```
T1: 主节点接收写请求，写入内存
T2: 主节点返回客户端"写入成功"
T3: 主节点宕机（数据还没同步到从节点）
T4: 哨兵选举新主节点（没有这条数据）
→ 数据丢失
```

**场景2：网络分区导致的数据丢失**
```
网络分区：
    [旧主节点] ← 客户端仍在写入
         |
    网络分区（孤立）
         |
    [新主节点 + 从节点 + 哨兵集群]

过程：
    1. 旧主节点在分区一侧，仍接受写请求
    2. 哨兵在分区另一侧，检测不到旧主，选举新主
    3. 分区恢复后，旧主变为从节点
    4. 旧主节点清空数据，从新主同步
    → 分区期间写入旧主的数据全部丢失
```

**缓解措施**：
```
配置：
    min-replicas-to-write 1
    min-replicas-max-lag 10

效果：
    旧主节点如果检测到没有从节点同步
    → 停止接受写请求
    → 减少数据丢失

限制：
    无法完全避免（网络分区前已写入的数据仍会丢失）
```

## 集群模式的一致性

### 数据分片：槽（Slot）机制

Redis Cluster 将数据分散在多个节点：

```
16384个槽（Slot）：
    ┌─────────────────────────────────┐
    │ Slot 0-5460    → Node A (Master)│
    │ Slot 5461-10922 → Node B (Master)│
    │ Slot 10923-16383 → Node C (Master)│
    └─────────────────────────────────┘

Key的分配：
    Slot = CRC16(key) % 16384
    例如：CRC16("user:1001") % 16384 = 3210
    → 存储在Node A

客户端的路由：
    1. 客户端计算key属于哪个slot
    2. 直接连接对应的节点
    3. 如果节点错误，返回MOVED重定向
```

### 集群的主从复制

```
每个Master节点可以有多个Slave：
    Node A (Master) ──> Node A1 (Slave)
                    └─> Node A2 (Slave)

    Node B (Master) ──> Node B1 (Slave)

    Node C (Master) ──> Node C1 (Slave)

故障转移：
    Node A宕机 → Node A1自动提升为Master
    原本的slot 0-5460由Node A1接管
```

### 集群的一致性问题

**问题1：网络分区**
```
分区场景：
    [Node A + Node B] ← 少数派
          |
    网络分区
          |
    [Node C + Node A1 + Node B1 + ...] ← 多数派

结果：
    多数派正常运行
    少数派检测到无法到达多数节点
    → 停止接受写请求（cluster-require-full-coverage=yes）

数据丢失：
    分区前写入Node A的数据，如果还没复制到Node A1
    → 分区恢复后，Node A变为Slave，数据丢失
```

**问题2：写入确认时机**
```
集群的异步复制：
    Client → Master → 返回OK
                  ↓
                Slave (异步)

Master返回OK时，Slave可能还没同步
→ Master宕机会丢失数据
```

**WAIT命令：同步复制**
```
WAIT numreplicas timeout
    等待至少numreplicas个从节点同步

例如：
    SET key value
    WAIT 1 1000
    → 等待至少1个从节点同步完成，超时1秒

效果：
    降低数据丢失概率
代价：
    增加写入延迟，降低性能
```

## 持久化与一致性

### RDB vs AOF

**RDB（快照）**：
```
优点：
    - 全量备份，恢复快
    - 文件紧凑，占用空间小
缺点：
    - 可能丢失最近一次快照后的所有数据
    - bgsave时fork可能阻塞

适用场景：
    能接受分钟级数据丢失的场景
```

**AOF（追加日志）**：
```
记录每个写操作到日志文件

三种同步策略：
    appendfsync always
        → 每个写操作都fsync，最安全，最慢

    appendfsync everysec (默认)
        → 每秒fsync一次，最多丢1秒数据

    appendfsync no
        → 由操作系统决定，性能最好，可能丢失较多数据

优点：
    - 数据丢失少
    - 支持后台重写（AOF Rewrite）压缩文件
缺点：
    - 文件较大
    - 恢复比RDB慢
```

### 持久化的一致性保证

**RDB的问题**：
```
T1: 上次bgsave完成
T2-T10: 大量写操作（未持久化）
T11: Redis崩溃
→ T2-T10的数据全部丢失
```

**AOF的问题**：
```
appendfsync everysec:
    T1: 写操作写入AOF缓冲区
    T2: Redis崩溃（还没fsync）
    → 最多丢失1秒数据

appendfsync always:
    每次写操作都fsync
    → 不丢数据，但性能极差
```

**混合持久化（Redis 4.0+）**：
```
RDB+AOF混合：
    AOF重写时：
        1. 生成RDB格式的基础数据
        2. 重写期间的增量写操作追加为AOF格式

    优点：
        - 恢复速度快（RDB部分）
        - 数据丢失少（AOF部分）
```

## CAP 权衡

Redis 在 CAP 定理中的选择：

**单机模式**：
- CA：强一致性 + 高可用（但无分区容错）

**主从 + 哨兵**：
- AP：可用性 + 分区容错
- 异步复制，无法保证强一致性
- 分区时，多数派继续服务

**集群模式**：
- AP：可用性 + 分区容错
- 异步复制，默认不保证强一致性
- 可以用WAIT命令牺牲性能换取一定的一致性

**Redis 不适合的场景**：
- 需要强一致性的金融交易
- 需要严格ACID保证的场景

**Redis 适合的场景**：
- 可接受最终一致性的缓存
- 可容忍少量数据丢失的会话存储
- 高性能要求 > 强一致性要求的场景

## 小结

Redis 数据一致性的核心机制：

**主从复制**：
- 全量复制（首次）+ 部分复制（断线重连）
- 复制积压缓冲区支持部分复制
- 异步复制，有延迟窗口

**哨兵故障转移**：
- 主观下线 + 客观下线的共识机制
- 自动选举新主节点
- 无法完全避免数据丢失

**集群分片**：
- 16384个槽分散数据
- 每个Master可有多个Slave
- 网络分区时牺牲一致性保证可用性

**持久化**：
- RDB快照：快但可能丢失较多数据
- AOF日志：慢但数据丢失少
- 混合持久化：兼顾速度和安全

**一致性权衡**：
- Redis 默认选择 AP（可用性+分区容错）
- 牺牲强一致性换取高性能
- 通过WAIT、min-replicas等机制缓解（但无法完全保证）

理解这些机制，才能在实际应用中合理使用 Redis，避免因误解一致性保证而导致数据问题。
