---
date: 2026-01-14
author: Gaaming Zhang
isOriginal: false
article: true
category:
  - Zookeeper
tag:
  - Zookeeper
---

# Zookeeper基本概念

ZooKeeper是一个开源的分布式应用程序协调服务,最初由雅虎研究院开发,现已成为Apache顶级项目。它是Google Chubby的开源实现,为分布式应用提供高效可靠的协调服务。

## 设计目标

Zookeeper的核心设计目标是将复杂且容易出错的分布式一致性服务封装起来,构建一个高效可靠的原语集,并提供一系列简单易用的接口供用户使用。它旨在解决分布式系统中的协调问题,使开发人员能够专注于业务逻辑而非底层的分布式协调机制。

## 主要特性

- **顺序一致性**:从同一客户端发起的事务请求将严格按照发起顺序被应用到Zookeeper中
- **原子性**:所有事务请求的处理结果在整个集群中所有机器上的应用情况保持一致
- **单一视图**:无论客户端连接到哪个Zookeeper服务器,看到的服务端数据模型都是一致的
- **可靠性**:一旦服务端成功应用了一个事务并完成客户端响应,该事务引起的状态变更将会被永久保留
- **实时性**:Zookeeper保证在一定时间段内,客户端最终能够从服务端读取到最新的数据状态

## 核心概念

### 1. 数据模型 (Data Model)

Zookeeper维护一个类似文件系统的树形层级命名空间,这种数据结构被称为**ZNode树**。

#### ZNode(数据节点)

ZNode是Zookeeper数据模型中的最小数据单元,每个ZNode都可以存储数据,同时还可以挂载子节点。

**ZNode的路径标识**:
- 使用斜杠(/)分隔的路径来标识,如`/app/config`
- 每个ZNode都有唯一的路径标识
- 路径必须是绝对路径,以`/`开头

**ZNode的类型**:

1. **持久节点(PERSISTENT)**
   - 创建后会一直存在于Zookeeper服务器上
   - 除非主动执行删除操作,否则节点不会消失
   - 适用于存储需要持久化的配置信息

2. **临时节点(EPHEMERAL)**
   - 生命周期与客户端会话绑定
   - 当创建该节点的客户端会话失效时,节点自动删除
   - 不能拥有子节点
   - 适用于服务注册、分布式锁等场景

3. **持久顺序节点(PERSISTENT_SEQUENTIAL)**
   - 具有持久节点的特性
   - 创建时ZooKeeper会自动在节点名称后追加一个单调递增的数字后缀
   - 例如创建`/app`,实际生成`/app0000000001`

4. **临时顺序节点(EPHEMERAL_SEQUENTIAL)**
   - 具有临时节点的特性
   - 创建时自动追加递增数字后缀
   - 适用于分布式队列、选举等场景

**ZNode的数据结构**:

每个ZNode除了存储数据内容外,还包含一系列状态信息(Stat对象):

```
cZxid: 创建节点时的事务ID
ctime: 节点创建时间
mZxid: 最后一次更新节点时的事务ID
mtime: 最后一次更新时间
pZxid: 子节点列表最后一次被修改时的事务ID
cversion: 子节点版本号
dataVersion: 数据版本号
aclVersion: ACL版本号
ephemeralOwner: 临时节点所属会话ID,持久节点为0
dataLength: 数据长度
numChildren: 子节点数量
```

### 2. 会话 (Session)

Session是Zookeeper中客户端与服务器之间的TCP长连接。

**会话的生命周期**:
1. 客户端启动,发起与服务器的TCP连接
2. 连接建立成功,会话生命周期开始
3. 客户端通过心跳机制与服务器保持会话有效
4. 会话超时或客户端主动关闭,会话结束

**SessionTimeout**:
- 会话超时时间,用于设置客户端会话的超时时长
- 当网络故障、服务器压力等原因导致连接断开时,只要在SessionTimeout内重新连接到集群中任意服务器,之前的会话仍然有效
- 超过SessionTimeout未能重连,会话失效,该会话创建的所有临时节点将被删除

**会话状态**:
- CONNECTING: 正在连接
- CONNECTED: 已连接
- RECONNECTING: 正在重连
- RECONNECTED: 已重连
- CLOSE: 会话关闭

### 3. 版本 (Version)

Zookeeper为每个ZNode维护三种版本号,实现乐观锁机制:

- **dataVersion**: 数据内容版本号
- **cversion**: 子节点版本号
- **aclVersion**: ACL版本号

每次数据更新时,对应的版本号会递增。客户端可以通过版本号实现条件更新,避免并发冲突。

### 4. 事务ID (ZXID)

ZXID(ZooKeeper Transaction ID)是Zookeeper中的全局唯一事务ID。

**ZXID的结构**:
- 64位数字,高32位为epoch,低32位为计数器
- **epoch**: 标识Leader的任期,每次选举产生新Leader时递增
- **计数器**: 在当前epoch内递增的事务序号

**ZXID的作用**:
- 唯一标识每一个事务操作
- 保证事务的全局顺序性
- 如果zxid1 < zxid2,则zxid1对应的事务一定发生在zxid2之前

### 5. Watcher (监听器)

Watcher是Zookeeper实现分布式协调的核心机制之一,允许客户端在指定节点上注册监听器。

**Watcher的特性**:
- **一次性触发**: Watcher触发后会被自动移除,需要持续监听时必须重新注册
- **异步通知**: 服务端异步向客户端发送事件通知
- **轻量级**: 通知只包含事件类型和路径,不包含数据内容

**Watcher的事件类型**:
- NodeCreated: 节点创建
- NodeDeleted: 节点删除
- NodeDataChanged: 节点数据变更
- NodeChildrenChanged: 子节点列表变更

**典型应用场景**:
- 配置中心: 监听配置节点变化,实现配置动态更新
- 服务发现: 监听服务节点变化,感知服务上下线
- 分布式锁: 监听锁节点变化,实现锁的获取与释放

### 6. ACL (访问控制列表)

ACL (Access Control List)提供了节点级别的权限控制机制。

**权限类型**:
- **CREATE (c)**: 创建子节点的权限
- **READ (r)**: 读取节点数据和子节点列表的权限
- **WRITE (w)**: 更新节点数据的权限
- **DELETE (d)**: 删除子节点的权限
- **ADMIN (a)**: 设置节点ACL的权限

**认证模式**:

1. **world**: 默认方式,所有人都可以访问
   ```
   setAcl /path world:anyone:cdrwa
   ```

2. **auth**: 已认证用户
   ```
   addauth digest username:password
   setAcl /path auth:username:cdrwa
   ```

3. **digest**: 用户名密码方式
   ```
   setAcl /path digest:username:base64(SHA1(password)):cdrwa
   ```

4. **ip**: 基于IP地址的访问控制
   ```
   setAcl /path ip:192.168.1.100:cdrwa
   ```

## 集群架构

### 1. 集群角色

Zookeeper集群采用主从架构,包含三种角色:

#### 1. Leader (领导者)

- **职责**:
  - 处理所有写请求,保证集群写操作的顺序性
  - 事务请求的唯一调度和处理者
  - 集群内部各服务器的调度者
  - 发起和维护与Follower及Observer的心跳连接

- **选举**: 集群启动或Leader崩溃时,通过选举算法产生新Leader

#### 2. Follower (跟随者)

- **职责**:
  - 处理客户端的读请求
  - 将写请求转发给Leader
  - 参与写请求的投票过程(需要半数以上通过)
  - 参与Leader选举投票

- **数据同步**: 与Leader保持数据同步

#### 3. Observer (观察者)

- **职责**:
  - 与Follower类似,处理读请求并转发写请求
  - 同步Leader的状态数据
  - **不参与**任何形式的投票(选举投票和写操作投票)

- **作用**: 扩展系统,提高读性能,不影响集群的写性能

**配置Observer**:
```properties
# 在配置文件中标记为Observer
peerType=observer

# 在server配置中添加:observer后缀
server.3=ip:port:port:observer
```

### 2. 集群配置

**zoo.cfg示例**:
```properties
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/var/lib/zookeeper
clientPort=2181

server.1=192.168.1.101:2888:3888
server.2=192.168.1.102:2888:3888
server.3=192.168.1.103:2888:3888
```

**配置说明**:
- `tickTime`: 基本时间单位(毫秒),用于心跳和超时计算
- `initLimit`: Follower初始化连接到Leader的超时时间(tickTime的倍数)
- `syncLimit`: Follower与Leader同步的超时时间
- `dataDir`: 数据存储目录
- `clientPort`: 客户端连接端口
- `server.X`: 服务器配置,X为myid值,格式为`host:port1:port2`
  - port1: Follower与Leader数据同步端口
  - port2: Leader选举端口

**myid文件**:
- 位于dataDir目录下
- 内容为server.X中的X值
- 每个节点的myid必须唯一

## ZAB协议

ZAB(ZooKeeper Atomic Broadcast)协议是Zookeeper保证分布式数据一致性的核心算法。

### 1. 协议模式

#### 1. 消息广播模式 (正常运行)

当集群中存在过半的Follower与Leader完成状态同步后,进入消息广播模式。

**工作流程**:
1. Leader接收客户端的写请求
2. Leader将写请求转换为Proposal(提议)
3. Leader将Proposal发送给所有Follower
4. Follower接收Proposal并写入本地事务日志,返回ACK
5. Leader收到过半Follower的ACK后,向所有Follower发送Commit消息
6. Follower收到Commit后提交事务
7. Leader响应客户端

**注意**: Observer同步Leader数据,但不参与投票过程。

#### 2. 崩溃恢复模式 (Leader故障)

当Leader崩溃或失去与过半Follower的通信时,进入崩溃恢复模式。

**触发条件**:
- Leader服务器崩溃
- Leader失去与过半Follower的网络连接
- 集群启动时

**恢复过程**:
1. 集群进入LOOKING状态,开始Leader选举
2. 选举出新的Leader
3. 新Leader与Follower进行数据同步
4. 过半机器完成同步后,退出恢复模式,进入消息广播模式

### 2. Leader选举

Leader选举存在两个阶段:服务器启动时选举和运行时重新选举。

#### 选举关键参数

1. **myid (服务器ID)**
   - 配置在myid文件中的唯一标识
   - 编号越大,在选举中权重越高

2. **zxid (事务ID)**
   - 最新事务的ZXID
   - 值越大说明数据越新,权重越高

3. **epoch (逻辑时钟)**
   - 选举轮次
   - 每完成一轮投票,epoch递增

#### 服务器状态

- **LOOKING**: 竞选状态,寻找Leader
- **FOLLOWING**: 跟随状态,同步Leader并参与投票
- **LEADING**: 领导状态,负责协调写操作
- **OBSERVING**: 观察状态,同步Leader但不参与投票

#### 选举算法

Zookeeper使用基于Fast Paxos的选举算法:

**投票信息**: (myid, zxid, epoch)

**选举规则**(优先级从高到低):
1. epoch最大的获胜
2. epoch相同,zxid最大的获胜
3. zxid相同,myid最大的获胜

**选举流程**:
1. 每个服务器初始投票给自己
2. 交换投票信息
3. 根据选举规则更新投票
4. 统计投票,如果某个服务器获得过半投票,选举结束
5. 更新服务器状态

#### 集群机器数量

**为什么建议使用奇数台服务器?**

Zookeeper集群需要过半机器存活才能正常工作:
- 3台集群:允许1台故障,可用性50%
- 4台集群:允许1台故障,可用性50%
- 5台集群:允许2台故障,可用性60%
- 6台集群:允许2台故障,可用性50%

可以看出,偶数台与少一台的奇数台容错能力相同,但增加了成本,因此建议使用奇数台。

**最小集群规模**: 3台(允许1台故障)

## CAP定理与BASE理论

### 1. CAP定理

分布式系统无法同时满足以下三个特性:

- **C (Consistency)**: 一致性 - 所有节点同一时间看到相同的数据
- **A (Availability)**: 可用性 - 每个请求都能得到响应(成功或失败)
- **P (Partition tolerance)**: 分区容错性 - 系统在网络分区时仍能继续工作

**Zookeeper的选择**: CP模型
- 保证一致性和分区容错性
- 当Leader失去与过半Follower的连接时,集群停止对外提供写服务,牺牲可用性
- 对比:Eureka选择AP模型,保证可用性但允许数据不一致

### 2. BASE理论

BASE是对CAP中一致性和可用性权衡的结果:

- **BA (Basically Available)**: 基本可用 - 允许损失部分可用性
- **S (Soft state)**: 软状态 - 允许系统存在中间状态
- **E (Eventually Consistent)**: 最终一致性 - 系统最终会达到一致状态

Zookeeper通过ZAB协议实现最终一致性,保证在网络分区恢复后数据能够达到一致。

## 应用场景

### 1. 配置管理

**原理**: 将配置信息存储在ZNode中,应用监听配置节点变化。

**实现方式**:
- 配置信息存储在持久节点
- 应用启动时读取配置并注册Watcher
- 配置变更时,Watcher触发,应用重新加载配置

**优势**: 配置集中管理,动态更新,避免重启服务

### 2. 命名服务

**原理**: 利用ZNode的唯一路径特性提供全局唯一名称。

**应用**:
- 分布式ID生成:使用顺序节点生成唯一ID
- 服务命名:为服务提供全局唯一的名称标识

### 3. 分布式锁

**原理**: 利用ZNode的唯一性和顺序性实现锁机制。

**实现方式**:
- 创建临时顺序节点表示加锁请求
- 序号最小的节点获得锁
- 其他节点监听前一个节点,等待锁释放
- 持锁节点完成操作后删除节点,释放锁

**优势**: 自动释放(会话失效时临时节点删除),公平性(按序获取锁)

### 4. 集群管理与服务注册

**原理**: 利用临时节点和Watcher机制实现服务的注册与发现。

**服务注册**:
- 服务启动时创建临时节点,节点路径包含服务地址
- 服务下线时,会话失效,临时节点自动删除

**服务发现**:
- 客户端监听服务节点的子节点列表
- 节点变化时,客户端更新可用服务列表

**集群管理**:
- 监控集群节点状态
- 节点上下线自动通知
- 实现集群的动态扩缩容

### 5. Master选举

**原理**: 利用ZNode的唯一性实现主节点选举。

**实现方式**:
- 所有候选节点尝试创建同一个临时节点
- 创建成功的节点成为Master
- 其他节点监听该节点,等待Master下线
- Master下线后,节点删除,其他节点重新竞争

### 6. 分布式队列

**原理**: 利用顺序节点实现FIFO队列。

**实现方式**:
- 生产者创建持久顺序节点,添加数据到队列
- 消费者获取子节点列表,按序号处理
- 处理完成后删除对应节点

### 7. 负载均衡

**原理**: 通过服务注册与发现,实现动态负载均衡。

**实现方式**:
- 服务提供者注册自身信息到ZNode
- 服务消费者获取所有可用服务列表
- 消费者根据负载均衡算法选择服务
- 监听服务节点变化,动态更新服务列表

## 数据存储

### 1. 内存存储

Zookeeper将全量数据存储在内存中,以提高吞吐量和降低延迟。

**数据结构**: 
- 基于ConcurrentHashMap的数据树
- 每个ZNode在内存中维护完整的状态信息

### 2. 持久化

为保证数据可靠性,Zookeeper采用两种持久化方式:

#### 1. 事务日志 (Transaction Log)

- 记录每一个写操作的事务
- 顺序写入,保证高性能
- 用于崩溃恢复时的数据恢复

#### 2. 快照 (Snapshot)

- 定期将内存数据序列化到磁盘
- 加快恢复速度,避免重放全部事务日志
- 采用异步线程生成,不阻塞正常服务

**数据恢复流程**:
1. 加载最新的快照文件
2. 重放快照之后的事务日志
3. 恢复内存数据树

## 常见问题

### 1. Zookeeper集群为什么建议使用奇数台服务器?

Zookeeper集群遵循过半存活原则,需要超过一半的服务器正常运行才能提供服务。奇数台和比它多一台的偶数台容错能力相同:

- 3台:最多容忍1台故障,可用性50%
- 4台:最多容忍1台故障,可用性50%
- 5台:最多容忍2台故障,可用性60%
- 6台:最多容忍2台故障,可用性50%

使用偶数台会增加硬件成本,但不能提升容错能力,因此建议使用奇数台。一般生产环境使用3台或5台即可满足高可用需求。

### 2. Zookeeper的Watcher机制为什么是一次性的?

Watcher设计为一次性触发主要基于以下考虑:

**性能考虑**: 如果Watcher持久存在,服务端需要维护大量的监听关系,当节点频繁变化时会产生大量通知,造成网络风暴和性能问题。

**简单性**: 一次性Watcher使得监听机制更加简单,客户端可以在收到通知后根据实际需要决定是否重新注册。

**可靠性**: 强制客户端重新注册可以避免监听"丢失",确保客户端明确知道当前的监听状态。

实际使用中,客户端可以在Watcher回调中重新注册Watcher,实现持续监听的效果。

### 3. Zookeeper如何保证数据一致性?

Zookeeper通过ZAB协议保证数据一致性:

**写操作一致性**: 
- 所有写操作都由Leader处理
- Leader将写请求转换为Proposal,发送给所有Follower
- 必须获得过半Follower的ACK才能提交事务
- 使用ZXID保证事务的全局顺序性

**读操作一致性**:
- 默认读操作可能读到非最新数据(最终一致性)
- 可以使用sync命令强制同步后再读取,保证读到最新数据

**崩溃恢复**:
- 选举产生新Leader时,选择拥有最大ZXID的节点
- 新Leader与Follower进行数据同步,确保所有节点数据一致

### 4. Zookeeper适合存储什么类型的数据?

Zookeeper**不适合**作为海量数据存储系统,适合存储的数据类型:

**适合存储**:
- 配置信息:应用配置、系统参数
- 元数据:集群状态、服务注册信息
- 协调数据:分布式锁、选举信息
- 小规模数据:通常单个ZNode数据不超过1MB

**不适合存储**:
- 大文件:Zookeeper限制单个ZNode最大1MB
- 海量数据:数据全部存储在内存中,受内存限制
- 频繁变化的数据:写操作需要过半确认,性能相对较低
- 业务数据:应该使用专业的数据库系统

### 5. Zookeeper与Eureka作为注册中心有什么区别?

两者在CAP定理上的选择不同,导致特性差异:

**Zookeeper (CP模型)**:
- 保证强一致性,牺牲部分可用性
- Leader故障时集群暂时不可用,直到选举出新Leader
- 适合对数据一致性要求高的场景
- 网络分区时可能导致服务不可用

**Eureka (AP模型)**:
- 保证高可用性,接受最终一致性
- 每个节点都可以提供服务,不存在单点故障
- 网络分区时仍可继续服务,但可能数据不一致
- 适合对可用性要求高,可以容忍数据短暂不一致的场景

**选择建议**:
- 金融、交易等对一致性要求极高的系统:选择Zookeeper
- 互联网应用、微服务架构等对可用性要求高的系统:选择Eureka
- Spring Cloud生态推荐使用Eureka或Consul