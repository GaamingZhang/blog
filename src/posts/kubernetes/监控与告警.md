---
date: 2026-02-13
author: Gaaming Zhang
isOriginal: false
article: true
category:
  - Kubernetes
tag:
  - Kubernetes
  - ClaudeCode
---

# Kubernetes监控与告警：从指标采集到SLO实践

监控是运维的基石，告警是故障响应的起点。在Kubernetes环境中，监控需要覆盖节点、容器、应用多个层次，告警需要精准、可操作、避免告警疲劳。本文将深入探讨Kubernetes监控告警的架构设计、关键指标、告警策略和SLO实践。

---

## 监控架构设计

### 多层次监控模型

```
┌─────────────────────────────────────────────────────────────┐
│                    Kubernetes监控层次                        │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  应用层                                                      │
│  ├── 业务指标：订单量、用户活跃数                             │
│  ├── 应用指标：QPS、延迟、错误率                              │
│  └── 自定义指标：队列深度、缓存命中率                          │
│                                                             │
│  Kubernetes层                                                │
│  ├── 工作负载：Pod状态、Deployment副本数                      │
│  ├── 资源对象：ConfigMap、Secret、PVC                         │
│  └── 控制器：HPA、VPA、调度器状态                              │
│                                                             │
│  容器层                                                      │
│  ├── 资源使用：CPU、内存、网络IO                              │
│  ├── 运行状态：重启次数、OOM事件                              │
│  └── 镜像信息：镜像版本、拉取状态                              │
│                                                             │
│  节点层                                                      │
│  ├── 硬件资源：CPU、内存、磁盘、网络                          │
│  ├── 系统状态：负载、进程数、文件句柄                          │
│  └── 组件状态：kubelet、kube-proxy、容器运行时                 │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Prometheus架构

```
┌─────────────────────────────────────────────────────────────┐
│                    Prometheus监控架构                        │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  数据采集层                                                  │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐         │
│  │Node Exporter│  │kube-state-  │  │ App Metrics │         │
│  │  (节点指标)  │  │ metrics     │  │  (应用指标)  │         │
│  └──────┬──────┘  │(K8s对象状态)│  └──────┬──────┘         │
│          │         └──────┬──────┘         │                │
│          │                │                │                │
│          ▼                ▼                ▼                │
│  ┌─────────────────────────────────────────────────────┐   │
│  │              Prometheus Server                       │   │
│  │  ┌──────────┐ ┌──────────┐ ┌──────────┐            │   │
│  │  │Retrieval │ │  TSDB    │ │HTTP Server│            │   │
│  │  │(采集)    │ │ (存储)   │ │ (查询)    │            │   │
│  │  └──────────┘ └──────────┘ └──────────┘            │   │
│  └─────────────────────────────────────────────────────┘   │
│          │                                                  │
│          ▼                                                  │
│  ┌─────────────────────────────────────────────────────┐   │
│  │              Alertmanager                            │   │
│  │  ┌──────────┐ ┌──────────┐ ┌──────────┐            │   │
│  │  │  分组    │ │  抑制    │ │  静默    │            │   │
│  │  └──────────┘ └──────────┘ └──────────┘            │   │
│  └─────────────────────────────────────────────────────┘   │
│          │                                                  │
│          ▼                                                  │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐         │
│  │   Grafana   │  │   PagerDuty │  │    Slack    │         │
│  │  (可视化)   │  │  (告警路由) │  │  (通知)     │         │
│  └─────────────┘  └─────────────┘  └─────────────┘         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 服务发现机制

Prometheus在Kubernetes中通过服务发现自动发现监控目标：

```yaml
# Prometheus配置示例
scrape_configs:
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      # 只采集带有prometheus.io/scrape=true注解的Pod
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      # 从注解中读取指标端口
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: (.+)
        replacement: ${1}:9090
```

**ServiceMonitor方式（推荐）**：

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: my-app-monitor
  labels:
    release: prometheus
spec:
  selector:
    matchLabels:
      app: my-app
  namespaceSelector:
    any: true
  endpoints:
    - port: metrics
      path: /metrics
      interval: 30s
      scrapeTimeout: 10s
```

---

## 关键指标体系

### 节点层指标

| 指标名称 | 说明 | 告警阈值 |
| -------- | ---- | -------- |
| node_cpu_seconds_total | CPU使用时间 | 使用率>85%持续5分钟 |
| node_memory_MemAvailable_bytes | 可用内存 | <10%总量 |
| node_filesystem_avail_bytes | 磁盘可用空间 | <15%总量 |
| node_load1 | 1分钟负载 | >CPU核数 |
| node_network_receive_bytes_total | 网络接收 | 带宽使用>80% |

**节点健康检查规则**：

```yaml
groups:
- name: node-alerts
  rules:
  - alert: NodeHighCPU
    expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "节点CPU使用率过高"
      description: "节点 {{ $labels.instance }} CPU使用率 {{ $value | printf \"%.1f\" }}%"

  - alert: NodeMemoryPressure
    expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "节点内存压力"
      description: "节点 {{ $labels.instance }} 内存使用率 {{ $value | printf \"%.1f\" }}%"

  - alert: NodeDiskPressure
    expr: (1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|nsfs"} / node_filesystem_size_bytes{fstype!~"tmpfs|nsfs"})) * 100 > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "节点磁盘压力"
      description: "节点 {{ $labels.instance }} 磁盘 {{ $labels.mountpoint }} 使用率 {{ $value | printf \"%.1f\" }}%"
```

### Kubernetes层指标

**kube-state-metrics核心指标**：

| 指标名称 | 说明 | 用途 |
| -------- | ---- | ---- |
| kube_pod_status_phase | Pod状态 | 监控Pod生命周期 |
| kube_pod_container_status_restarts_total | 容器重启次数 | 检测CrashLoopBackOff |
| kube_deployment_status_replicas_unavailable | 不可用副本数 | 检测Deployment异常 |
| kube_node_status_condition | 节点条件状态 | 检测节点NotReady |
| kube_persistentvolumeclaim_status_phase | PVC状态 | 检测存储问题 |

**工作负载监控规则**：

```yaml
groups:
- name: kubernetes-alerts
  rules:
  - alert: PodCrashLooping
    expr: rate(kube_pod_container_status_restarts_total[15m]) * 60 * 15 > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Pod频繁重启"
      description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} 在过去15分钟内重启 {{ $value | printf \"%.0f\" }} 次"

  - alert: DeploymentReplicasUnavailable
    expr: kube_deployment_status_replicas_unavailable / kube_deployment_spec_replicas > 0.1
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Deployment副本不可用"
      description: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} 有 {{ $value | printf \"%.0f\" }}% 副本不可用"

  - alert: NodeNotReady
    expr: kube_node_status_condition{condition="Ready",status="true"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "节点NotReady"
      description: "节点 {{ $labels.node }} 处于NotReady状态超过5分钟"
```

### 应用层指标

**RED方法（Rate-Errors-Duration）**：

```
┌─────────────────────────────────────────────────────────────┐
│                    应用监控黄金指标                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Rate（请求速率）                                            │
│  ├── QPS：每秒请求数                                         │
│  ├── 按方法分组：GET/POST/PUT/DELETE                         │
│  └── 按端点分组：/api/users, /api/orders                     │
│                                                             │
│  Errors（错误率）                                            │
│  ├── 错误请求比例：4xx/5xx / 总请求                          │
│  ├── 错误绝对值：每秒错误数                                   │
│  └── 按错误码分组：400, 404, 500, 503                        │
│                                                             │
│  Duration（延迟）                                            │
│  ├── 平均延迟：P50                                           │
│  ├── 尾部延迟：P95, P99                                      │
│  └── 最大延迟：P100                                          │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

**应用指标暴露示例（Go）**：

```go
import (
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promhttp"
)

var (
    httpRequestsTotal = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "http_requests_total",
            Help: "Total number of HTTP requests",
        },
        []string{"method", "path", "status"},
    )

    httpRequestDuration = prometheus.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "http_request_duration_seconds",
            Help:    "HTTP request duration in seconds",
            Buckets: []float64{.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10},
        },
        []string{"method", "path"},
    )
)

func init() {
    prometheus.MustRegister(httpRequestsTotal)
    prometheus.MustRegister(httpRequestDuration)
}

// 在HTTP handler中记录指标
func metricsMiddleware(next http.Handler) http.Handler {
    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        start := time.Now()
        
        rw := &responseWriter{ResponseWriter: w}
        next.ServeHTTP(rw, r)
        
        duration := time.Since(start).Seconds()
        httpRequestsTotal.WithLabelValues(r.Method, r.URL.Path, strconv.Itoa(rw.status)).Inc()
        httpRequestDuration.WithLabelValues(r.Method, r.URL.Path).Observe(duration)
    })
}
```

---

## PromQL进阶

### 常用查询模式

**计算速率**：

```promql
# 每秒请求数（QPS）
rate(http_requests_total[5m])

# 每秒错误数
rate(http_requests_total{status=~"5.."}[5m])

# 错误率
sum(rate(http_requests_total{status=~"5.."}[5m])) 
/ sum(rate(http_requests_total[5m]))
```

**计算百分位数**：

```promql
# P99延迟
histogram_quantile(0.99, 
  rate(http_request_duration_seconds_bucket[5m])
)

# 按路径分组的P99延迟
histogram_quantile(0.99, 
  sum by (path, le) (rate(http_request_duration_seconds_bucket[5m]))
)
```

**资源使用率计算**：

```promql
# CPU使用率（容器）
sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (pod)
/ sum(kube_pod_container_resource_limits{resource="cpu"}) by (pod) * 100

# 内存使用率（容器）
sum(container_memory_working_set_bytes{container!=""}) by (pod)
/ sum(kube_pod_container_resource_limits{resource="memory"}) by (pod) * 100
```

### 高级聚合技巧

**Top N查询**：

```promql
# CPU使用率最高的10个Pod
topk(10, 
  sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (pod)
)

# 内存使用量最大的5个容器
bottomk(5, 
  sum(container_memory_working_set_bytes{container!=""}) by (container)
)
```

**时间偏移对比**：

```promql
# 与一周前对比的QPS变化
sum(rate(http_requests_total[5m])) 
/ sum(rate(http_requests_total[5m] offset 1w))

# 与昨天同时段对比的延迟变化
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))
- histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m] offset 1d))
```

**多指标关联**：

```promql
# 高CPU且高内存的Pod
sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (pod) > 0.8
and on(pod)
sum(container_memory_working_set_bytes{container!=""}) by (pod) > 1073741824
```

---

## 探针机制深度实践

### 三种探针类型

```
┌─────────────────────────────────────────────────────────────┐
│                    Kubernetes探针机制                        │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  livenessProbe（存活探针）                                   │
│  ├── 目的：检测容器是否存活                                   │
│  ├── 失败动作：重启容器                                       │
│  └── 场景：死锁、进程卡死                                     │
│                                                             │
│  readinessProbe（就绪探针）                                   │
│  ├── 目的：检测容器是否准备好接收流量                          │
│  ├── 失败动作：从Service端点移除                               │
│  └── 场景：应用启动中、依赖不可用                              │
│                                                             │
│  startupProbe（启动探针）                                     │
│  ├── 目的：检测应用是否启动完成                                │
│  ├── 失败动作：重启容器                                       │
│  └── 场景：慢启动应用                                         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### HTTP探针配置

```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
    httpHeaders:
    - name: X-Custom-Header
      value: health-check
  initialDelaySeconds: 30    # 容器启动后等待时间
  periodSeconds: 10          # 检查间隔
  timeoutSeconds: 5          # 超时时间
  failureThreshold: 3        # 连续失败次数
  successThreshold: 1        # 连续成功次数

readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3
  successThreshold: 1
```

### gRPC探针（Kubernetes 1.24+）

```yaml
livenessProbe:
  grpc:
    port: 50051
    service: liveness         # gRPC健康检查服务名
  initialDelaySeconds: 10
  periodSeconds: 10

readinessProbe:
  grpc:
    port: 50051
    service: readiness
  initialDelaySeconds: 5
  periodSeconds: 5
```

**gRPC健康检查服务实现**：

```go
import (
    "google.golang.org/grpc/health"
    "google.golang.org/grpc/health/grpc_health_v1"
)

func setupHealthCheck(s *grpc.Server) {
    healthServer := health.NewServer()
    
    // 设置服务健康状态
    healthServer.SetServingStatus("liveness", grpc_health_v1.HealthCheckResponse_SERVING)
    healthServer.SetServingStatus("readiness", grpc_health_v1.HealthCheckResponse_SERVING)
    
    grpc_health_v1.RegisterHealthServer(s, healthServer)
}

// 动态更新健康状态
func (s *Server) SetUnhealthy() {
    s.healthServer.SetServingStatus("readiness", grpc_health_v1.HealthCheckResponse_NOT_SERVING)
}
```

### 探针失败指标监控

```yaml
groups:
- name: probe-alerts
  rules:
  - alert: HighProbeFailureRate
    expr: |
      sum(rate(probe_success_total[5m])) by (service)
      / sum(rate(probe_attempts_total[5m])) by (service) < 0.95
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "探针失败率过高"
      description: "服务 {{ $labels.service }} 探针成功率 {{ $value | printf \"%.2f\" }}"

  - alert: ProbeLatencyHigh
    expr: histogram_quantile(0.99, rate(probe_duration_seconds_bucket[5m])) > 1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "探针延迟过高"
      description: "P99探针延迟 {{ $value | printf \"%.2f\" }}s"
```

---

## AlertManager配置实战

### 路由配置

```yaml
global:
  resolve_timeout: 5m
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alertmanager@example.com'

route:
  group_by: ['alertname', 'namespace']
  group_wait: 30s          # 等待同组告警聚合
  group_interval: 5m       # 同组新告警发送间隔
  repeat_interval: 4h      # 重复告警发送间隔
  receiver: 'default-receiver'
  routes:
    # 关键告警：立即通知
    - match:
        severity: critical
      receiver: 'critical-receiver'
      group_wait: 10s
      repeat_interval: 1h
      
    # 警告级别：聚合后通知
    - match:
        severity: warning
      receiver: 'warning-receiver'
      group_wait: 5m
      repeat_interval: 12h
      
    # 特定命名空间路由
    - match:
        namespace: production
      receiver: 'prod-receiver'
      continue: true  # 继续匹配其他规则

receivers:
- name: 'default-receiver'
  slack_configs:
  - channel: '#alerts'
    send_resolved: true

- name: 'critical-receiver'
  pagerduty_configs:
  - service_key: '<pagerduty-key>'
    severity: critical
  slack_configs:
  - channel: '#critical-alerts'
    send_resolved: true

- name: 'warning-receiver'
  email_configs:
  - to: 'ops@example.com'
    send_resolved: true
```

### 抑制规则

```yaml
inhibit_rules:
  # 节点NotReady时，抑制该节点上所有Pod告警
  - source_match:
      alertname: 'NodeNotReady'
    target_match_re:
      alertname: 'PodNotReady|PodCrashLooping'
    equal: ['node']
    
  # 集群不可用时，抑制所有应用告警
  - source_match:
      alertname: 'KubernetesClusterDown'
    target_match_re:
      alertname: '.*'
      
  # 数据库不可用时，抑制相关应用告警
  - source_match:
      alertname: 'DatabaseDown'
    target_match:
      namespace: 'production'
    equal: ['namespace']
```

### 静默规则

```bash
# 创建静默（计划维护期间）
amtool silence add \
  --alertname="NodeHighCPU" \
  --instance="node-1" \
  --duration="2h" \
  --comment="计划维护：CPU扩容"

# 查看活跃静默
amtool silence query

# 删除静默
amtool silence expire <silence-id>
```

---

## SLO告警实践

### SLO概念

```
┌─────────────────────────────────────────────────────────────┐
│                    SLO体系架构                               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  SLI（服务等级指标）                                          │
│  ├── 可用性：成功请求 / 总请求                                │
│  ├── 延迟：请求响应时间                                       │
│  └── 吞吐量：每秒处理请求数                                   │
│                                                             │
│  SLO（服务等级目标）                                          │
│  ├── 可用性SLO：99.9%                                        │
│  ├── 延迟SLO：P99 < 200ms                                    │
│  └── 错误预算：1 - SLO = 0.1%                                │
│                                                             │
│  错误预算                                                    │
│  ├── 月度预算：30天 × 0.1% = 43.2分钟                        │
│  ├── 预算消耗：故障时间 / 月度预算                            │
│  └── 预算耗尽：停止发布，专注可靠性                           │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### SLO告警规则

```yaml
groups:
- name: slo-alerts
  rules:
  # 可用性SLO告警
  - alert: SLOAvailabilityBurnRateHigh
    expr: |
      (
        sum(rate(http_requests_total{status!~"5.."}[1h]))
        / sum(rate(http_requests_total[1h]))
      ) < 0.999
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "可用性SLO即将违反"
      description: "过去1小时可用性 {{ $value | printf \"%.4f\" }}，低于SLO 99.9%"

  # 错误预算消耗告警
  - alert: ErrorBudgetBurnRateCritical
    expr: |
      (
        sum(rate(http_requests_total{status=~"5.."}[1h]))
        / sum(rate(http_requests_total[1h]))
      )
      /
      (1 - 0.999)  # 错误预算
      > 10         # 消耗速率超过10倍
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "错误预算快速消耗"
      description: "错误预算消耗速率是正常的 {{ $value | printf \"%.0f\" }} 倍"

  # 延迟SLO告警
  - alert: SLOLatencyP99High
    expr: |
      histogram_quantile(0.99, 
        sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
      ) > 0.2
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "延迟SLO即将违反"
      description: "P99延迟 {{ $value | printf \"%.3f\" }}s，超过SLO 200ms"
```

### 多窗口告警策略

```yaml
# 短窗口快速告警 + 长窗口确认
groups:
- name: multi-window-slo
  rules:
  - alert: SLOViolationShortWindow
    expr: |
      sum(rate(http_requests_total{status=~"5.."}[5m]))
      / sum(rate(http_requests_total[5m])) > 0.01
    labels:
      severity: warning
    annotations:
      summary: "短期错误率上升"

  - alert: SLOViolationLongWindow
    expr: |
      sum(rate(http_requests_total{status=~"5.."}[1h]))
      / sum(rate(http_requests_total[1h])) > 0.001
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "长期错误率超标"
      description: "过去1小时错误率超过SLO，需要立即处理"
```

---

## 常见问题与解答

### 1. Prometheus存储满了怎么办？

**短期解决**：

```bash
# 缩短数据保留时间
prometheus --storage.tsdb.retention.time=15d

# 或限制存储大小
prometheus --storage.tsdb.retention.size=50GB
```

**长期方案**：

- 使用Thanos或VictoriaMetrics实现长期存储
- 配置远程存储后端
- 降低采集频率或减少指标数量

### 2. 如何避免告警风暴？

**策略**：

1. **合理分组**：按alertname、namespace等维度分组
2. **设置持续时间**：避免瞬时波动触发告警
3. **使用抑制规则**：根因告警抑制传播告警
4. **分级告警**：critical立即通知，warning聚合通知

### 3. 如何监控Kubernetes集群外的服务？

```yaml
# 创建Endpoints指向外部服务
apiVersion: v1
kind: Endpoints
metadata:
  name: external-service
subsets:
  - addresses:
      - ip: 192.168.1.100
    ports:
      - port: 9104

---
# 创建Service
apiVersion: v1
kind: Service
metadata:
  name: external-service
  labels:
    app: external-service
spec:
  ports:
    - port: 9104
      targetPort: 9104

---
# 创建ServiceMonitor
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: external-service
spec:
  selector:
    matchLabels:
      app: external-service
  endpoints:
    - port: ""
      targetPort: 9104
```

### 4. 如何实现告警的分级路由？

```yaml
route:
  receiver: 'default'
  routes:
    # P1：立即电话+短信
    - match:
        severity: critical
        team: platform
      receiver: 'platform-pagerduty'
      
    # P2：Slack通知
    - match:
        severity: warning
      receiver: 'slack-warnings'
      
    # P3：邮件通知
    - match:
        severity: info
      receiver: 'email-info'
```

### 5. 如何监控HPA和VPA的效果？

```yaml
# HPA扩缩容监控
- alert: HPA频繁扩缩容
  expr: rate(kube_hpa_status_current_replicas[1h]) > 0.1
  for: 30m
  labels:
    severity: warning
  annotations:
    summary: "HPA频繁扩缩容"
    description: "HPA {{ $labels.hpa }} 在过去1小时内频繁扩缩容"

# VPA推荐值监控
- alert: VPA推荐值与实际差异大
  expr: |
    abs(
      kube_pod_container_resource_requests{resource="memory"}
      - on(pod, container) group_left()
      vpa_container_recommendation{target="memory"}
    ) / kube_pod_container_resource_requests{resource="memory"} > 0.5
  for: 1h
  labels:
    severity: info
  annotations:
    summary: "VPA推荐值与实际配置差异大"
    description: "容器 {{ $labels.container }} 的资源配置可能需要调整"
```
