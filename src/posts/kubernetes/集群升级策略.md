---
date: 2026-01-30
author: Gaaming Zhang
isOriginal: false
article: true
category:
  - Kubernetes
tag:
  - Kubernetes
  - ClaudeCode
---

# Kubernetes 升级策略

## 什么是"升级"？

在 Kubernetes 世界里，"升级"这个词其实包含两个不同的概念：

1. **应用升级**：把你的应用从 v1 升级到 v2，比如发布新功能或修复 bug
2. **集群升级**：把 Kubernetes 本身从 1.27 升级到 1.28

这两种升级的策略和风险完全不同。本文主要讨论应用升级策略，因为这是日常开发中更常遇到的场景。

## 为什么升级需要策略？

想象你经营一家 24 小时营业的餐厅，要把厨房设备从老式的换成新式的。你有几个选择：

1. **关门装修**：停业一天，一次性换完（停机更新）
2. **逐步替换**：一边营业一边换，每次换一台（滚动更新）
3. **新建一个厨房**：在旁边建新厨房，装修好后瞬间切换（蓝绿部署）
4. **先让部分顾客试用**：新厨房先服务10%的顾客，没问题再扩大（金丝雀发布）

每种方式都有优缺点，需要根据你的业务特点选择。

## 滚动更新（Rolling Update）

这是 Kubernetes 默认的更新策略，也是最常用的方式。

### 原理

滚动更新的核心思想是：**逐步用新版本的 Pod 替换旧版本**，而不是一次性全部替换。

假设你有 6 个 Pod 运行 v1 版本，要升级到 v2：

```
开始:   v1 v1 v1 v1 v1 v1
  ↓     先启动2个v2，同时还有6个v1在服务
过程:   v1 v1 v1 v1 v1 v1 v2 v2
  ↓     v2就绪后，终止2个v1
过程:   v1 v1 v1 v1 v2 v2
  ↓     重复这个过程...
完成:   v2 v2 v2 v2 v2 v2
```

整个过程中，始终有 Pod 在提供服务，用户无感知。

### 关键配置

```yaml
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%        # 更新时最多可以多出多少Pod
      maxUnavailable: 25%  # 更新时最多可以少多少Pod
```

- `maxSurge`：允许临时多出的 Pod 数量。设置高一些可以加快更新速度
- `maxUnavailable`：允许不可用的 Pod 数量。设置为 0 可以保证更新期间容量不减少

### 必须配置就绪探针

滚动更新依赖**就绪探针（Readiness Probe）**来判断新 Pod 是否可以接收流量。没有就绪探针，Kubernetes 无法知道新 Pod 什么时候真正准备好，可能会把流量发到还没初始化完成的 Pod 上。

```yaml
readinessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
```

## 蓝绿部署（Blue-Green）

### 原理

蓝绿部署的思路是：**同时运行两套完整的环境，通过切换流量实现瞬间升级**。

- 蓝色环境：当前正在服务的版本
- 绿色环境：新版本，已经部署好但还没有流量

当你确认绿色环境一切正常后，把流量从蓝色切到绿色，瞬间完成升级。如果出问题，再切回蓝色。

### 优缺点

**优点**：
- 切换瞬间完成，回滚也是瞬间
- 新版本可以在切换前充分测试

**缺点**：
- 需要双倍资源
- 数据库 schema 变更等场景比较复杂

### 实现方式

在 Kubernetes 中，通常通过修改 Service 的 selector 来切换流量：

```bash
# 切换到绿色环境
kubectl patch service web-app -p '{"spec":{"selector":{"version":"green"}}}'

# 如果有问题，切回蓝色
kubectl patch service web-app -p '{"spec":{"selector":{"version":"blue"}}}'
```

## 金丝雀发布（Canary）

### 原理

金丝雀发布的名字来源于煤矿工人用金丝雀探测有毒气体的做法。核心思想是：**先让一小部分用户使用新版本，观察是否有问题，再逐步扩大范围**。

比如：
1. 先让 10% 的流量走新版本
2. 观察错误率、响应时间等指标
3. 没问题的话，逐步增加到 30%、50%、100%
4. 如果有问题，立即把流量切回旧版本

### 实现方式

最简单的方式是通过调整副本数：

```
稳定版 v1：9 个 Pod（占 90% 流量）
金丝雀 v2：1 个 Pod（占 10% 流量）
```

因为 Service 会按 Pod 数量分配流量，所以 v2 会接收大约 10% 的请求。

更精细的控制可以通过 Ingress 实现，按权重分配流量。

### 什么时候可以全量发布？

当以下指标稳定后：
- 错误率没有上升
- 响应时间正常
- 资源使用正常
- 没有新的告警

## 如何选择策略？

| 策略 | 适用场景 | 资源需求 | 回滚速度 |
|------|----------|----------|----------|
| 滚动更新 | 大多数场景 | 稍多 | 较慢 |
| 蓝绿部署 | 需要瞬间切换 | 双倍 | 瞬间 |
| 金丝雀 | 高风险变更 | 稍多 | 较快 |

一般建议：
- **常规更新**：使用滚动更新，简单可靠
- **重大变更**：使用金丝雀发布，逐步验证
- **数据库相关变更**：需要特别小心，可能需要蓝绿部署配合其他手段

## 保护服务可用性：PodDisruptionBudget

更新过程中，你希望确保始终有足够的 Pod 在服务。PodDisruptionBudget（PDB）可以帮你做到这一点：

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: web-app-pdb
spec:
  minAvailable: 2    # 至少保持 2 个 Pod 可用
  selector:
    matchLabels:
      app: web-app
```

有了这个配置，Kubernetes 在进行任何可能中断 Pod 的操作（包括更新、节点维护等）时，都会确保至少有 2 个 Pod 保持可用。

## 常用命令

```bash
# 更新镜像版本
kubectl set image deployment/web-app app=myapp:v2

# 查看更新进度
kubectl rollout status deployment/web-app

# 查看更新历史
kubectl rollout history deployment/web-app

# 回滚到上一个版本
kubectl rollout undo deployment/web-app

# 回滚到指定版本
kubectl rollout undo deployment/web-app --to-revision=2
```

## 常见问题

### Q1: 滚动更新卡住了怎么办？

更新卡住通常是因为新的 Pod 无法变成 Ready 状态。排查步骤：

1. 查看 Pod 状态：`kubectl get pods`
2. 查看 Pod 详情：`kubectl describe pod <pod-name>`
3. 查看 Pod 日志：`kubectl logs <pod-name>`

常见原因：
- 镜像拉取失败（检查镜像名称和仓库访问权限）
- 健康检查失败（检查应用是否正确响应健康检查请求）
- 资源不足（检查节点是否有足够的 CPU 和内存）

### Q2: 如何实现零停机更新？

零停机更新需要多个配置配合：

1. **配置就绪探针**：让 K8s 知道 Pod 什么时候真正准备好
2. **合理的终止宽限期**：给应用足够的时间处理完正在进行的请求
3. **preStop 钩子**：在 Pod 被终止前执行清理操作
4. **PodDisruptionBudget**：确保更新期间有足够的 Pod 在服务

### Q3: 金丝雀发布发现问题怎么办？

立即把金丝雀版本的副本数设为 0：

```bash
kubectl scale deployment web-app-canary --replicas=0
```

所有流量会自动回到稳定版本。然后排查问题，修复后再重新发布。

### Q4: 如何监控更新过程？

几个关键指标：
- Pod 的 Ready 状态
- 应用的错误率
- 应用的响应时间
- 新版本的资源使用情况

可以使用 Prometheus + Grafana 来可视化这些指标，或者简单地用 `kubectl get pods -w` 实时观察 Pod 状态变化。

### Q5: 集群升级（Kubernetes 版本）怎么做？

集群升级比应用升级复杂得多，基本流程是：

1. 备份 etcd
2. 升级控制平面节点
3. 逐个升级工作节点（先驱逐 Pod，再升级，再恢复）

建议：
- 先在测试环境验证
- 只跨一个小版本升级（比如 1.27 → 1.28）
- 保留回滚能力

具体操作因集群搭建方式而异（kubeadm、云厂商托管等），请参考对应的官方文档。

## 参考资源

- [Kubernetes Deployment 滚动更新官方文档](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment)
- [Kubernetes 更新策略指南](https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/)
- [PodDisruptionBudget 官方文档](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/)
