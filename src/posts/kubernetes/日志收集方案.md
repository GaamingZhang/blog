---
date: 2026-01-30
author: Gaaming Zhang
isOriginal: true
article: true
category:
  - Kubernetes
tag:
  - Kubernetes
  - ClaudeCode
---

# Kubernetes 日志收集方案

## 为什么需要集中收集日志？

想象一下这个场景：你的应用出了问题，用户反馈页面报错。你想查看日志来排查问题，但发现应用运行在10个Pod上，而且因为自动伸缩，有些Pod已经被销毁了。这时候你会发现：

- **Pod是短暂的**：Pod重启或被删除后，里面的日志就消失了
- **日志是分散的**：几十上百个Pod的日志散落在不同节点上
- **查询很困难**：你不知道用户的请求到底被哪个Pod处理了

这就像图书馆的书如果没有索引系统，你每次找书都要把所有书架翻一遍。集中日志收集就是给你的应用日志建立一个"索引系统"，让你能快速找到需要的信息。

## 日志收集的基本原理

在 Kubernetes 中，容器的标准输出（stdout）和标准错误（stderr）会被容器运行时捕获，保存到节点的 `/var/log/containers/` 目录下。日志收集的核心思路就是：**把这些散落在各节点的日志文件，统一收集到一个地方存储和查询**。

```
你的应用输出日志 → 容器运行时捕获 → 保存到节点磁盘 → 日志收集器读取 → 发送到中央存储
```

## 三种收集模式

### 1. 节点级代理（推荐）

在每个节点上运行一个日志收集器（用 DaemonSet 部署），它负责收集该节点上所有容器的日志。

**优点**：资源效率高，一个收集器服务整个节点
**缺点**：所有应用的日志格式需要相对统一

这就像每栋楼有一个物业管理员，统一收取所有住户的快递，然后送到快递柜。

### 2. Sidecar 模式

在每个 Pod 里额外运行一个日志收集容器，专门负责这个 Pod 的日志。

**优点**：灵活，可以针对不同应用做定制处理
**缺点**：资源开销大，每个 Pod 都要多跑一个容器

这就像每家住户都单独请了一个快递员。

### 3. 直接推送

应用自己把日志直接发送到日志后端。

**优点**：简单直接
**缺点**：增加了应用的复杂度，应用需要知道日志后端地址

**对于大多数场景，推荐使用节点级代理模式**，它在效率和灵活性之间取得了很好的平衡。

## 两个主流方案

### EFK Stack：功能强大

EFK 是 Elasticsearch + Fluentd + Kibana 的组合：

- **Elasticsearch**：存储和索引日志，支持强大的全文搜索
- **Fluentd**：收集和转发日志
- **Kibana**：可视化查询界面

**适合场景**：需要复杂日志查询、日志量大、对搜索功能要求高的团队

### PLG Stack：轻量高效

PLG 是 Promtail + Loki + Grafana 的组合：

- **Loki**：只索引日志的标签（如 Pod 名、命名空间），不索引日志内容
- **Promtail**：收集日志并添加标签
- **Grafana**：查询和展示界面（和监控共用）

**适合场景**：中小规模集群、已经在用 Prometheus + Grafana 的团队

**两者最大的区别**：Elasticsearch 对日志内容建立全文索引，搜索强但资源消耗大；Loki 只索引标签，搜索能力弱但资源消耗小。就像图书馆的精细目录和简单分类的区别。

## 核心配置示例

以下是一个简化的 Fluentd DaemonSet 配置，展示日志收集的核心流程：

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      containers:
        - name: fluentd
          image: fluent/fluentd-kubernetes-daemonset:v1.14
          volumeMounts:
            # 挂载节点的日志目录，这样才能读取到容器日志
            - name: varlog
              mountPath: /var/log
              readOnly: true
      volumes:
        - name: varlog
          hostPath:
            path: /var/log
```

关键点：
- 使用 **DaemonSet** 确保每个节点都运行一个收集器
- 挂载 **/var/log** 目录来读取容器日志文件
- 收集器会自动发现新的日志文件并持续收集

## 日志格式很重要

无论用哪个方案，都强烈建议使用**结构化日志**（JSON格式）：

```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "level": "INFO",
  "service": "user-api",
  "message": "User login successful",
  "userId": "12345",
  "requestId": "req-abc123"
}
```

为什么？因为结构化日志：
- 可以按字段过滤，比如"只看 ERROR 级别"
- 可以按字段统计，比如"统计每个服务的错误数"
- 可以关联请求，通过 requestId 找到完整的请求链路

## 常用命令

在日志系统搭建好之前，你仍然可以用 kubectl 查看日志：

```bash
# 查看 Pod 日志
kubectl logs my-pod

# 实时跟踪日志（类似 tail -f）
kubectl logs -f my-pod

# 查看最近1小时的日志
kubectl logs --since=1h my-pod

# 查看某个标签的所有 Pod 日志
kubectl logs -l app=myapp
```

## 常见问题

### Q1: 日志丢失怎么办？

日志丢失通常有几个原因：

1. **Pod 被销毁了**：这是最常见的原因。如果没有集中收集，Pod 一删日志就没了。解决方案就是部署日志收集系统。

2. **磁盘空间满了**：节点的 `/var/log` 目录空间不足，新日志无法写入。需要配置日志轮转和清理策略。

3. **收集器出问题了**：检查日志收集器 Pod 是否正常运行，查看它自己的日志有没有报错。

### Q2: 日志量太大怎么处理？

这是个常见问题，几个建议：

- **调整日志级别**：生产环境用 INFO 或 WARN，不要用 DEBUG
- **设置保留策略**：比如只保留最近7天或30天的日志
- **过滤无用日志**：健康检查等高频且无价值的日志可以不收集

### Q3: EFK 和 PLG 怎么选？

简单的选择建议：

- 如果你已经在用 Grafana 做监控 → 选 PLG，可以复用
- 如果你需要强大的日志搜索功能 → 选 EFK
- 如果你的集群规模不大、预算有限 → 选 PLG，资源消耗小很多
- 如果你不确定 → 从 PLG 开始，它更简单，后续有需要可以迁移

### Q4: 如何追踪一个请求的完整日志？

在分布式系统中，一个用户请求可能经过多个服务。要把这些日志关联起来，需要使用**分布式追踪ID**：

1. 在请求入口生成一个唯一的 traceId
2. 每个服务处理请求时，都在日志中记录这个 traceId
3. 查日志时，用 traceId 过滤就能看到完整链路

这通常需要配合链路追踪系统（如 Jaeger、Zipkin）一起使用。
