---
date: 2025-07-01
author: Gaaming Zhang
isOriginal: false
article: true
category:
  - 操作系统
tag:
  - 操作系统
---

# awk：面向字段的文本处理工具

## awk 是什么

**awk** 是一个专为结构化文本设计的编程语言工具,就像Excel处理表格一样,awk把文本文件视为由行和列组成的表格数据。它的名字来源于三位作者的姓氏首字母:Alfred Aho、Peter Weinberger和Brian Kernighan。

### 为什么需要 awk

**处理表格化数据**:
- 日志文件:提取特定列、统计数据
- CSV文件:数据清洗、格式转换
- 系统信息:分析进程列表、网络连接

**数据分析**:
- 分组统计:按某个字段分组计算
- 聚合计算:求和、平均、最大最小值
- 去重计数:统计唯一值数量

**数据转换**:
- 格式化输出:对齐列、添加表头
- 字段重组:调整列顺序、合并字段
- 数据清洗:删除空行、去除空格

### awk vs 其他工具

| 工具  | 擅长领域           | 数据视角 | 典型用途               |
| ----- | ------------------ | -------- | ---------------------- |
| grep  | 搜索匹配           | 整行     | 查找包含关键词的行     |
| sed   | 文本替换、删除行   | 字节流   | 批量修改、删除特定行   |
| awk   | 字段处理、数据分析 | 表格列   | 提取列、统计计算、分组 |

**选择建议**:
- 只查找 → grep
- 只替换或删除 → sed
- 涉及列操作或计算 → awk

---

## awk 的核心思想

### 表格数据模型

awk 将文本视为表格,每一行是一条**记录**,每行中的空格分隔的部分是**字段**:

```
文本文件                    awk 的理解
-----------                 -----------
John 25 Beijing       →     记录1: [$1="John"] [$2="25"] [$3="Beijing"]
Alice 30 Shanghai     →     记录2: [$1="Alice"] [$2="30"] [$3="Shanghai"]
Bob 28 Shenzhen       →     记录3: [$1="Bob"] [$2="28"] [$3="Shenzhen"]
```

**关键概念**:
- **$0**:整行内容(完整记录)
- **$1, $2, $3...**:第1列、第2列、第3列
- **NF**:当前行的字段数(Number of Fields)
- **NR**:当前行号(Number of Records)

### 三阶段处理流程

awk 的处理分为三个阶段:

```
                 ┌──────────────┐
                 │  BEGIN 块    │  初始化阶段
                 │ (执行一次)   │  - 设置变量
                 │              │  - 打印表头
                 └──────┬───────┘
                        ↓
                 ┌──────────────┐
   文件每一行 →  │   主体块     │  数据处理阶段
                 │ (循环执行)   │  - 匹配模式
                 │              │  - 执行操作
                 └──────┬───────┘
                        ↓
                 ┌──────────────┐
                 │   END 块     │  收尾阶段
                 │ (执行一次)   │  - 统计汇总
                 │              │  - 打印结果
                 └──────────────┘
```

**三个阶段的作用**:

**BEGIN 块**:在处理文件之前执行
- 初始化变量
- 设置分隔符
- 打印表头

**主体块**:对每一行执行
- 模式匹配:判断是否处理这一行
- 执行操作:提取字段、累加统计

**END 块**:处理完所有行后执行
- 打印统计结果
- 输出汇总信息

### 模式-动作 范式

awk 的基本语法是:

```bash
awk 'pattern { action }' file
```

**含义**:
- **pattern**:条件,决定哪些行要处理
- **action**:操作,对匹配的行做什么

**示例**:
```bash
# 打印第2列大于25的行的第1列和第2列
awk '$2 > 25 { print $1, $2 }' data.txt
```

解析:
- `$2 > 25`:模式(条件),只处理第2列>25的行
- `print $1, $2`:动作,打印第1列和第2列

---

## awk 的工作流程

### 字段分割机制

awk 自动将每一行按分隔符切分为多个字段:

```
默认分隔符:空格或制表符

输入: "John  25   Beijing"
     ↓
分割: [$1="John"] [$2="25"] [$3="Beijing"]
     ↓
$0 = "John  25   Beijing"  (整行)
NF = 3                      (字段数)
```

**自定义分隔符**:
```bash
# 使用冒号分隔
awk -F: '{ print $1 }' /etc/passwd

# 使用逗号分隔 (处理 CSV)
awk -F, '{ print $1, $3 }' data.csv
```

### 内置变量

awk 提供了一些自动维护的变量:

**记录相关**:
- **NR**:当前处理的行号(从1开始)
- **NF**:当前行的字段数
- **$0**:当前行的完整内容

**分隔符相关**:
- **FS**:输入字段分隔符(Field Separator)
- **OFS**:输出字段分隔符(Output Field Separator)

**示例**:
```bash
# 打印行号和内容
awk '{ print NR ":", $0 }' file.txt

# 只处理字段数>=3的行
awk 'NF >= 3 { print }' file.txt

# 设置输出分隔符为逗号
awk 'BEGIN { OFS="," } { print $1, $2 }' file.txt
```

### 模式匹配

**正则表达式模式**:
```bash
# 只处理包含 "error" 的行
awk '/error/ { print }' log.txt
```

**条件模式**:
```bash
# 第3列大于5000
awk '$3 > 5000 { print }' data.txt

# 多条件
awk '$2 > 25 && $3 == "Beijing" { print }' data.txt
```

**范围模式**:
```bash
# 从包含 "start" 的行到包含 "end" 的行
awk '/start/, /end/ { print }' file.txt
```

---

## 关联数组:awk 的核心能力

### 什么是关联数组

**普通数组**(索引是数字):
```
arr[0] = "apple"
arr[1] = "banana"
arr[2] = "orange"
```

**关联数组**(索引是字符串):
```
count["apple"] = 10
count["banana"] = 20
count["orange"] = 15
```

awk 的数组都是关联数组,类似于:
- Python 的字典 (dict)
- Java 的 HashMap
- JavaScript 的对象

### 为什么关联数组重要

关联数组让awk能够:
- **分组统计**:按某个字段分组计数
- **去重**:记录已见过的值
- **查找**:快速判断某个值是否存在

**典型模式**:计数

```bash
# 统计各城市出现的次数
awk '{ count[$3]++ } END { for (city in count) print city, count[city] }' data.txt
```

工作流程:
```
第1行: Beijing    → count["Beijing"] = 1
第2行: Shanghai   → count["Shanghai"] = 1
第3行: Beijing    → count["Beijing"] = 2
第4行: Shenzhen   → count["Shenzhen"] = 1
第5行: Shanghai   → count["Shanghai"] = 2

END块输出:
Beijing 2
Shanghai 2
Shenzhen 1
```

**典型模式**:求和

```bash
# 按名字分组,累加工资
awk '{ total[$1] += $2 } END { for (name in total) print name, total[name] }' salary.txt
```

**典型模式**:去重

```bash
# 只打印第一次出现的行
awk '!seen[$0]++' file.txt
```

解析:
- `seen[$0]`:以整行内容为键的数组元素
- `!seen[$0]++`:如果该行未见过(值为0),!0为真,打印该行;同时值加1
- 第二次遇到相同行时,值已是1,!1为假,不打印

---

## 常见使用场景

### 场景1:提取特定列

**需求**:从日志中提取IP地址和状态码

输入:
```
192.168.1.1 - - [20/Dec/2024] "GET /index.html" 200
192.168.1.2 - - [20/Dec/2024] "POST /api/data" 201
```

命令:
```bash
awk '{ print $1, $9 }' access.log
```

### 场景2:条件过滤

**需求**:找出工资大于5000的员工

命令:
```bash
awk '$3 > 5000 { print $1, $3 }' salary.txt
```

### 场景3:统计分析

**需求**:统计不同状态码的请求数

命令:
```bash
awk '{ count[$9]++ } END { for (code in count) print code, count[code] }' access.log
```

### 场景4:计算聚合

**需求**:计算平均工资

命令:
```bash
awk '{ sum += $2; n++ } END { print sum/n }' salary.txt
```

### 场景5:数据去重

**需求**:删除重复行

命令:
```bash
awk '!seen[$0]++' file.txt
```

### 场景6:格式转换

**需求**:CSV转TSV (逗号转制表符)

命令:
```bash
awk -F, 'BEGIN { OFS="\t" } { print $1, $2, $3 }' file.csv
```

---

## awk vs sed vs grep

### 使用场景对比

**grep - 文本搜索**:
- 只查找,不修改
- 关注:整行是否匹配
- 典型用途:过滤日志、查找文件内容

**sed - 文本编辑**:
- 替换、删除、插入行
- 关注:字节流处理
- 典型用途:批量替换、删除特定行

**awk - 数据处理**:
- 提取列、统计计算、分组聚合
- 关注:字段和表格结构
- 典型用途:日志分析、数据统计、报表生成

### 选择决策树

```
需要操作列或计算?
├─ 是 → awk
└─ 否 ↓

需要替换或删除内容?
├─ 是 → sed
└─ 否 ↓

只需要查找?
└─ 是 → grep
```

### 组合使用

三个工具可以通过管道组合:

```bash
# grep 过滤 + awk 提取列 + sed 替换
grep "ERROR" app.log | awk '{ print $1, $5 }' | sed 's/ERROR/错误/g'
```

**各工具的角色**:
- grep:筛选出包含"ERROR"的行
- awk:提取第1列和第5列
- sed:将"ERROR"替换为"错误"

---

## 实用技巧

### 设置分隔符

**命令行设置**:
```bash
awk -F: '{ print $1 }' /etc/passwd
```

**BEGIN块设置**:
```bash
awk 'BEGIN { FS=":" } { print $1 }' /etc/passwd
```

### 格式化输出

**print** 自动添加空格和换行:
```bash
awk '{ print $1, $2 }' file.txt
```

**printf** 精确控制格式:
```bash
awk '{ printf "%-10s %5d\n", $1, $2 }' file.txt
```

格式说明:
- `%-10s`:左对齐,宽度10的字符串
- `%5d`:右对齐,宽度5的整数
- `%.2f`:保留2位小数的浮点数

### 跳过表头

```bash
# 跳过第1行
awk 'NR > 1 { print }' file.txt

# 或使用 FNR (处理单个文件)
awk 'FNR > 1 { print }' file.txt
```

### 统计词频

```bash
awk '{ for (i=1; i<=NF; i++) count[$i]++ } END { for (w in count) print w, count[w] }' file.txt
```

工作原理:
- 外层循环:遍历每一行
- 内层循环:遍历每个字段
- 关联数组:以单词为键,累加计数

---

## 理解 awk 的限制

### 不适合的场景

**复杂业务逻辑**:
- awk 适合简单的数据处理
- 复杂的业务规则建议用Python/Perl

**大规模数据**:
- awk 逐行处理,内存占用小
- 但关联数组会占用内存,超大文件需注意

**需要正则回溯**:
- awk 的正则支持有限
- 复杂的正则表达式用 Perl 更合适

### 适合的场景

**表格化数据**:
- CSV、TSV、日志文件
- 固定格式的系统输出

**快速统计**:
- 计数、求和、平均
- 分组聚合

**数据清洗**:
- 提取特定列
- 去重、过滤
- 格式转换

---

## 核心要点

**awk 的本质**:面向字段的文本处理工具,将文本视为表格数据。

**核心概念**:
- **字段分割**:自动将行分割为列($1, $2, $3...)
- **三阶段处理**:BEGIN(初始化) → 主体(逐行处理) → END(汇总)
- **关联数组**:以字符串为键的数组,实现分组统计和去重

**典型应用**:
- **日志分析**:提取IP、状态码,统计访问量
- **数据统计**:求和、平均、计数、分组
- **格式转换**:CSV转TSV、调整列顺序
- **数据清洗**:去重、过滤、提取

**使用技巧**:
- 用`-F`指定分隔符
- 用`NR`判断行号
- 用`NF`判断字段数
- 用关联数组实现统计
- 用`!seen[$0]++`去重

**与其他工具对比**:
- grep:只查找 → awk:查找+提取+计算
- sed:替换删除 → awk:字段操作+统计
- 选择原则:涉及列操作或计算就用awk

**最佳实践**:
- 简单任务优先用awk(比写脚本快)
- 复杂逻辑考虑Python/Perl
- 配合管道使用(grep | awk | sort)
- 测试时先不加`-i`,确认结果再修改原文件
