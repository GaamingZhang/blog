---
date: 2025-07-01
author: Gaaming Zhang
isOriginal: false
article: true
category:
  - 操作系统
tag:
  - 操作系统
---

# 系统内核上下文切换

## 核心概念

**上下文切换**（Context Switch）是指 CPU 从一个进程或线程切换到另一个进程或线程时，保存当前执行状态并恢复新任务执行状态的过程。这是操作系统实现多任务并发执行的核心机制。

---

## 什么是上下文（Context）

**上下文**包含了 CPU 恢复某个任务执行所需的所有信息：

**1. CPU 寄存器状态**：
- **通用寄存器**：如 EAX, EBX, ECX, EDX（x86）或 R0-R15（ARM）
- **程序计数器（PC/IP）**：指向下一条要执行的指令地址
- **栈指针（SP）**：指向当前栈顶
- **帧指针（FP/BP）**：指向当前栈帧
- **状态寄存器（EFLAGS）**：包含条件码、中断标志等
- **段寄存器**：CS（代码段）、DS（数据段）、SS（栈段）等

**2. 内存管理信息**：
- **页表基址寄存器**：如 CR3（x86），指向进程的页表
- **TLB（Translation Lookaside Buffer）**：地址转换缓存（切换时需要刷新）

**3. 内核栈信息**：
- 每个进程/线程都有独立的内核栈
- 保存系统调用参数、返回地址等

**4. 其他状态**：
- FPU（浮点运算单元）状态
- SIMD 寄存器（如 SSE、AVX）
- 调试寄存器
- 性能计数器

---

## 上下文切换的类型

**1. 进程上下文切换（Process Context Switch）**

切换不同进程时发生，开销最大。

**需要保存/恢复的内容**：
```
- 所有 CPU 寄存器
- 页表（虚拟内存映射）
- 打开的文件描述符信息
- 信号处理信息
- 进程优先级和调度信息
- 内核栈
- TLB 缓存（需要刷新）
```

**触发条件**：
- 时间片用完（时钟中断）
- 进程阻塞（等待 I/O、sleep、等待锁等）
- 进程主动让出 CPU（yield）
- 进程终止
- 更高优先级进程就绪

**2. 线程上下文切换（Thread Context Switch）**

**同一进程内的线程切换**：
```
优势：
- 共享相同的虚拟内存空间（不需要切换页表）
- 不需要刷新 TLB
- 共享打开的文件描述符
- 开销远小于进程切换

仍需切换：
- CPU 寄存器
- 程序计数器
- 栈指针
- 线程私有数据
```

**不同进程的线程切换**：
- 等同于进程上下文切换，开销相同

**3. 中断上下文切换（Interrupt Context Switch）**

**硬件中断发生时**：
```
1. CPU 自动保存部分寄存器（如 PC、状态寄存器）
2. 跳转到中断处理程序
3. 中断处理程序可能保存更多寄存器
4. 处理完成后恢复寄存器
5. 返回被中断的任务

特点：
- 不是完整的进程切换
- 但会打断当前执行流
- 频繁中断也会影响性能
```

**4. 系统调用上下文切换**

**用户态 ↔ 内核态切换**：
```
不是完整的上下文切换，但涉及：
- 特权级切换（用户态 Ring 3 → 内核态 Ring 0）
- 栈切换（用户栈 → 内核栈）
- 保存部分寄存器
- 开销小于进程切换，但仍有成本
```

---

## 上下文切换的详细过程

**完整的进程上下文切换流程**：

```
假设：进程 A 切换到进程 B

第一阶段：保存进程 A 的上下文
┌─────────────────────────────────────────┐
│ 1. 触发切换（时钟中断/系统调用/阻塞）    │
│ 2. 进入内核态                            │
│ 3. 保存 A 的 CPU 寄存器到 A 的 PCB       │
│    - 通用寄存器 (EAX, EBX, ...)         │
│    - 程序计数器 (PC)                     │
│    - 栈指针 (SP)                         │
│    - 状态寄存器 (EFLAGS)                 │
│ 4. 保存 A 的浮点/SIMD 寄存器状态         │
│ 5. 更新 A 的进程状态（运行 → 就绪/阻塞） │
└─────────────────────────────────────────┘

第二阶段：调度决策
┌─────────────────────────────────────────┐
│ 6. 调用调度器（scheduler）               │
│ 7. 从就绪队列选择进程 B                  │
│    - 根据调度算法（CFS/优先级等）        │
│ 8. 更新 B 的进程状态（就绪 → 运行）      │
└─────────────────────────────────────────┘

第三阶段：恢复进程 B 的上下文
┌─────────────────────────────────────────┐
│ 9. 切换地址空间（页表）                  │
│    - 更新 CR3 寄存器为 B 的页表基址      │
│    - 刷新 TLB（清空地址转换缓存）        │
│ 10. 切换内核栈（从 A 的内核栈到 B 的）   │
│ 11. 从 B 的 PCB 恢复 CPU 寄存器          │
│ 12. 恢复 B 的浮点/SIMD 寄存器            │
│ 13. 更新 TSS（任务状态段）               │
│ 14. 跳转到 B 的程序计数器位置            │
│ 15. 进程 B 继续执行                      │
└─────────────────────────────────────────┘
```

**关键代码示例（Linux 内核）**：

```c
// Linux 内核调度核心函数
// kernel/sched/core.c

static void __sched notrace __schedule(bool preempt)
{
    struct task_struct *prev, *next;
    unsigned long *switch_count;
    struct rq *rq;
    int cpu;

    cpu = smp_processor_id();
    rq = cpu_rq(cpu);
    prev = rq->curr;  // 当前进程

    // 1. 选择下一个要运行的进程
    next = pick_next_task(rq, prev, &rf);
    
    if (likely(prev != next)) {
        rq->nr_switches++;  // 切换计数
        rq->curr = next;
        
        // 2. 执行上下文切换
        rq = context_switch(rq, prev, next, &rf);
    }
}

// 上下文切换的核心实现
static __always_inline struct rq *
context_switch(struct rq *rq, struct task_struct *prev,
               struct task_struct *next, struct rq_flags *rf)
{
    // 1. 切换内存管理上下文（页表）
    if (!next->mm) {
        // 内核线程，使用 prev 的地址空间
        next->active_mm = prev->active_mm;
    } else {
        // 切换到 next 的地址空间
        switch_mm_irqs_off(prev->active_mm, next->mm, next);
    }
    
    // 2. 切换寄存器状态和栈
    // 这是架构相关的汇编代码
    switch_to(prev, next, prev);
    
    return finish_task_switch(prev);
}
```

**switch_to 汇编实现（x86_64）**：

```asm
# arch/x86/include/asm/switch_to.h
# 保存和恢复寄存器

ENTRY(__switch_to_asm)
    # 保存 prev 的寄存器
    pushq   %rbp
    pushq   %rbx
    pushq   %r12
    pushq   %r13
    pushq   %r14
    pushq   %r15
    
    # 保存栈指针
    movq    %rsp, TASK_threadsp(%rdi)  # 保存到 prev->thread.sp
    
    # 恢复 next 的栈指针
    movq    TASK_threadsp(%rsi), %rsp  # 从 next->thread.sp 恢复
    
    # 恢复 next 的寄存器
    popq    %r15
    popq    %r14
    popq    %r13
    popq    %r12
    popq    %rbx
    popq    %rbp
    
    # 跳转到 next 的返回地址
    ret
END(__switch_to_asm)
```

---

## 上下文切换的开销

**时间开销**：

```
典型的上下文切换时间：
- 进程切换：1-10 微秒（现代 CPU）
- 线程切换（同进程）：0.1-1 微秒
- 系统调用：0.05-0.5 微秒

组成部分：
1. 直接开销：
   - 保存/恢复寄存器：~100 纳秒
   - 切换页表：~50 纳秒
   - 内核调度代码执行：~500 纳秒
   - 总计：~1 微秒

2. 间接开销（更大，通常占总开销的70-80%）：
   - TLB 刷新：TLB miss 导致页表查询（内存访问延迟 100-200 纳秒/次）
   - CPU 缓存失效：L1/L2/L3 cache miss（L1 miss 约 10 纳秒，L3 miss 约 100 纳秒）
   - 流水线冲刷：指令预取失效（现代 CPU 流水线深度可达 20-30 级）
   - 分支预测失效：重新填充分支预测缓冲（约 15-20 个周期）
```

**TLB 刷新的具体影响**：

**TLB（Translation Lookaside Buffer）** 是 CPU 内部的高速缓存，存储最近使用的虚拟地址到物理地址的映射。TLB 的访问延迟仅需 1-2 个 CPU 周期，而访问主存的页表则需要 100+ 个周期。

```
┌─────────────────────────────────────────────────────┐
│ TLB 结构与刷新机制                                   │
│ - 多级结构：L1 TLB (10-100 条目) + L2 TLB (1k-10k 条目) │
│ - 进程切换时：需要刷新所有 TLB 条目（除非支持 ASID）  │
│ - 线程切换时：无需刷新 TLB（共享同一页表）           │
│ - 现代优化：PCID/ASID（地址空间标识符）允许保留 TLB 条目 │
└─────────────────────────────────────────────────────┘

TLB 失效的性能代价：
- 单次 TLB miss：100-200 纳秒（需要查询页表）
- 连续 TLB miss：可能导致页表遍历（多次内存访问）
- 大量 TLB miss：CPU 停滞等待地址转换完成
```

**CPU 缓存失效的深度分析**：

现代 CPU 具有多层缓存结构，上下文切换会导致缓存内容部分或完全失效：

```
┌─────────────────────────────────────────────────────┐
│ 进程 A 运行时                                        │
│ - L1 I-cache: 充满 A 的指令，命中率 95%+              │
│ - L1 D-cache: 充满 A 的数据，命中率 90%+              │
│ - L2 Cache: 包含 A 的热点数据和指令                    │
│ - L3 Cache: 包含 A 的更多数据，跨核心共享              │
│ - 分支预测器: 已学习 A 的分支模式，预测准确率 90%+     │
└─────────────────────────────────────────────────────┘
                      ↓ 切换到进程 B
┌─────────────────────────────────────────────────────┐
│ 进程 B 运行时                                        │
│ - L1 I-cache: 几乎完全失效，需要重新加载 B 的指令       │
│ - L1 D-cache: 几乎完全失效，需要重新加载 B 的数据       │
│ - L2 Cache: 部分失效，取决于 B 的数据访问模式          │
│ - L3 Cache: 部分失效，核心间共享的数据可能保留          │
│ - 分支预测器: 几乎完全失效，需要重新学习 B 的分支模式     │
└─────────────────────────────────────────────────────┘
```

**缓存预热时间**：
- L1 Cache (32KB): 几千个周期（约 1-2 微秒）
- L2 Cache (256KB): 几万个周期（约 5-10 微秒）
- L3 Cache (8MB): 几十万个周期（约 50-100 微秒）

**流水线和分支预测影响**：
- 现代 CPU 流水线深度：20-30 级（如 Intel Skylake 为 14 级）
- 上下文切换时流水线完全冲刷，重新填充需要 10-20 个周期
- 分支预测缓冲（BPB）完全失效，导致大量分支预测错误
- 指令预取器需要重新学习程序的访问模式

**现代 CPU 技术的影响**：
- **超线程（HT）**：同一核心的线程切换开销更低（共享部分缓存和 TLB）
- **PCID/ASID**：减少 TLB 刷新开销，允许不同进程共享 TLB 条目
- **大页（Huge Pages）**：减少 TLB 条目数量，提高 TLB 命中率
- **NUMA 架构**：进程迁移到不同 NUMA 节点时，缓存失效更严重
- **硬件虚拟化**：虚拟机间切换开销更大（需要额外的 CPU 模式切换）

**测量上下文切换开销的工具**：

```bash
# 1. vmstat - 查看上下文切换次数
vmstat 1

# 输出示例：
# procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
#  r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
#  1  0      0 123456  12345 789012    0    0     0     0  500 2000  5  2 93  0  0
#                                                           ^^  ^^^^
#                                                           |   |
#                                              中断次数 ---|   |--- 上下文切换次数

# 2. pidstat - 按进程查看
pidstat -w 1

# 输出：
# cswch/s - 自愿上下文切换（等待资源）
# nvcswch/s - 非自愿上下文切换（被抢占）

# 3. perf - 性能分析
perf stat -e context-switches,cpu-migrations -a sleep 10

# 4. sar - 系统活动报告
sar -w 1 10  # 每秒 1 次，共 10 次

# 5. 使用 eBPF 追踪
bpftrace -e 'tracepoint:sched:sched_switch { @[comm] = count(); }'
```

---

## 触发上下文切换的场景

**1. 时间片耗尽（Time Slice Expired）**

```c
// Linux 使用完全公平调度器（CFS）
// 没有固定时间片，而是根据虚拟运行时间

// 时钟中断处理
void scheduler_tick(void)
{
    struct task_struct *curr = current;
    
    // 更新进程运行时间
    update_rq_clock(rq);
    curr->sched_class->task_tick(rq, curr, 0);
    
    // 检查是否需要重新调度
    if (need_resched()) {
        set_tsk_need_resched(curr);
    }
}

// CFS 检查是否需要抢占
static void check_preempt_curr_fair(struct rq *rq, struct task_struct *p)
{
    // 如果新进程的虚拟运行时间明显小于当前进程
    // 则设置需要重新调度标志
}
```

**2. 进程阻塞（Blocking）**

```c
// 等待 I/O
read(fd, buffer, size);  // 阻塞，触发切换

// 等待锁
pthread_mutex_lock(&mutex);  // 如果锁被占用，阻塞

// 主动睡眠
sleep(5);  // 主动让出 CPU

// 等待信号
wait(NULL);  // 等待子进程
```

**内核实现**：
```c
// 进程进入睡眠状态
void schedule_timeout(long timeout)
{
    // 设置进程状态为 TASK_INTERRUPTIBLE 或 TASK_UNINTERRUPTIBLE
    __set_current_state(TASK_INTERRUPTIBLE);
    
    // 添加到等待队列
    schedule();  // 触发调度，切换到其他进程
}
```

**3. 更高优先级进程就绪**

```c
// 实时进程（SCHED_FIFO, SCHED_RR）优先级更高
// 当实时进程就绪时，立即抢占普通进程

void wake_up_new_task(struct task_struct *p)
{
    // 新进程加入就绪队列
    activate_task(rq, p, 0);
    
    // 检查是否需要抢占当前进程
    check_preempt_curr(rq, p);
}
```

**4. 中断处理**

```
硬件中断发生：
1. CPU 保存当前指令地址
2. 跳转到中断处理程序
3. 处理中断
4. 可能唤醒等待该中断的进程
5. 中断返回时检查是否需要调度
```

**5. 系统调用**

```c
// 某些系统调用会触发调度
sched_yield();  // 主动让出 CPU
nanosleep();    // 睡眠
wait();         // 等待
futex();        // 等待锁
```

---

## 减少上下文切换的优化策略

**1. 减少线程/进程数量**

```python
# 不好的做法：为每个连接创建线程
def handle_client(client_socket):
    # 处理请求
    pass

# 每个连接一个线程 → 大量上下文切换
for client in clients:
    threading.Thread(target=handle_client, args=(client,)).start()

# 优化：使用线程池
from concurrent.futures import ThreadPoolExecutor

executor = ThreadPoolExecutor(max_workers=10)  # 固定数量
for client in clients:
    executor.submit(handle_client, client)

# 更好：使用异步 I/O（避免线程切换）
import asyncio

async def handle_client(reader, writer):
    # 异步处理，单线程处理多个连接
    pass

asyncio.run(main())
```

**2. 使用协程/异步编程**

```go
// Go 协程：用户态调度，避免内核上下文切换
func main() {
    for i := 0; i < 10000; i++ {
        go func(id int) {
            // 协程由 Go 运行时调度，不触发内核上下文切换
            time.Sleep(time.Second)
        }(i)
    }
}

// Go 调度器（GMP 模型）：
// G (Goroutine) - 协程
// M (Machine) - 内核线程
// P (Processor) - 逻辑处理器
// 多个 G 复用少量 M，减少内核切换
```

**3. 绑定 CPU 亲和性**

```c
// 将进程/线程绑定到特定 CPU，减少迁移
#include <sched.h>

cpu_set_t cpuset;
CPU_ZERO(&cpuset);
CPU_SET(0, &cpuset);  // 绑定到 CPU 0

pthread_t thread;
pthread_create(&thread, NULL, worker, NULL);
pthread_setaffinity_np(thread, sizeof(cpu_set_t), &cpuset);

// 优势：
// - 减少 CPU 缓存失效
// - 减少 CPU 迁移开销
// - 提高缓存命中率
```

```bash
# 使用 taskset 命令
taskset -c 0,1 ./my_program  # 绑定到 CPU 0 和 1

# 查看进程的 CPU 亲和性
taskset -p <pid>
```

**4. 减少锁竞争**

```c
// 高竞争锁导致频繁阻塞和切换
pthread_mutex_t lock;

// 优化策略：

// 1. 使用无锁数据结构
#include <stdatomic.h>
atomic_int counter = 0;
atomic_fetch_add(&counter, 1);  // 无需加锁

// 2. 读写锁（读多写少场景）
pthread_rwlock_t rwlock;
pthread_rwlock_rdlock(&rwlock);  // 多个读者不阻塞
pthread_rwlock_wrlock(&rwlock);  // 写者独占

// 3. 细粒度锁（减小临界区）
// 不好：
pthread_mutex_lock(&global_lock);
// 大量代码
pthread_mutex_unlock(&global_lock);

// 好：
// 短临界区
pthread_mutex_lock(&lock);
critical_operation();
pthread_mutex_unlock(&lock);

// 4. Lock-Free 算法
// 使用 CAS (Compare-And-Swap)
bool compare_and_swap(int* ptr, int old, int new) {
    return __sync_bool_compare_and_swap(ptr, old, new);
}
```

**5. 使用批处理**

```c
// 减少系统调用次数
// 不好：逐个写入
for (int i = 0; i < 1000; i++) {
    write(fd, &data[i], sizeof(data[i]));  // 1000 次系统调用
}

// 好：批量写入
write(fd, data, 1000 * sizeof(data[0]));  // 1 次系统调用

// 使用缓冲 I/O
FILE *fp = fopen("file.txt", "w");
for (int i = 0; i < 1000; i++) {
    fprintf(fp, "%d\n", i);  // 缓冲，不立即触发系统调用
}
fflush(fp);  // 显式刷新
```

**6. 中断合并（Interrupt Coalescing）**

```bash
# 网卡中断合并，减少中断次数
ethtool -C eth0 rx-usecs 100 rx-frames 32

# 参数说明：
# rx-usecs: 等待时间（微秒）
# rx-frames: 批量处理的帧数
# 效果：减少中断处理导致的上下文切换
```

**7. 使用用户态调度**

```
用户态线程（Green Threads）：
- 由用户态调度器管理
- 避免内核上下文切换
- 例如：Go 的 goroutine、Erlang 的进程

优势：
- 切换速度快（纳秒级）
- 可以支持大量并发（百万级）
- 减少内核调用

劣势：
- 无法利用多核（需配合内核线程）
- I/O 操作可能阻塞整个调度器
```

---

## 监控和分析上下文切换

**1. 查看系统整体情况**

```bash
# vmstat - 虚拟内存统计
vmstat 1 10
# cs 列：每秒上下文切换次数
# 正常值：几百到几千
# 异常值：几万到几十万（性能问题）

# sar - 系统活动报告
sar -w 1 10
# cswch/s：每秒总上下文切换次数
# proc/s：每秒创建的进程数

# top - 实时监控
top
# 按 1 查看每个 CPU
# %sy 高：系统态占用高，可能是频繁切换
```

**2. 查看进程级别**

```bash
# pidstat - 进程统计
pidstat -w -p <pid> 1

# 输出解释：
# cswch/s：自愿上下文切换（voluntary context switches）
#   - 进程主动放弃 CPU（等待 I/O、睡眠、等待锁）
#   - 数值高：可能是 I/O 密集或锁竞争严重
#
# nvcswch/s：非自愿上下文切换（non-voluntary context switches）
#   - 时间片用完被强制切换
#   - 数值高：可能是 CPU 竞争激烈或线程过多

# 示例分析：
# Process: nginx worker
# cswch/s: 1000   → 高，可能在频繁等待网络 I/O
# nvcswch/s: 10   → 低，正常

# Process: CPU 密集型任务
# cswch/s: 5      → 低，很少主动让出 CPU
# nvcswch/s: 100  → 高，被频繁抢占（可能需要更多 CPU 或降低其他进程优先级）
```

**3. 使用 perf 分析**

```bash
# 记录上下文切换事件
perf record -e sched:sched_switch -a -g sleep 10

# 查看报告
perf report

# 实时查看
perf top -e sched:sched_switch

# 查看特定进程
perf record -e sched:sched_switch -p <pid> sleep 10

# 分析 CPU 迁移（进程在不同 CPU 间切换）
perf stat -e context-switches,cpu-migrations -p <pid> sleep 10
```

**4. 使用 eBPF/bpftrace（高级应用）**

```bash
# 1. 基础统计 - 每个进程的上下文切换次数
bpftrace -e '
    tracepoint:sched:sched_switch {
        @[args->prev_comm] = count();
    }
'

# 2. 上下文切换延迟分析（直方图）
bpftrace -e '
    tracepoint:sched:sched_switch {
        @start[args->next_pid] = nsecs;
    }
    
    tracepoint:sched:sched_switch /@start[args->prev_pid]/ {
        $duration = nsecs - @start[args->prev_pid];
        @latency = hist($duration);
        delete(@start[args->prev_pid]);
    }
'

# 3. 进程间切换关系热力图
bpftrace -e '
    tracepoint:sched:sched_switch {
        @switch[args->prev_comm, args->next_comm] = count();
    }
'

# 4. 与进程状态关联的上下文切换分析
bpftrace -e '
    tracepoint:sched:sched_switch {
        # prev_state: 0=running, 1=sleeping, 2=disk sleep, 4=zombie
        $prev_state = args->prev_state;
        @state_count[$prev_state] = count();
        @state_switch[$prev_state, args->prev_comm] = count();
    }
'

# 5. 与系统调用关联的上下文切换
bpftrace -e '
    tracepoint:syscalls:sys_enter_* {
        @syscall_start[tid] = nsecs;
        @syscall_name[tid] = args->__syscall_name;
    }
    
    tracepoint:syscalls:sys_exit_* {
        if (@syscall_start[tid]) {
            $duration = nsecs - @syscall_start[tid];
            @syscall_latency[@syscall_name[tid]] = hist($duration);
            delete(@syscall_start[tid]);
            delete(@syscall_name[tid]);
        }
    }
    
    tracepoint:sched:sched_switch {
        if (@syscall_start[tid]) {
            @syscall_interrupted[@syscall_name[tid]] = count();
        }
    }
'

# 6. 使用 BCC 工具进行深度分析
# 安装：apt-get install bcc-tools 或 yum install bcc-tools

# 分析上下文切换的调用栈
sched_switch -p <pid> -D 10

# 查看系统调用与上下文切换的关系
syscount -p <pid> -d

# 分析调度延迟
runqlat -p <pid>

# 7. 实时监控与可视化
bpftrace -e '
    BEGIN {
        print("@switch_count[prev_comm, next_comm]\n");
    }
    
    tracepoint:sched:sched_switch {
        @switch_count[args->prev_comm, args->next_comm] = count();
        clear(@switch_count);
        print(@switch_count);
    }
'

# 8. 基于 eBPF 的自定义监控
# 以下是一个简单的 BCC Python 脚本示例
exec("""
from bcc import BPF

# eBPF 程序
prog = """
#include <linux/sched.h>

struct switch_data {
    u64 timestamp;
    char prev_comm[TASK_COMM_LEN];
    char next_comm[TASK_COMM_LEN];
    u32 prev_pid;
    u32 next_pid;
    u32 prev_state;
};

BPF_HASH(switch_events);

TRACEPOINT_PROBE(sched, sched_switch) {
    struct switch_data data = {};
    data.timestamp = bpf_ktime_get_ns();
    bpf_probe_read_kernel_str(&data.prev_comm, sizeof(data.prev_comm), 
                              args->prev_comm);
    bpf_probe_read_kernel_str(&data.next_comm, sizeof(data.next_comm), 
                              args->next_comm);
    data.prev_pid = args->prev_pid;
    data.next_pid = args->next_pid;
    data.prev_state = args->prev_state;
    
    // 发送到用户空间
    bpf_trace_printk("%s -> %s, %d -> %d, state: %d", 
                     data.prev_comm, data.next_comm, 
                     data.prev_pid, data.next_pid, 
                     data.prev_state);
    
    return 0;
}
"""

# 加载并运行
b = BPF(text=prog)
print("Tracing context switches... Hit Ctrl-C to end.")
print("Prev -> Next, Prev PID -> Next PID, State")

# 打印结果
try:
    b.trace_print()
except KeyboardInterrupt:
    pass
""")
```

**eBPF 分析的优势**：
- **低开销**：内核态直接处理，无需复制大量数据到用户态
- **实时性**：可以实时监控和分析
- **可定制性**：可以根据需要编写自定义的分析程序
- **深度分析**：可以关联系统调用、进程状态、CPU 事件等多维度信息
- **安全可靠**：运行在内核沙箱中，不会影响系统稳定性

**5. 分析案例**

```bash
# 场景：系统响应缓慢

# 步骤 1：检查系统整体
$ vmstat 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
25  0      0 123456  12345 789012    0    0     0     0 5000 50000 60 30 10  0  0
                                                             ^^^^^
                                                             异常高！

# 步骤 2：找出罪魁祸首
$ pidstat -w 1
03:00:01 PM   UID       PID   cswch/s nvcswch/s  Command
03:00:02 PM     0      1234      5000       100  my_app
                                ^^^^
                                异常高的自愿切换

# 步骤 3：分析原因
$ strace -c -p 1234
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 98.50    0.123456        1234       100           futex
  0.80    0.001000          10       100           read
...
# futex 调用多 → 锁竞争严重

# 步骤 4：深入分析
$ perf record -e sched:sched_switch -p 1234 -g sleep 10
$ perf report
# 查看调用栈，找到频繁切换的代码位置

# 步骤 5：解决方案
# - 减少锁粒度
# - 使用无锁数据结构
# - 减少线程数量
```

---

## 不同调度策略对上下文切换的影响

**Linux 调度策略**：

```c
// 1. SCHED_NORMAL (SCHED_OTHER)
// 普通进程，CFS 调度
// 特点：公平分配 CPU 时间

// 2. SCHED_FIFO
// 实时进程，先进先出
// 特点：运行直到阻塞或主动让出，不会被同优先级抢占
sched_setscheduler(pid, SCHED_FIFO, &param);

// 3. SCHED_RR
// 实时进程，轮转
// 特点：有时间片，时间片用完后轮转到同优先级队列末尾

// 4. SCHED_BATCH
// 批处理进程
// 特点：CPU 密集型，减少抢占，降低切换频率

// 5. SCHED_IDLE
// 空闲进程
// 特点：只在 CPU 空闲时运行

// 6. SCHED_DEADLINE
// 截止时间调度
// 特点：保证在截止时间前完成
```

**设置调度策略**：

```c
#include <sched.h>

// 设置为批处理（减少切换）
struct sched_param param;
param.sched_priority = 0;
sched_setscheduler(0, SCHED_BATCH, &param);

// 设置为实时（减少被抢占）
param.sched_priority = 50;  // 1-99
sched_setscheduler(0, SCHED_FIFO, &param);
```

```bash
# 使用 chrt 命令
chrt -b 0 ./my_batch_program    # 批处理
chrt -f 50 ./my_rt_program      # 实时 FIFO
chrt -r 50 ./my_rt_program      # 实时 RR

# 查看进程的调度策略
chrt -p <pid>
```

---

### 相关高频面试题

## Q1: 进程切换和线程切换的开销有什么区别？

**答案**：

**主要区别**：

| 项目         | 进程切换                       | 线程切换（同进程） |
| ------------ | ------------------------------ | ------------------ |
| 虚拟内存空间 | 需要切换（切换页表、刷新 TLB） | 共享，不需要切换   |
| 页表切换     | 需要（更新 CR3 寄存器）        | 不需要             |
| TLB 刷新     | 需要（地址空间变化）           | 不需要             |
| CPU 缓存     | 大量失效                       | 部分失效           |
| 文件描述符   | 独立                           | 共享               |
| 全局变量/堆  | 独立                           | 共享               |
| 开销         | 1-10 微秒                      | 0.1-1 微秒         |

**详细说明**：

```
进程切换需要：
1. 保存/恢复所有寄存器
2. 切换页表（CR3 寄存器）
3. 刷新 TLB（全部或部分）
4. 切换内核栈
5. CPU 缓存大量失效（L1/L2/L3）

同进程内线程切换只需要：
1. 保存/恢复 CPU 寄存器
2. 切换栈指针（SP）
3. 少量 CPU 缓存失效（栈数据）

关键：页表切换和 TLB 刷新是最大开销来源
```

**测试代码**：

```c
// 测量线程切换时间
#define _GNU_SOURCE
#include <pthread.h>
#include <time.h>
#include <stdio.h>

volatile int flag = 0;

void* thread_func(void* arg) {
    for (int i = 0; i < 1000000; i++) {
        while (flag != (int)arg);
        flag = 1 - (int)arg;
    }
    return NULL;
}

int main() {
    pthread_t t1, t2;
    struct timespec start, end;
    
    pthread_create(&t1, NULL, thread_func, (void*)0);
    pthread_create(&t2, NULL, thread_func, (void*)1);
    
    clock_gettime(CLOCK_MONOTONIC, &start);
    pthread_join(t1, NULL);
    pthread_join(t2, NULL);
    clock_gettime(CLOCK_MONOTONIC, &end);
    
    long ns = (end.tv_sec - start.tv_sec) * 1000000000L + (end.tv_nsec - start.tv_nsec);
    printf("平均切换时间: %ld ns\n", ns / 2000000);
    return 0;
}
```

## Q2: 什么情况下会发生大量的上下文切换？如何优化？

**答案**：

**常见导致大量切换的场景**：

**1. 线程/进程过多**
```
问题：100 个线程竞争 4 个 CPU 核心
现象：大量非自愿切换（nvcswch/s 高）
优化：使用线程池，限制并发数量
```

**2. 锁竞争严重**
```
问题：多个线程频繁争抢同一个锁
现象：大量自愿切换（cswch/s 高）
优化：
- 减小锁粒度
- 使用读写锁
- 使用无锁数据结构
- 使用原子操作
```

**3. I/O 密集型应用**
```
问题：频繁的文件读写、网络请求
现象：大量自愿切换（等待 I/O）
优化：
- 使用异步 I/O（io_uring、epoll）
- 批量处理
- 使用缓冲
```

**4. 不合理的睡眠/唤醒**
```c
// 不好：忙等待
while (!condition) {
    usleep(1000);  // 频繁睡眠唤醒
}

// 好：条件变量
pthread_mutex_lock(&mutex);
while (!condition) {
    pthread_cond_wait(&cond, &mutex);  // 高效等待
}
pthread_mutex_unlock(&mutex);
```

**5. 频繁的系统调用**
```c
// 不好：频繁系统调用
for (int i = 0; i < 1000000; i++) {
    gettimeofday(&tv, NULL);  // 每次都陷入内核
}

// 好：使用 vDSO（虚拟动态共享对象）
clock_gettime(CLOCK_MONOTONIC_COARSE, &ts);  // 用户态读取，无系统调用

// 或批量处理
```

**优化检查清单**：
```bash
# 1. 检查线程数
ps -eLf | grep my_app | wc -l

# 2. 检查锁竞争
perf record -e lock:lock_acquire -p <pid>
perf report

# 3. 检查系统调用
strace -c -p <pid>

# 4. 检查 I/O 等待
iostat -x 1

# 5. 调整调度策略
chrt -b 0 ./my_app  # 批处理，减少抢占
```

## Q3: TLB 是什么？为什么上下文切换要刷新 TLB？

**答案**：

**TLB（Translation Lookaside Buffer）**：

```
定义：地址转换后备缓冲区
作用：缓存虚拟地址到物理地址的映射
位置：CPU 内部，速度极快

工作原理：
┌─────────────────────────────────────────┐
│ CPU 访问虚拟地址 0x1000                  │
└────────────┬────────────────────────────┘
             ↓
    ┌────────────────┐
    │ 查询 TLB       │
    └───┬────────┬───┘
        │        │
   命中 │        │ 未命中
        ↓        ↓
   直接获取   查询页表（慢）
   物理地址   ↓
              更新 TLB
              ↓
              获取物理地址

TLB 命中：1-2 个时钟周期
TLB 未命中：几十到几百个时钟周期（需要查页表）
```

**为什么要刷新 TLB**：

```
原因：不同进程有不同的虚拟地址空间

例子：
进程 A：虚拟地址 0x1000 → 物理地址 0xA000
进程 B：虚拟地址 0x1000 → 物理地址 0xB000
        (相同虚拟地址，不同物理地址)

如果不刷新 TLB：
1. 进程 A 运行，TLB 缓存：0x1000 → 0xA000
2. 切换到进程 B
3. 进程 B 访问 0x1000
4. TLB 命中，返回 0xA000（错误！应该是 0xB000）
5. 访问了进程 A 的内存 → 严重错误

因此必须刷新 TLB，确保地址转换正确
```

**TLB 刷新的代价**：

```
影响：
1. 所有 TLB 条目失效
2. 进程 B 开始时，每次地址访问都 TLB miss
3. 需要查询页表（慢）
4. 逐渐重新填充 TLB

示例：
TLB 大小：64 条目
页大小：4KB
覆盖范围：64 × 4KB = 256KB

进程工作集：10MB
TLB miss 率：(10MB - 256KB) / 10MB ≈ 97.5%

性能影响：
正常内存访问：5 ns
TLB miss：50 ns（10 倍慢）
```

**优化策略**：

```
1. 使用大页（Huge Pages）
   - 普通页：4KB
   - 大页：2MB 或 1GB
   - TLB 条目不变，覆盖范围大大增加

启用大页：
echo 1024 > /proc/sys/vm/nr_hugepages  # 2GB

程序中使用：
void* ptr = mmap(NULL, size, PROT_READ|PROT_WRITE,
                 MAP_PRIVATE|MAP_ANONYMOUS|MAP_HUGETLB, -1, 0);

2. PCID（Process Context Identifier）
   - 现代 x86 CPU 支持
   - TLB 条目带进程标识
   - 切换时不需要完全刷新
   - 只刷新当前进程的条目

3. CPU 亲和性
   - 进程绑定到固定 CPU
   - 减少 CPU 迁移
   - 保持 TLB 热度
```

## Q4: 如何测量上下文切换对程序性能的实际影响？

**答案**：

**测量方法**：

**1. 基准测试对比**

```c
// benchmark.c - 对比有无上下文切换的性能
#include <stdio.h>
#include <pthread.h>
#include <unistd.h>
#include <time.h>

#define ITERATIONS 10000000

// 单线程计算（无切换）
void single_thread_work() {
    long sum = 0;
    for (int i = 0; i < ITERATIONS; i++) {
        sum += i;
    }
}

// 多线程计算（有切换）
void* thread_work(void* arg) {
    long sum = 0;
    for (int i = 0; i < ITERATIONS / 4; i++) {
        sum += i;
    }
    return NULL;
}

void multi_thread_work() {
    pthread_t threads[4];
    for (int i = 0; i < 4; i++) {
        pthread_create(&threads[i], NULL, thread_work, NULL);
    }
    for (int i = 0; i < 4; i++) {
        pthread_join(threads[i], NULL);
    }
}

int main() {
    struct timespec start, end;
    
    // 测试单线程
    clock_gettime(CLOCK_MONOTONIC, &start);
    single_thread_work();
    clock_gettime(CLOCK_MONOTONIC, &end);
    long single = (end.tv_sec - start.tv_sec) * 1000000000L + 
                  (end.tv_nsec - start.tv_nsec);
    
    // 测试多线程
    clock_gettime(CLOCK_MONOTONIC, &start);
    multi_thread_work();
    clock_gettime(CLOCK_MONOTONIC, &end);
    long multi = (end.tv_sec - start.tv_sec) * 1000000000L + 
                 (end.tv_nsec - start.tv_nsec);
    
    printf("单线程: %ld ns\n", single);
    printf("多线程: %ld ns\n", multi);
    printf("差异: %ld ns (%.2f%%)\n", multi - single, 
           (multi - single) * 100.0 / single);
    
    return 0;
}
```

**2. 使用 perf 测量**

```bash
# 测量上下文切换次数和开销
perf stat -e context-switches,cpu-clock,task-clock ./my_program

# 输出示例：
# Performance counter stats for './my_program':
#
#         12,345      context-switches          #    0.123 K/sec
#     10,000.00 msec cpu-clock                 #    1.000 CPUs utilized
#     10,000.00 msec task-clock                #    1.000 CPUs utilized
#
#      10.000123456 seconds time elapsed

# 分析：
# 每秒 123 次切换
# 总运行时间 10 秒
# 总切换次数 1234
# 平均每次切换 = 10s / 1234 ≈ 8ms（包含切换和运行时间）
```

**3. 使用 pidstat 监控**

```bash
# 运行程序时监控
pidstat -w -p $(pgrep my_program) 1

# 同时监控 CPU 使用
pidstat -u -w -p $(pgrep my_program) 1

# 分析：
# %usr 高，nvcswch/s 高 → CPU 竞争，考虑减少线程
# %system 高，cswch/s 高 → 系统调用频繁或锁竞争
```

**4. 计算切换成本**

```python
#!/usr/bin/env python3
# analyze_context_switch.py

import subprocess
import re
import time

def get_context_switches(pid):
    """获取进程的上下文切换次数"""
    with open(f'/proc/{pid}/status') as f:
        for line in f:
            if line.startswith('voluntary_ctxt_switches'):
                vol = int(line.split()[1])
            elif line.startswith('nonvoluntary_ctxt_switches'):
                nonvol = int(line.split()[1])
    return vol, nonvol

def measure_impact(pid, duration=10):
    """测量上下文切换对性能的影响"""
    # 获取初始值
    vol1, nonvol1 = get_context_switches(pid)
    start_time = time.time()
    
    # 获取初始 CPU 时间
    with open(f'/proc/{pid}/stat') as f:
        fields = f.read().split()
        utime1 = int(fields[13])
        stime1 = int(fields[14])
    
    time.sleep(duration)
    
    # 获取最终值
    vol2, nonvol2 = get_context_switches(pid)
    end_time = time.time()
    
    with open(f'/proc/{pid}/stat') as f:
        fields = f.read().split()
        utime2 = int(fields[13])
        stime2 = int(fields[14])
    
    # 计算
    total_cs = (vol2 - vol1) + (nonvol2 - nonvol1)
    cpu_ticks = (utime2 - utime1) + (stime2 - stime1)
    real_time = end_time - start_time
    
    # Linux 的 sysconf(_SC_CLK_TCK) 通常是 100
    cpu_time = cpu_ticks / 100.0
    
    print(f"监控时长: {real_time:.2f} 秒")
    print(f"上下文切换: {total_cs} 次 ({total_cs/real_time:.2f}/秒)")
    print(f"  自愿: {vol2-vol1}")
    print(f"  非自愿: {nonvol2-nonvol1}")
    print(f"CPU 时间: {cpu_time:.2f} 秒")
    print(f"CPU 利用率: {cpu_time/real_time*100:.2f}%")
    
    if total_cs > 0:
        avg_cs_cost = (real_time - cpu_time) / total_cs
        print(f"平均切换成本（估算）: {avg_cs_cost*1000000:.2f} 微秒")

if __name__ == '__main__':
    import sys
    if len(sys.argv) != 2:
        print(f"用法: {sys.argv[0]} <pid>")
        sys.exit(1)
    
    pid = int(sys.argv[1])
    measure_impact(pid)
```

**5. 对比优化前后**

```bash
# 优化前：过多线程
./my_app --threads=100 &
APP_PID=$!
perf stat -e context-switches -p $APP_PID sleep 10
# 结果：50,000 次切换

# 优化后：合理线程数
./my_app --threads=4 &
APP_PID=$!
perf stat -e context-switches -p $APP_PID sleep 10
# 结果：1,000 次切换

# 性能提升计算：
# 假设每次切换 5 微秒
# 节省：(50000 - 1000) × 5μs = 245ms / 10s = 2.45% 性能提升
```

## Q5: 用户态切换和内核态切换有什么区别？

**答案**：

**内核态上下文切换**（进程/线程切换）：

```
特点：
- 由内核调度器管理
- 需要切换到内核态（Ring 0）
- 保存完整的进程/线程状态
- 切换页表和地址空间（进程切换）
- 时间：1-10 微秒

触发：
- 时间片用完
- 系统调用阻塞（read/write/sleep）
- 等待锁
- 中断

过程：
1. 陷入内核（用户态 → 内核态）
2. 保存当前进程状态
3. 调度算法选择下一个进程
4. 切换地址空间（进程切换）
5. 恢复新进程状态
6. 返回用户态
```

**用户态上下文切换**（协程/纤程）：

```
特点：
- 由用户态调度器管理（如 Go runtime）
- 不需要陷入内核
- 只保存/恢复寄存器和栈
- 不切换地址空间
- 时间：10-100 纳秒（快 100 倍）

实现：
- 协程库（libco、boost.coroutine）
- 语言内置（Go goroutine、Python asyncio）
- 手动实现（setjmp/longjmp、ucontext）

局限：
- 阻塞性系统调用会阻塞整个线程
- 需要配合非阻塞 I/O
- 无法利用多核（需配合多线程）
```

**对比示例**：

```c
// 内核态切换（pthread）
#include <pthread.h>

void* thread_func(void* arg) {
    // 执行工作
    return NULL;
}

int main() {
    pthread_t t;
    pthread_create(&t, NULL, thread_func, NULL);  // 创建内核线程
    pthread_join(&t, NULL);  // 等待（可能触发内核调度）
    return 0;
}

// 每次调度都要陷入内核
```

```go
// 用户态切换（goroutine）
package main

func worker() {
    // 执行工作
}

func main() {
    go worker()  // 创建 goroutine（用户态）
    // Go 运行时在用户态调度，不陷入内核
}

// M:N 模型：M 个 goroutine 复用 N 个内核线程
```

**性能对比**：

```
创建开销：
- 内核线程：~10 微秒，栈大小 1-8 MB
- 协程：~100 纳秒，栈大小 2-4 KB（按需增长）

切换开销：
- 内核线程：1-10 微秒
- 协程：10-100 纳秒

并发能力：
- 内核线程：数千（受内存限制）
- 协程：百万级（轻量级）

适用场景：
- 内核线程：CPU 密集型，需要利用多核
- 协程：I/O 密集型，大量并发连接
```

**混合使用**：

```
最佳实践：M:N 线程模型
- M 个用户态协程
- N 个内核线程（通常 = CPU 核心数）
- 用户态调度器将协程分配到内核线程

例如 Go 的 GMP 模型：
G (Goroutine)：用户态协程
M (Machine)：内核线程
P (Processor)：逻辑处理器，持有 G 的队列

工作流程：
1. 创建大量 goroutine（G）
2. Go runtime 调度 G 到 P
3. P 绑定到 M（内核线程）运行
4. G 阻塞时，M 可以切换到其他 G（用户态切换）
5. 系统调用阻塞时，M 与 P 解绑，新 M 接管 P

优势：
- 用户态快速切换
- 利用多核（多个 M）
- 避免阻塞整个程序
```

---

### 关键点总结

**上下文切换核心**：
- **定义**：保存当前任务状态，恢复新任务状态
- **组成**：CPU 寄存器、页表、内核栈、程序计数器
- **开销**：进程 > 线程 > 系统调用 > 协程

**触发条件**：
- 时间片用完（被动）
- 阻塞等待（主动）
- 中断处理
- 优先级抢占

**优化策略**：
- 减少线程数（使用线程池）
- 减少锁竞争（无锁编程）
- 使用异步 I/O
- CPU 亲和性绑定
- 使用协程（用户态调度）

**监控工具**：
```bash
vmstat      # 系统整体
pidstat -w  # 进程级别
perf stat   # 性能分析
bpftrace    # eBPF 追踪
```

**关键指标**：
- `cswch/s`：自愿切换（I/O/锁等待）
- `nvcswch/s`：非自愿切换（CPU 竞争）
- 正常值：几百到几千/秒
- 异常值：几万以上/秒