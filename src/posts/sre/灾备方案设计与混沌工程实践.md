---
date: 2026-02-13
author: Gaaming Zhang
isOriginal: true
article: true
category:
  - SRE
tag:
  - SRE
  - 灾备
  - 混沌工程
  - 高可用
---

# 灾备方案设计与混沌工程实践

## 从一次惨痛的故障说起

某电商平台的大促夜，机房突发断电，主集群全线不可用。运维团队翻出半年前写的"灾备方案文档"，手忙脚乱地执行切换流程——DNS 修改需要等待 TTL 过期，数据库主从延迟导致部分订单数据不一致，备用集群因为长期未验证而有多个服务启动失败。最终系统恢复用了 4 个小时，损失数千万。

事后复盘，这个团队不是没有灾备方案，而是从未演练过。**一个从未被验证的灾备方案，和没有方案没有本质区别。**

这就是灾备设计和混沌工程要共同解决的问题：前者建立系统韧性的架构基础，后者通过主动制造故障来持续验证这个基础是否真实有效。

## RTO 和 RPO：灾备的核心度量

在讨论任何灾备方案之前，必须先明确两个数字。

**RTO（Recovery Time Objective，恢复时间目标）** 定义的是从灾难发生到系统恢复可用，业务能接受的最长时间。如果 RTO 是 30 分钟，意味着无论发生什么故障，你必须在 30 分钟内让系统重新对外提供服务。

**RPO（Recovery Point Objective，恢复点目标）** 定义的是灾难发生后，允许丢失的最大数据量，通常以时间窗口衡量。如果 RPO 是 1 小时，意味着系统可以接受回滚到故障发生前 1 小时的数据状态。

两个指标都越小越好，但代价是指数级增长的成本。

```
成本
 ↑
高|  ★ 双活（RTO≈0, RPO≈0）
  |
  |      ★ 热备（RTO<5min, RPO<1min）
  |
  |           ★ 温备（RTO<30min, RPO<15min）
低|                  ★ 冷备（RTO<4h, RPO<24h）
  +────────────────────────────→ RTO/RPO（越小越好）
```

:::tip 如何确定 RTO 和 RPO
RTO 和 RPO 不是技术团队单方面决定的，而是业务方和技术团队共同协商的结果。核心问题是：**每小时故障对业务造成多少损失？** 用这个损失对比灾备方案的建设和运维成本，找到合理的平衡点。支付核心系统和内部报表系统的 RTO/RPO 要求天差地别。
:::

不同业务级别的参考值：

| 业务级别 | 典型场景 | RTO 目标 | RPO 目标 | 灾备模式 |
|----------|----------|----------|----------|----------|
| 核心（P0） | 支付、交易核心链路 | < 5 分钟 | ≈ 0 | 双活 |
| 重要（P1） | 用户登录、商品展示 | < 30 分钟 | < 5 分钟 | 热备 |
| 一般（P2） | 订单查询、报表 | < 2 小时 | < 1 小时 | 温备 |
| 边缘（P3） | 内部工具、日志平台 | < 24 小时 | < 24 小时 | 冷备 |

## 灾备架构模式

### Active-Active（双活）

两个（或多个）数据中心同时承载生产流量，互为备份。任意一个机房故障，流量切换到另一个机房，理论上 RTO 趋近于 0。

```
用户请求
    ↓
 负载均衡（全局流量调度）
  ↙          ↘
机房 A        机房 B
（50% 流量）  （50% 流量）
    ↕  数据双向同步  ↕
  数据层        数据层
```

双活的最大挑战是**数据冲突**。两个机房同时写入时，同一条记录可能被并发修改。常见的解决策略是**用户分片路由**：将用户按某个维度（如 user_id % 2）路由到固定机房，每个用户的数据只在一个机房写入，从而天然避免写冲突。但这种方案在机房故障时需要重新路由，会有短暂的写入限制窗口。

### Active-Passive（主备）

主机房承载全部流量，备机房实时接收数据同步，但不对外提供服务。主机房故障时，将流量切换到备机房。

切换耗时 = 故障检测时间 + 决策时间 + 切换执行时间。实际生产中，这个时间通常在 5~30 分钟之间，主要瓶颈在于**如何快速、准确地判断主机房已经真正故障**（而非网络抖动误判）。

### Pilot Light（最小核心常备）

备用环境只保持最核心的服务处于运行状态（如数据库），其他服务组件处于关闭状态，灾难发生时才快速启动。

适合对成本敏感、但希望缩短冷启动时间的场景。核心数据库始终同步，非核心的应用服务可以通过 IaC（Terraform、Helm）在几分钟内拉起，比纯冷备显著缩短 RTO。

### Warm Standby（温备）

备用环境持续运行，但规模缩小（如生产的 1/4 资源），能承载部分流量。灾难发生后，扩容到完整规模接管所有流量。温备是成本与 RTO 之间的折衷方案，适合大多数中等业务重要性的系统。

**四种模式对比：**

| 模式 | RTO | RPO | 备用环境成本 | 复杂度 | 数据冲突风险 |
|------|-----|-----|-------------|--------|-------------|
| 双活 | ≈ 0 | ≈ 0 | 100%（与生产等同） | 极高 | 有，需处理 |
| 主备 | 5~30 分钟 | 秒级 | 50%~80% | 中 | 无 |
| Pilot Light | 30 分钟~2 小时 | 分钟级 | 10%~20% | 低 | 无 |
| 温备 | 15~30 分钟 | 分钟级 | 30%~50% | 中低 | 无 |

## Kubernetes 多集群灾备设计

### 同城双活架构

两个物理上隔离的机房（不同供电、不同网络接入），部署两套 Kubernetes 集群，通过全局负载均衡（如 F5 GTM、AWS Route 53 健康检查路由）分发流量。

```
┌──────────────────────────────────────────────────────┐
│              全局流量调度（GSLB/DNS）                 │
│         健康检查 + 权重路由 + 故障自动摘除             │
└──────────────┬─────────────────────┬─────────────────┘
               │                     │
        ┌──────▼──────┐       ┌──────▼──────┐
        │  机房 A      │       │  机房 B      │
        │  K8s Cluster │       │  K8s Cluster │
        │  50% 流量    │       │  50% 流量    │
        └──────┬───────┘       └──────┬───────┘
               │   数据双向同步         │
        ┌──────▼───────────────────────▼──────┐
        │         分布式数据库 / 中间件         │
        │   MySQL Group Replication / Galera   │
        │   Redis Cluster（跨机房模式）         │
        │   Kafka MirrorMaker 2                │
        └──────────────────────────────────────┘
```

### 异地灾备架构

主集群在 A 城市，灾备集群在 B 城市（通常相距 200 公里以上，避免区域性灾难同时影响两地）。日常所有流量走主集群，灾备集群只做数据同步不对外提供服务。

异地灾备的核心挑战是**网络延迟**。跨城市的数据同步延迟通常在 10~50ms，这意味着：
- 数据库同步天然存在延迟，RPO 不可能为 0
- 同步方式通常采用异步复制，以避免跨城延迟拖慢主库写入

### 有状态服务的灾备挑战

无状态服务（如 API 应用）的灾备相对简单：镜像同步到备用集群，Helm Chart 重新 apply 即可。有状态服务才是真正的难题。

**MySQL 灾备**：主从异步复制是基础，GTID 模式能更精确地跟踪复制位点，便于切换后的一致性校验。关键指标是 `Seconds_Behind_Master`，需要持续监控确保延迟不超过 RPO 目标。

**Redis 灾备**：Sentinel 模式在同城双活中可以跨机房部署 Sentinel 节点（奇数个，避免脑裂），但 Redis 本身的复制是异步的，主节点故障时最近写入的数据存在丢失风险。对于 RPO 要求极高的场景，需要在业务层面实现双写逻辑。

**Kafka 灾备**：MirrorMaker 2 是目前最成熟的跨集群 Topic 同步方案，支持偏移量映射，消费者切换到备集群后能从正确的位置继续消费，而非从头重放。

### 灾备切换流程

```
┌────────────────────────────────────────────────────┐
│               灾备切换流程                           │
│                                                    │
│  1. 故障检测（自动）                                │
│     监控系统：HTTP/TCP 探测失败 > N 次               │
│     告警触发 → 值班工程师收到通知                    │
│                                                    │
│  2. 决策确认（人工/自动）                            │
│     确认是真实故障而非误报                           │
│     确认切换时机（是否有正在进行的事务？）            │
│                                                    │
│  3. 数据层切换                                      │
│     停止主库写入（防止新数据写入不一致）              │
│     等待备库追平同步延迟                             │
│     将备库提升为主库                                │
│                                                    │
│  4. 流量切换                                        │
│     DNS 切换（注意 TTL，提前降低 TTL 到 60s）        │
│     或负载均衡层摘除故障节点                         │
│     或业务层配置更新（连接池指向新地址）              │
│                                                    │
│  5. 验证                                           │
│     核心接口冒烟测试                                │
│     监控面板确认流量和错误率正常                     │
└────────────────────────────────────────────────────┘
```

:::warning DNS 切换的 TTL 陷阱
DNS 切换的生效时间取决于 TTL 值。如果 TTL 是 3600 秒（1 小时），切换 DNS 后，客户端的 DNS 缓存可能还会持续 1 小时指向旧地址。**灾备演练前，应将关键域名的 TTL 降低到 60 秒，并保持这个配置**，而不是故障发生后才临时修改——那时修改已经来不及。
:::

## 混沌工程：主动发现系统弱点

### 为什么需要主动制造故障

软件系统的脆弱性往往隐藏在正常运行的表象之下。重试逻辑是否真的有效？熔断器的阈值配置是否合理？某个依赖服务超时，调用方是否会优雅降级？

这些问题，**只有在故障发生时才会暴露**——要么你主动创造故障来发现它，要么等真实故障找上门来。

Netflix 在向云端迁移的过程中，意识到分布式系统的故障模式无法通过静态测试覆盖，于是创造了 Chaos Monkey：一个在生产环境随机终止虚拟机的工具，强迫团队构建真正的高可用架构。这个工具演化成了完整的混沌工程理念。

### 混沌工程的四步循环

```
┌──────────────────────────────────────────────────────┐
│                 混沌工程四步法                         │
│                                                      │
│  Step 1: 建立稳态假设                                 │
│  定义"正常"是什么：SLO 指标（P99 延迟 < 200ms,        │
│  错误率 < 0.1%）、业务指标（每秒订单数 > 100）         │
│                                                      │
│  Step 2: 假设实验变量                                 │
│  "如果某个 Pod 被随机杀死，稳态会被打破吗？"           │
│  "如果数据库延迟增加 500ms，API 超时会级联吗？"        │
│                                                      │
│  Step 3: 执行实验（控制爆炸半径）                      │
│  从最小影响范围开始：测试环境 → 灰度 → 生产            │
│  设置实验时间窗口，准备好一键回滚                      │
│                                                      │
│  Step 4: 验证和改进                                   │
│  稳态是否被打破？打破了说明发现了系统弱点               │
│  没有打破说明这个弱点已被正确处理                      │
│  记录结果 → 修复弱点 → 纳入回归实验集                  │
└──────────────────────────────────────────────────────┘
```

:::tip 混沌工程不是破坏测试
混沌工程的目标不是"搞坏系统"，而是**在可控条件下验证系统的韧性假设**。每个实验都应该有明确的假设、边界控制和回滚预案。实验前应确保监控完善，能够观察到实验的效果，否则你只是在盲目制造混乱。
:::

## Chaos Mesh 实战

Chaos Mesh 是 CNCF 孵化的开源混沌工程平台，以 Kubernetes CRD 的方式定义混沌实验，是目前 Kubernetes 生态中最成熟的混沌工程工具。

### 核心 CRD 体系

```
Chaos Mesh CRD 分类：

  故障注入类：
    PodChaos     → Pod 级别（杀 Pod、暂停容器）
    NetworkChaos → 网络级别（延迟、丢包、断网）
    StressChaos  → 资源压力（CPU/内存压力）
    IOChaos      → 文件 I/O 级别（延迟、错误注入）
    HTTPChaos    → HTTP 层（修改请求/响应）
    DNSChaos     → DNS 解析异常

  实验编排类：
    Workflow     → 多步骤实验编排（串行/并行）
    Schedule     → 定时执行混沌实验
```

### PodChaos：验证 Pod 自愈能力

最基础的实验：随机杀死目标命名空间中的 Pod，验证 Kubernetes 的自愈能力和应用的无状态设计。

```yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: pod-kill-order-service
  namespace: chaos-testing
spec:
  action: pod-kill          # 杀死 Pod（还有 pod-failure、container-kill）
  mode: random-max-percent  # 随机比例杀死
  value: "30"               # 最多杀死 30% 的匹配 Pod
  selector:
    namespaces:
      - production
    labelSelectors:
      app: order-service    # 只影响 order-service 的 Pod
  duration: "5m"            # 实验持续 5 分钟后自动停止
  scheduler:
    cron: "@every 10m"      # 每 10 分钟执行一次（可选，不配置则手动触发）
```

实验期间，观察以下指标：
- Deployment 的就绪副本数是否在 30 秒内恢复到期望值
- 服务的错误率是否在可接受范围内（SLO 有没有被打破）
- 如果错误率飙升，说明负载均衡没有足够快地摘除不健康的 Pod，需要调整就绪探针配置

### NetworkChaos：验证超时和重试机制

网络故障是分布式系统中最常见的故障类型，也最难复现。NetworkChaos 可以精确注入延迟、丢包、网络分区等场景。

```yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: network-delay-payment-to-db
  namespace: chaos-testing
spec:
  action: delay             # 注入延迟（还有 loss、duplicate、corrupt、partition）
  mode: all                 # 影响所有匹配的 Pod
  selector:
    namespaces:
      - production
    labelSelectors:
      app: payment-service  # 对 payment-service 的出站流量注入延迟
  delay:
    latency: "500ms"        # 增加 500ms 延迟
    correlation: "50"       # 相关性 50%（模拟真实抖动，非所有包都延迟）
    jitter: "100ms"         # ±100ms 的抖动
  direction: to             # 出站方向（from/to/both）
  externalTargets:
    - "mysql.production.svc.cluster.local"  # 只影响到 MySQL 的流量
  duration: "10m"
```

:::danger 网络分区实验的风险
`action: partition` 会完全切断两组 Pod 之间的网络连接，可能导致分布式锁超时、事务中断、脑裂等严重后果。**这类实验必须在非生产环境或业务低峰期执行**，且必须提前确认有应急回滚能力（`kubectl delete networkchaos` 即可立即恢复）。
:::

### StressChaos：验证资源隔离

```yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: StressChaos
metadata:
  name: cpu-stress-api-gateway
  namespace: chaos-testing
spec:
  mode: one                 # 只影响一个 Pod
  selector:
    namespaces:
      - production
    labelSelectors:
      app: api-gateway
  stressors:
    cpu:
      workers: 2            # 启动 2 个 worker 消耗 CPU
      load: 80              # 使 CPU 使用率达到 80%
    memory:
      workers: 1
      size: "512MB"         # 额外消耗 512MB 内存
  duration: "5m"
```

StressChaos 用于验证 Kubernetes 的资源 limits 是否正确配置，以及邻近 Pod 的资源是否受到影响（资源隔离是否有效）。

### Workflow：自动化编排混沌实验

复杂的灾备演练场景需要多个故障按顺序或并发注入，Workflow 支持串行（Serial）和并行（Parallel）编排：

```yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: Workflow
metadata:
  name: full-disaster-drill
  namespace: chaos-testing
spec:
  entry: entry-point
  templates:
    - name: entry-point
      templateType: Serial   # 串行执行
      children:
        - kill-one-pod       # 先杀一个 Pod
        - inject-network-delay  # 再注入网络延迟
        - stress-cpu         # 最后施加 CPU 压力

    - name: kill-one-pod
      templateType: PodChaos
      deadline: "2m"
      podChaos:
        action: pod-kill
        mode: one
        selector:
          labelSelectors:
            app: order-service

    - name: inject-network-delay
      templateType: NetworkChaos
      deadline: "5m"
      networkChaos:
        action: delay
        mode: all
        selector:
          labelSelectors:
            app: payment-service
        delay:
          latency: "300ms"
```

### 安全机制：控制爆炸半径

Chaos Mesh 提供多层安全机制，防止实验范围超出预期：

1. **Namespace Selector**：通过 `namespaces` 字段限定影响范围，建议为混沌实验单独维护一个白名单，默认不允许对 `kube-system` 等系统命名空间注入故障

2. **Duration 限制**：所有实验都必须设置 `duration`，超时自动恢复，防止工程师忘记清理遗留的故障注入

3. **Annotation 保护**：对关键组件的 Pod 添加注解 `chaos-mesh.org/ignore-chaos: "true"`，Chaos Mesh 会跳过这些 Pod

4. **实验暂停**：通过修改实验的 `spec.pause: true` 可以立即暂停所有故障注入，无需删除实验对象

## 故障演练流程设计

### 演练前：准备是成功的一半

**演练 Checklist（Pre-flight）**

```
□ 确认演练目标和预期结果（不是"做一做混沌实验"，而是"验证 X 假设"）
□ 确认监控大盘已就绪（Grafana、Alertmanager 均正常工作）
□ 通知相关团队（业务负责人、值班工程师、DBA）
□ 确认回滚方案可执行（kubectl delete / kubectl rollout undo）
□ 准备停止实验的紧急命令，确认执行权限
□ 确认备份/快照已完成（数据库演练前必须）
□ 若为生产演练，确认业务低峰时段（如凌晨 2-4 点）
□ 确认演练范围边界（哪些命名空间，哪些服务受影响）
```

### 演练中：实时观察，记录时间线

演练期间，建议专门安排一人负责**记录时间线**，不要依赖事后回忆：

```
时间线记录模板：
  10:00:00 - 开始注入 NetworkChaos（payment-service 到 MySQL 延迟 500ms）
  10:00:15 - payment-service 的 P99 延迟从 80ms 升至 620ms
  10:00:30 - Alertmanager 触发 HighLatency 告警（符合预期）
  10:00:45 - 熔断器开启，下游调用返回 fallback 响应（符合预期）
  10:02:00 - 发现 order-service 未配置超时，开始出现线程堆积（发现问题）
  10:05:00 - 停止故障注入
  10:05:30 - payment-service P99 延迟恢复正常
  10:06:00 - order-service 线程池耗尽，服务完全不可用（级联故障！）
```

### 演练后：Post-mortem 和改进跟踪

演练后 48 小时内必须完成 Post-mortem，记录：
- 实验假设 vs 实际观测结果
- 发现的系统弱点列表
- 每个弱点的负责人和修复 deadline
- 改进措施纳入下次演练的回归验证

**演练分级：**

| 演练类型 | 范围知晓程度 | 特点 | 适用场景 |
|----------|-------------|------|----------|
| 白盒演练 | 所有团队提前知情 | 风险可控，易于发现技术问题 | 初期阶段，新系统上线 |
| 灰盒演练 | 只有操作团队知情，业务团队不知情 | 验证业务团队的应急响应能力 | 常规季度演练 |
| 黑盒演练 | 全员不知情（只有最高 SRE 知道） | 最真实，但风险最高 | 成熟团队年度大演练 |

### 年度演练计划示例

```
Q1（1-3月）：
  - 月度：Pod 随机杀死演练（开发/测试环境）
  - 季度：主要服务节点故障演练（生产灰度）

Q2（4-6月）：
  - 月度：网络延迟注入演练
  - 季度：数据库主从切换演练（计划内切换）

Q3（7-9月）：
  - 月度：机房单点断电模拟
  - 季度：全链路压测 + 混沌实验联合演练

Q4（10-12月）：
  - 月度：灾备全流程切换演练
  - 年度：黑盒大演练（模拟真实灾难场景）
  - 年度总结：RTO/RPO 目标达成情况评估
```

## 数据库灾备专项

### MySQL：GTID 模式下的精确切换

传统主从复制依赖二进制日志文件名和位点（binlog position）来标记同步状态。切换主库时，需要手动确认新主库的 binlog 位点，容易出错。

GTID（Global Transaction ID）模式为每个事务分配全局唯一 ID，从库可以精确知道"我已经执行了哪些事务"，切换主库时只需指定新主库地址，MySQL 自动找到正确的同步起点：

```sql
-- 传统切换（需要手动确认位点，易出错）
CHANGE MASTER TO MASTER_HOST='new-master', MASTER_LOG_FILE='binlog.000123', MASTER_LOG_POS=4567;

-- GTID 模式切换（自动处理位点）
CHANGE MASTER TO MASTER_HOST='new-master', MASTER_AUTO_POSITION=1;
```

监控复制延迟是 MySQL 灾备的关键运维动作：

```promql
# MySQL 主从延迟（需要 mysqld_exporter）
mysql_slave_status_seconds_behind_master > 30
```

延迟超过 RPO 目标时必须告警，延迟长期积累意味着备库无法在 RPO 内追平。

### Redis：哨兵与集群的灾备差异

Redis Sentinel 模式中，哨兵节点负责监控主节点状态并在故障时发起自动切换（failover）。哨兵节点本身需要奇数个（至少 3 个）分布在不同机器上，防止哨兵节点本身单点故障导致误判。

Redis Cluster 模式将数据分片到 16384 个 slot，每个分片有自己的主从，某个分片的主节点故障时，该分片的从节点会自动提升为主节点。Cluster 模式在灾备场景下更复杂，但单个分片故障不会影响整个集群，容灾粒度更细。

**切换后的缓存预热**是一个容易被忽视的问题：新的 Redis 主节点接管后，内存中的缓存数据可能已经过期或缺失，大量请求会穿透到数据库，形成**缓存雪崩**。灾备方案中必须包含缓存预热策略（如提前触发热点数据加载，或配合熔断器限制穿透流量）。

:::warning 连接池重连的隐患
应用层的数据库连接池通常会缓存已建立的连接。主库切换后，旧连接仍然指向原主库地址（已宕机），连接池需要感知到连接失效才会重建。如果连接池的健康检查间隔过长（如 30 分钟），可能导致切换后应用层持续报错很长时间。**灾备切换后，必须主动触发应用层连接池重置**，或将连接池的 `testOnBorrow` 设置为 true 确保每次使用连接前验证其有效性。
:::

## 监控与自动化切换

### 灾备触发条件的多维度健康检查

单一维度的健康检查容易产生误判。推荐三层检查机制：

```
Layer 1：基础连通性（TCP/HTTP 探测）
  - 端口 80/443 是否可达
  - HTTP 200 响应是否正常
  - 响应时间 < N ms

Layer 2：业务功能验证
  - 核心 API 接口调用成功率
  - 关键业务流程（登录、下单）是否可用

Layer 3：数据层验证
  - 数据库读写是否正常
  - 缓存命中率是否在正常范围
```

只有三层健康检查同时异常，才触发灾备切换，避免单一探测失败引发误切换。

### 自动切换 vs 人工审批

| 切换方式 | 优点 | 风险 | 推荐场景 |
|----------|------|------|----------|
| 全自动切换 | RTO 最短，无需人工介入 | 误切风险高，可能因短暂抖动触发切换 | 仅用于 RTO < 1 分钟的极高可用场景 |
| 人工确认 | 准确性高，避免误操作 | 依赖人工响应，RTO 增加 5~15 分钟 | 大多数生产场景推荐 |
| 半自动 | 自动检测、自动准备，人工一键确认 | 需要完善的工具支持 | 平衡 RTO 和安全性的最优选择 |

### Alertmanager 灾备告警规则

```yaml
groups:
  - name: disaster-recovery
    rules:
      # 主集群 API 成功率跌破阈值
      - alert: PrimaryClusterDegraded
        expr: |
          sum(rate(http_requests_total{cluster="primary", status=~"5.."}[5m]))
          / sum(rate(http_requests_total{cluster="primary"}[5m])) > 0.05
        for: 3m
        labels:
          severity: critical
          action: "考虑灾备切换"
        annotations:
          summary: "主集群错误率持续超过 5%，请评估是否执行灾备切换"
          runbook: "https://wiki.internal/dr-runbook"

      # MySQL 主从延迟超过 RPO 目标
      - alert: MySQLReplicationLagExceedRPO
        expr: mysql_slave_status_seconds_behind_master > 300
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "MySQL 主从延迟 {{ $value }}s，超过 RPO 目标（300s）"
          description: "灾备切换时将有数据丢失风险，需要人工评估"

      # 数据中心网络分区检测
      - alert: DatacenterNetworkPartition
        expr: |
          up{job="cross-dc-probe"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "跨数据中心探测失败，可能发生网络分区"
```

## 小结

- RTO 和 RPO 是灾备方案的核心度量，不同业务级别应对应不同的灾备模式，不要对所有系统一刀切要求双活
- 灾备架构选型：双活 RTO/RPO 最优但成本最高，温备是大多数中等业务的合理选择，关键在于根据业务价值做权衡
- Kubernetes 多集群灾备中，无状态服务容易，有状态服务（MySQL/Redis/Kafka）才是真正的挑战，需要专项设计
- 混沌工程不是破坏测试，而是通过受控实验验证系统韧性假设，Chaos Mesh 以 CRD 形式提供了完整的 Kubernetes 混沌实验能力
- 爆炸半径控制是混沌工程的核心原则：Namespace 限制、Duration 超时、白名单保护，缺一不可
- 一个从未被演练的灾备方案等于没有方案，年度演练计划和 Post-mortem 文化是灾备体系持续有效的保证
- 数据库切换后的连接池重连和缓存预热，是灾备切换后最常见的"踩坑"场景，需要提前在演练中验证

---

## 常见问题

### Q1：RTO 和 RPO 应该由谁来定义？技术团队还是业务团队？

这是一个典型的"技术决策需要业务参与"的例子。纯粹由技术团队定义 RTO/RPO 往往会导致两种错误：要么过于保守（追求极低的 RTO/RPO 却没有必要），要么过于乐观（承诺了业务期望的指标却没有技术支撑）。

正确的流程是：业务团队提供**故障成本评估**（每小时故障损失多少收入、影响多少用户）；技术团队提供**不同 RTO/RPO 目标对应的建设成本和技术方案**；双方在成本和业务价值之间找到最优平衡点。这个讨论通常以 SLA 的形式固化下来，并作为灾备方案的设计基准。

### Q2：Chaos Mesh 能在生产环境使用吗？风险如何控制？

Chaos Mesh 可以在生产环境使用，但需要建立完善的安全机制。核心控制手段有四个：第一，通过 `namespaces` selector 精确限定实验范围，只影响参与演练的服务；第二，对所有关键基础设施 Pod（如监控、CI/CD）添加 `chaos-mesh.org/ignore-chaos: "true"` 注解；第三，所有实验必须设置 `duration`，确保自动恢复；第四，每次生产演练前必须通过测试环境预验证，确认实验的实际效果符合预期。

生产演练应从影响最小的实验开始（如单个 Pod 的 pod-kill），随着团队经验积累逐步升级实验强度，不要一开始就做全链路的黑盒实验。

### Q3：数据库在灾备切换后出现大量连接报错，如何排查？

这个症状通常有三个根因：第一，**连接池缓存了旧连接**，应用层还在向原主库地址发送请求；解决方法是切换后重启应用或强制连接池重建（如调用 `/actuator/refresh` 等热刷新接口）。第二，**新主库的最大连接数不足**，主备规格不一致导致备库无法承载生产流量；演练时需要提前验证备库的资源规格。第三，**GTID 不一致导致从库拒绝同步**，特别是在发生过手动修改数据的情况下；使用 `SHOW SLAVE STATUS\G` 检查具体的 GTID 错误信息，必要时使用 `SET GLOBAL SQL_SLAVE_SKIP_COUNTER` 跳过问题事务（需谨慎评估数据一致性影响）。

### Q4：混沌工程实验发现了系统弱点，但修复周期很长，期间如何降低风险？

发现弱点但无法立即修复是常见场景。短期缓解策略：为该弱点建立专项监控告警（比如 order-service 未配置超时，先加一个"响应时间超过 5 秒持续 1 分钟则告警"的规则），确保问题触发时能快速感知；同时将该故障场景加入"已知风险清单"，在相关技术评审和上线评估中重点关注。中期来说，弱点修复应该作为技术债纳入正式的 Backlog，按照风险等级排列优先级。对于高风险弱点（如核心链路无超时保护），应该与业务团队沟通，评估是否需要临时限流或降级措施来降低影响范围。

### Q5：如何评估灾备方案是否达到了设计目标？

灾备方案的有效性只能通过实际演练数据来评估，不能依赖纸面的架构图。评估的核心指标是：第一，**演练实测 RTO vs 目标 RTO**——每次切换演练都应精确记录从故障触发到系统恢复的实际耗时，与目标值对比；第二，**演练实测 RPO vs 目标 RPO**——切换后检查数据完整性，量化实际丢失的数据量；第三，**演练成功率**——年度演练的成功比例，以及发现新问题的数量（发现问题是好事，说明演练有价值）。建议建立灾备能力成熟度的年度评估机制，将演练结果、改进项完成率等作为 SRE 团队的 OKR 指标，确保灾备能力持续改进而非一次性建设。
