---
date: 2026-02-11
author: Gaaming Zhang
isOriginal: false
article: true
category:
  - AlertManager
tag:
  - AlertManager
  - ClaudeCode
---

# AlertManager 告警阈值设计：静态与动态阈值的选型与实践

## 告警噪音的代价

设想这样一个场景：凌晨两点，你的手机连续震动了七次。打开一看，全是同一批告警——CPU 使用率超过 70%，而这个集群每天凌晨都会跑批处理任务，CPU 升到 80% 是正常现象，三年来从未出过事。你把手机翻回去，心里默默记下：明天把这条规则的阈值调高一点。

这就是**告警疲劳**（Alert Fatigue）的典型起点。当告警频繁误报，on-call 工程师开始形成条件反射——先判断"这条告警是不是又在乱叫"，而不是第一时间响应。当真正的故障触发告警时，反应速度已经慢了半拍。

误报（False Positive）和漏报（False Negative）是告警设计中永恒的权衡。阈值设太松，误报频出；阈值设太紧，真实故障被漏掉。这不是一个可以用"调个参数"解决的问题，它涉及到对业务负载规律的理解，以及对告警系统设计哲学的选择。

---

## 静态阈值

### 什么是静态阈值

静态阈值是最直接的告警方式：当某个指标的当前值超过一个固定数值时，触发告警。

```yaml
# 最朴素的静态阈值告警
- alert: HighCPUUsage
  expr: node_cpu_utilization > 0.8
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "CPU 使用率超过 80%"
```

规则逻辑一目了然，任何人读到这条规则都能立即理解它的含义。这是静态阈值最大的优点：**可解释性强**。

### 静态阈值的适用场景

静态阈值适合那些**天然存在明确上限**的指标：

- **资源硬限制**：磁盘使用率超过 90% 就是危险，不论是白天还是夜晚
- **错误率**：HTTP 5xx 错误率超过 1% 在任何时间都应该告警
- **SLA 指标**：接口 P99 延迟超过 500ms，无论流量高低都不可接受
- **安全类指标**：登录失败次数、非法请求数，这类指标本身就不应该有周期性波动

负载天然稳定、业务规律单一的系统，用静态阈值就够了。一个内部工具服务，白天有少量请求，夜间几乎无流量，没有大促也没有明显的流量峰谷，静态阈值简单够用。

### 静态阈值的局限

问题出在**有周期性负载的系统**上。电商平台的 QPS 在早上十点和凌晨两点可以相差十倍；定时批处理会让 CPU 在特定时段规律性飙高；周末流量和工作日流量结构不同。对这类系统设置统一的静态阈值，要么白天频繁误报，要么夜间漏掉真实问题。

### 让静态阈值更精准的工程实践

**基于历史百分位数设定阈值**

阈值不应该拍脑袋决定。收集最近 30 天的指标数据，用 P95 或 P99 作为 warning 阈值的参考基准，用 P99.9 作为 critical 阈值的参考：

```promql
# 查询过去 30 天 CPU 使用率的 P95
quantile_over_time(0.95, node_cpu_utilization[30d])
```

这样设出来的阈值，告警频率大约是历史数据的 5%（P95）或 1%（P99），既不会沉默，也不至于噪声太大。

**分时段阈值**

对于有明显昼夜差异的指标，可以用 Prometheus 的 `hour()` 函数实现时段区分：

```yaml
- alert: HighRequestLatency
  expr: |
    (
      histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 0.5
      and on() hour() >= 8 < 22
    )
    or
    (
      histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 1.0
      and on() hour() < 8
    )
  for: 3m
  labels:
    severity: warning
  annotations:
    summary: "P99 延迟超标（{{ if (hour() >= 8 and hour() < 22) }}日间阈值 500ms{{ else }}夜间阈值 1000ms{{ end }}）"
```

白天业务高峰用严格阈值，夜间批处理时段用宽松阈值，同一张规则覆盖两种场景。

**`for` 持续时间：过滤瞬间抖动**

`for` 字段是静态阈值规则中最容易被忽视、也最重要的参数。它要求告警条件**持续满足指定时长**才真正触发：

```yaml
- alert: HighMemoryUsage
  expr: container_memory_usage_bytes / container_spec_memory_limit_bytes > 0.85
  for: 10m   # 持续 10 分钟才告警，过滤掉瞬间的 GC 前内存峰值
```

网络抖动、GC 停顿、偶发的请求尖刺，这些通常在几秒到一两分钟内自行恢复。`for: 1m` 到 `for: 5m` 可以过滤掉大量不需要人工介入的瞬时抖动。但不要设置过长——`for: 30m` 意味着故障发生半小时后才告警，那几乎没有意义。

**多级告警分层设计**

避免所有告警都以最高优先级到达。一套合理的分层设计：

```yaml
# Warning：需要关注，不一定需要立即处理
- alert: DiskUsageWarning
  expr: node_filesystem_usage_ratio > 0.80
  for: 10m
  labels:
    severity: warning

# Critical：需要立即介入
- alert: DiskUsageCritical
  expr: node_filesystem_usage_ratio > 0.90
  for: 5m
  labels:
    severity: critical

# Page：影响用户，分钟级响应
- alert: DiskUsagePage
  expr: node_filesystem_usage_ratio > 0.95
  for: 2m
  labels:
    severity: page
```

Warning 发到 Slack，Critical 发邮件，Page 才打电话叫醒人。这样 on-call 工程师的注意力可以精准聚焦在真正紧急的问题上。

---

## 动态阈值

### 核心思路

动态阈值的本质是：**不用一个固定数值，而是用指标自身的历史行为来判断当前值是否异常**。

"当前值是否异常"的判断依据从"是否超过某个数字"变成了"是否偏离了历史规律"。这让告警规则对周期性负载变化天然免疫——批处理时段的 CPU 高峰，在同比数据面前就是正常值。

### 方案一：同比告警（基于 offset）

同比是最朴素的动态阈值：**当前值与一周前同一时间的值相比，偏差超过阈值则告警**。

Prometheus 的 `offset` 修饰符可以查询过去某个时间点的值：

```yaml
- alert: QpsDropSignificantly
  expr: |
    (
      rate(http_requests_total[5m])
      /
      rate(http_requests_total[5m] offset 1w)
    ) < 0.5
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "QPS 较上周同期下降超过 50%，可能存在流量异常"
```

这条规则在大促期间不会误报，因为去年大促的同比值同样很高。在夜间批处理时段也不会误报，因为一周前的同时段 QPS 本来就低。

同比告警的前提是业务有稳定的周期性规律，且历史数据可信。如果一周前有大规模故障或业务变更，同比基线就失去了参考价值。对于快速增长的业务，去年同期的数据往往不具有比较意义，这时候用环比（与昨天同期相比）可能更合适：

```promql
# 环比：与昨天同时段相比
rate(http_requests_total[5m]) / rate(http_requests_total[5m] offset 1d)
```

### 方案二：滑动均值 + 标准差（3-sigma 规则）

统计学中的 3-sigma 规则：正态分布下，超过均值 ±3 个标准差的数据点，概率只有 0.3%。如果当前值偏离近期均值超过 3 倍标准差，大概率是异常。

Prometheus 中用 Recording Rule 预先计算基线：

```yaml
# recording rules：预先计算 1 小时滑动均值和标准差
- record: job:http_request_duration_seconds:rate5m_avg1h
  expr: avg_over_time(rate(http_request_duration_seconds_sum[5m])[1h:5m])

- record: job:http_request_duration_seconds:rate5m_stddev1h
  expr: stddev_over_time(rate(http_request_duration_seconds_sum[5m])[1h:5m])
```

```yaml
# 告警规则：当前值偏离均值超过 3 个标准差
- alert: LatencyAnomalyDetected
  expr: |
    abs(
      rate(http_request_duration_seconds_sum[5m])
      - job:http_request_duration_seconds:rate5m_avg1h
    )
    > 3 * job:http_request_duration_seconds:rate5m_stddev1h
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "延迟指标出现统计异常，偏离近 1 小时均值超过 3 个标准差"
```

需要注意，3-sigma 适用于近似正态分布的指标。对于长尾分布明显的延迟数据，直接套用效果可能不理想，需要先对指标做对数变换或选取合适的时间窗口。

### 方案三：Recording Rule 构建历史基线

更精细的做法是用 Recording Rule 构建"预期基线"，告警规则比较当前值与基线的比值：

```yaml
# recording rules：以过去 4 周同时段数据构建基线
- record: job:http_requests:baseline_1w
  expr: |
    (
      rate(http_requests_total[5m] offset 1w)
      + rate(http_requests_total[5m] offset 2w)
      + rate(http_requests_total[5m] offset 3w)
      + rate(http_requests_total[5m] offset 4w)
    ) / 4
```

```yaml
# 告警规则：当前值偏离基线超过 30%
- alert: RequestRateDeviation
  expr: |
    abs(
      rate(http_requests_total[5m]) - job:http_requests:baseline_1w
    ) / job:http_requests:baseline_1w > 0.3
  for: 10m
  labels:
    severity: warning
  annotations:
    summary: "请求量偏离历史基线超过 30%"
```

四周均值作为基线，比单周同比更鲁棒，单周的偶发异常对基线的影响被稀释掉了。

### 动态阈值的适用场景

动态阈值在以下场景明显优于静态阈值：

- **流量有明显昼夜/周期性规律**：电商、社交、内容平台
- **大促或活动期间**：流量基准本身就和平日不同，需要和同期相比而非绝对值
- **服务刚上线，合理阈值未知**：用历史行为自动建立基线，避免人工拍脑袋
- **多租户系统中不同租户规模差异悬殊**：每个租户用自己的历史基线，避免用统一阈值导致大租户频繁告警、小租户问题被忽略

---

## 减少误报的工程实践

### inhibit_rules：告警抑制

当集群节点宕机时，运行在这个节点上的所有 Pod 都会触发告警。但真正的根因只有一个：节点故障。用 `inhibit_rules` 可以让子告警在父告警存在时自动静默：

```yaml
# alertmanager.yml
inhibit_rules:
  - source_match:
      alertname: NodeDown
      severity: critical
    target_match:
      severity: warning
    equal:
      - node   # 同一节点上，NodeDown 压制所有 warning 级别告警
```

这样 on-call 工程师收到的是一条"节点宕机"，而不是几十条"容器不可用"、"服务无响应"的堆砌。

### group_wait / group_interval / repeat_interval 调优

```yaml
# alertmanager.yml
route:
  group_wait: 30s          # 初次告警等待时间，收集同组告警合并发送
  group_interval: 5m       # 同组新增告警的等待时间
  repeat_interval: 4h      # 已发送告警的重复发送间隔
```

`repeat_interval` 是减少告警噪音的关键参数。默认值往往是 1h，意味着一个持续故障每小时打扰一次 on-call。对于大多数 warning 级告警，设置为 4h 或 8h 更合适——已经被通知了，没解决的问题不需要每小时再提醒一次。对于 critical 和 page 级告警，1h 甚至 30m 的重复间隔才能确保问题不被遗忘。

### Dead Man's Switch（心跳告警）

一个常被忽视的场景：告警系统本身出了问题，指标采集中断，没有数据，自然也不会产生任何告警——包括应该产生的告警。这是最危险的漏报。

Dead Man's Switch 通过"始终为真的告警"来验证告警链路的健康：

```yaml
# 这条告警始终触发
- alert: Watchdog
  expr: vector(1)
  labels:
    severity: none
  annotations:
    summary: "告警链路心跳，如果你停止收到这条消息，说明告警系统出现问题"
```

在 AlertManager 中配置一个专门接收 `Watchdog` 的 receiver，该 receiver 定期向外部心跳服务（如 Dead Man's Snitch、Healthchecks.io 或自建的心跳接口）发送 HTTP 请求。如果心跳中断，外部服务主动告警。这样形成了一个闭环验证，确保监控系统本身的可靠性。

### 每条告警都应对应一个人工操作

一条好的告警规则，应该能回答这个问题：**收到这条告警，on-call 工程师应该做什么？**

如果这个问题的答案是"看一下，发现没问题就关掉"，那这条告警就不应该存在，或者阈值需要调整。

"症状导向告警"（Symptom-based Alerting）比"原因导向告警"（Cause-based Alerting）更有价值。告警"用户侧错误率超过 1%"比"数据库连接池耗尽"更直接——前者说明用户在受影响，后者可能是前者的原因之一，但也可能有其他原因。优先告警症状，在 Runbook 中引导工程师排查原因。

---

## 静态 vs 动态阈值选型对比

| 维度 | 静态阈值 | 动态阈值 |
|------|---------|---------|
| 实现复杂度 | 低，直接写数值 | 中高，需要 recording rule 或 offset 查询 |
| 可解释性 | 高，任何人都能理解 "大于 80% 告警" | 中，需要理解基线构建逻辑 |
| 适应周期性负载 | 差，需要手动设分时段规则 | 好，天然感知历史规律 |
| 冷启动问题 | 无，立即生效 | 有，需要积累足够历史数据 |
| 业务变更敏感性 | 需要手动重新调参 | 自动适应，但变更初期可能有过渡噪声 |
| 运维成本 | 低，规则稳定后几乎不需维护 | 中，需要定期验证基线质量 |
| 适合指标类型 | 有明确安全边界的指标 | 业务指标、流量类指标 |

两者不是非此即彼的关系，成熟的告警体系通常是**静态阈值兜底 + 动态阈值感知异常**的组合：

```
基础设施层（磁盘、CPU、内存） → 静态阈值为主
服务层（错误率、延迟）       → 静态阈值 + SLO 驱动
业务层（QPS、转化率）        → 动态阈值为主
```

---

## 实战：一套合理的告警体系设计思路

### 从四个黄金信号出发

Google SRE 提出的四个黄金信号是告警设计的起点：

- **延迟（Latency）**：请求完成所需时间，区分成功请求和失败请求的延迟
- **流量（Traffic）**：系统承受的请求量
- **错误（Errors）**：失败请求的比例
- **饱和度（Saturation）**：资源的使用程度，即距离上限还有多少余量

优先为这四个维度设置告警，而不是为每一个可观测的指标都设置阈值。一个系统有几百个 Prometheus 指标，但真正需要 on-call 响应的，通常只涉及其中十几条。

### SLO 驱动的告警设计

以 SLO 为核心设计告警，比基于单一指标更能反映用户体验。

假设服务的 SLO 是：过去 30 天内，99.9% 的请求延迟低于 200ms（即允许 0.1% 的请求超标，对应 30 天约 43 分钟的 "budget"）。

基于错误预算消耗速率的告警：

```yaml
# 1 小时内的错误预算消耗速率超过 2%/小时 → 预计 50 小时内耗尽预算
- alert: SLOBurnRateFast
  expr: |
    sum(rate(http_request_duration_seconds_bucket{le="0.2"}[1h]))
    /
    sum(rate(http_request_duration_seconds_count[1h]))
    < 0.999
  for: 2m
  labels:
    severity: critical

# 6 小时内的慢速消耗
- alert: SLOBurnRateSlow
  expr: |
    sum(rate(http_request_duration_seconds_bucket{le="0.2"}[6h]))
    /
    sum(rate(http_request_duration_seconds_count[6h]))
    < 0.999
  for: 30m
  labels:
    severity: warning
```

快速消耗用短窗口检测，确保突发问题快速响应；慢速消耗用长窗口检测，识别持续性的性能退化。双窗口结合，在误报率和响应速度之间取得更好的平衡。

### 分层告警架构

```
┌─────────────────────────────────────────┐
│              业务层告警                   │
│  转化率异常、支付成功率、核心功能可用性      │
│  特点：最终用户可感知，优先级最高            │
└────────────────────┬────────────────────┘
                     │ 向下追溯根因
┌────────────────────▼────────────────────┐
│              服务层告警                   │
│  错误率、延迟、QPS 异常、服务不可用         │
│  特点：面向服务指标，用 SLO 驱动            │
└────────────────────┬────────────────────┘
                     │ 向下追溯根因
┌────────────────────▼────────────────────┐
│            基础设施层告警                  │
│  磁盘、内存、CPU、网络、节点状态             │
│  特点：资源类指标，用静态阈值               │
└─────────────────────────────────────────┘
```

业务层告警直接对应用户影响，是最需要立即响应的。基础设施层告警更多是辅助排查的上下文信息。当业务层告警触发时，工程师可以顺着链路向下查看服务层和基础设施层的状态，快速定位根因。

---

## 小结

- **静态阈值**适合有明确安全边界的指标（资源使用率、错误率），优先基于历史百分位数设定，配合分时段规则和 `for` 参数过滤抖动
- **动态阈值**适合有周期性规律的业务指标，同比/环比是最直接的实现，3-sigma 规则适用于分布较均匀的指标
- 实践中两者结合：基础设施用静态阈值，业务指标用动态阈值，SLO 错误预算消耗速率告警覆盖用户体验
- 减少误报的工程手段：`inhibit_rules` 避免告警风暴，合理配置 `repeat_interval`，Dead Man's Switch 保证告警链路可靠
- 每条告警规则都应该对应一个明确的人工响应动作——这是判断告警规则是否有价值的最简单标准

---

## 常见问题

### Q1：如何判断一条告警规则是否需要调整阈值？

衡量告警质量有两个核心指标：误报率和告警行动率。统计一段时间内（通常一个月）：有多少告警被触发后，对应的 Runbook 步骤被执行，或问题被确认需要人工处理（告警行动率）；有多少告警被直接关闭，没有对应的处置动作（误报率）。如果某条规则的误报率超过 30%，就需要审视阈值设置。实操方法：在 AlertManager 中打好 `resolved_by` 标注（告警是自动恢复还是人工关闭），定期出报告，识别高噪声规则。

### Q2：动态阈值方案需要多长时间的历史数据才能稳定？

这取决于采用哪种方案。同比告警至少需要一到两周的稳定历史数据，以避免冷启动期的大量误报。3-sigma 规则基于 1 小时滑动窗口，通常几个小时后就能建立相对稳定的基线。基于四周均值的历史基线方案则需要完整的四周数据，才能覆盖工作日与周末、不同周次的规律。在数据积累期间，建议同时保留静态阈值作为兜底，待动态阈值稳定后再逐步替代。

### Q3：`for` 时间应该怎么设置，有没有经验值？

`for` 的设置取决于业务可以容忍的故障检测延迟，以及希望过滤的抖动时长。通用参考：`for: 1m` 适合需要快速检测的关键服务（如支付链路错误率）；`for: 5m` 适合大多数服务指标；`for: 15m` 适合资源类指标（如磁盘使用率增长，通常不需要分钟级响应）。注意 `for` 时间必须小于告警的实际危害时间——如果磁盘写满需要 30 分钟才会影响业务，那 `for: 15m` 是合理的；如果服务出错后 2 分钟就会触发熔断影响用户，`for: 5m` 就太慢了。

### Q4：如何处理告警风暴（同一时间大量告警同时触发）？

告警风暴通常由级联故障触发：一个基础设施问题导致几十个服务同时异常。应对策略有三层：第一层，使用 AlertManager 的 `group_by` 将相关告警合并为一条通知，减少通知数量；第二层，使用 `inhibit_rules` 让根因告警压制症状告警，只保留最核心的几条；第三层，在告警规则层面加入 `unless` 条件，当已知的上游问题存在时，下游告警自动静默。核心原则是：告警风暴是告警设计问题，不是通知渠道的问题，不能靠把手机调成静音来解决。

### Q5：在微服务场景中，如何避免因为一个服务的问题触发几十条告警？

关键是建立**拓扑感知的告警规则**。给每个服务的告警添加 `owner`、`tier`、`depends_on` 等自定义标签，通过 AlertManager 的路由规则按 owner 发送给对应的团队，而不是全部打给同一个人。利用 `inhibit_rules` 建立服务依赖图：当数据库告警触发时，依赖该数据库的所有上游服务的告警可以被抑制。另外，推荐从业务层告警开始，只有当"用户侧不可用"这个顶层告警触发时，才拉起对应的 on-call。服务内部的指标异常（如连接池使用率高）作为 warning 发到 Slack 就够了，不需要打电话。

## 参考资源

- [Prometheus Alerting 规则](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/)
- [Alertmanager 配置指南](https://prometheus.io/docs/alerting/latest/alertmanager/)
- [Google SRE 监控实践](https://sre.google/sre-book/monitoring-distributed-systems/)
