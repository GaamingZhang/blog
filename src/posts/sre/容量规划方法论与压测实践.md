---
date: 2026-02-13
author: Gaaming Zhang
isOriginal: true
article: true
category:
  - SRE
tag:
  - SRE
  - 容量规划
  - 压测
  - 性能
---

# 容量规划方法论与压测实践

## 没有基线数据时的困境

想象这样一个场景：大促前两周，业务方告诉你"这次活动预计流量是平时的十倍"。你打开 Kubernetes 集群，盯着当前运行的三个副本，脑海中浮现出一个问题：**我需要扩到多少台？**

如果没有历史数据，这个问题几乎无法回答。你不知道单个实例能处理多少请求，不知道数据库在高并发下会不会成为瓶颈，不知道缓存命中率在流量翻倍时会如何变化。最常见的结局是两种极端：要么保守扩容，备了五十台机器，实际用了十台，资源浪费严重；要么冒险保守，结果大促中出现服务降级，损失了真实的营收。

这两种极端的共同原因，是缺乏系统的容量规划。

容量规划的本质可以用三个步骤概括：**了解现状（建立性能基线）→ 建立模型（压测验证容量上限）→ 预测未来（根据增长趋势和弹性策略提前准备资源）**。缺少任何一环，容量规划就只是拍脑袋。

---

## 性能基线建立

### 关键指标的选择

并非所有指标都有容量规划价值。有效的基线只需要关注五类指标：

- **QPS（吞吐量）**：衡量系统处理请求的速率，是容量规划的主要自变量
- **P99 延迟**：99 分位的请求延迟，比平均延迟更能反映用户真实体验
- **CPU 利用率**：最直接的计算资源消耗指标
- **内存利用率**：内存压力会引发 GC 频繁、OOM Kill 等连锁问题
- **网络带宽**：在高吞吐的数据传输场景（如文件服务、流媒体）中往往是瓶颈

基线的意义在于：你需要知道在"正常负载"下，每一项指标的典型值和波动范围。

### 用 Prometheus 提取基线数据

基线不是某一时刻的快照，而是过去一段时间（通常 30 天）的统计特征。Prometheus 的历史数据天然适合做这个分析。

**过去 30 天的平均 QPS**（以 `http_requests_total` 为例）：

```promql
rate(http_requests_total{job="api-server"}[5m])
```

要得到"30 天平均值"，可以在 Grafana 中将时间范围设置为 30 天，观察这条曲线的中位数和峰值。更精确的做法是用 `avg_over_time` 计算：

```promql
avg_over_time(
  rate(http_requests_total{job="api-server"}[5m])[30d:5m]
)
```

**P99 延迟基线**（基于 Histogram 指标）：

```promql
histogram_quantile(
  0.99,
  sum(rate(http_request_duration_seconds_bucket{job="api-server"}[5m])) by (le)
)
```

**CPU 峰值利用率**（过去 30 天中每天的最高值）：

```promql
max_over_time(
  avg(rate(container_cpu_usage_seconds_total{container="api-server"}[5m])) by (pod)[30d:1h]
)
```

通过这三组查询，你能回答：正常情况下系统每秒处理多少请求，延迟是否达标，CPU 在什么时间点接近瓶颈。

### 安全水位为什么不能设 100%

一个常见的误区是"CPU 利用率 80% 才报警，说明 79% 是安全的"。但安全水位的设定必须考虑两个现实：

第一，**流量不是均匀的**。即使平均 QPS 是 1000，瞬时峰值可能是平均值的 2～3 倍。如果系统在平均负载下跑到 80% CPU，那在瞬时峰值下就会超过 100%，请求开始排队，延迟指数级增长。

第二，**扩容不是瞬时的**。HPA 从触发到新 Pod 就绪，通常需要 30 秒到 2 分钟。这段时间内，原有实例需要独立承载全部流量。

因此，**生产环境的安全水位建议是 CPU ≤ 70%、内存 ≤ 80%**。超过这个阈值就应该触发扩容，而不是等到资源耗尽。

---

## 压测设计方法论

建立了基线之后，下一步是通过压测找到系统的容量上限。基线告诉你"现在承受着什么"，压测告诉你"最多能承受多少"。

### 四种压测场景

**基准测试**是单接口压测，目的是在无外部依赖干扰下确定某个接口的性能上限。通常使用单一接口、固定参数，并发数从低到高，找到延迟开始明显上升的拐点。这是建立"单接口容量参数"的基础。

**混合场景测试**模拟真实用户行为。真实流量不是单一接口，而是登录、查询、下单、支付的混合。按照真实流量中各接口的比例设计场景，才能模拟真实的资源消耗模式，避免单接口压测数据过于乐观。

**阶梯压测**是容量规划最核心的压测方式。从低并发开始，每隔一段时间增加一个并发档位，同时记录 QPS、延迟和资源利用率的变化。当某项指标出现明显拐点（延迟突然增大、QPS 增长停滞），就说明到达了瓶颈边界。

**峰值压测**模拟秒杀、大促等突发高并发场景，在短时间内施加远超正常水平的负载，验证系统在极端条件下的降级和恢复能力。

### 压测工具选型

| 工具 | 语言 | 脚本方式 | 分布式支持 | 适用场景 | 上手难度 |
|------|------|----------|------------|----------|----------|
| K6 | Go | JavaScript | 原生支持（K6 Cloud / Operator） | 现代 API 压测，CI 集成 | 低 |
| JMeter | Java | GUI / XML | 分布式 Controller-Worker | 传统企业压测，GUI 操作 | 中 |
| Locust | Python | Python 代码 | 分布式（主从模式） | 复杂业务逻辑建模 | 中 |
| Vegeta | Go | 命令行 | 单机 | 简单 HTTP 基准测试，CI Pipeline | 极低 |

**推荐选择 K6**。原因有三：脚本使用 JavaScript 编写，开发体验好；原生支持 Prometheus Remote Write 输出，压测数据可直接接入已有监控体系；提供 K6 Operator，可在 Kubernetes 中直接运行分布式压测，无需额外维护压测集群。

### K6 阶梯压测脚本

以下是一个完整的 K6 阶梯压测脚本，覆盖预热、阶梯加压、峰值保持、降压回收四个阶段：

```javascript
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate, Trend } from 'k6/metrics';

// 自定义指标
const errorRate = new Rate('error_rate');
const apiLatency = new Trend('api_latency', true);

export const options = {
  // 阶梯压测场景配置
  scenarios: {
    ramping_load: {
      executor: 'ramping-vus',
      startVUs: 0,
      stages: [
        { duration: '2m', target: 50 },   // 预热阶段：2 分钟加压到 50 VU
        { duration: '5m', target: 50 },   // 稳定阶段：保持 50 VU 观察基准
        { duration: '2m', target: 200 },  // 加压阶段：2 分钟加压到 200 VU
        { duration: '5m', target: 200 },  // 稳定阶段：保持 200 VU 观察系统状态
        { duration: '2m', target: 500 },  // 加压阶段：2 分钟加压到 500 VU
        { duration: '5m', target: 500 },  // 高负载稳定：500 VU（寻找拐点）
        { duration: '2m', target: 800 },  // 继续加压：推进到极限
        { duration: '3m', target: 800 },  // 极限验证阶段
        { duration: '3m', target: 0 },    // 降压回收
      ],
      gracefulRampDown: '30s',
    },
  },

  // 压测通过标准（SLO 验证）
  thresholds: {
    http_req_duration: ['p(99)<500'],  // P99 延迟 < 500ms
    error_rate: ['rate<0.01'],         // 错误率 < 1%
    http_req_failed: ['rate<0.01'],
  },
};

const BASE_URL = __ENV.TARGET_URL || 'http://api-server:8080';

export default function () {
  // 模拟真实用户行为：查询接口
  const listRes = http.get(`${BASE_URL}/api/v1/products`, {
    headers: { 'Content-Type': 'application/json' },
    tags: { endpoint: 'list_products' },
  });

  check(listRes, {
    'list status 200': (r) => r.status === 200,
    'list latency < 200ms': (r) => r.timings.duration < 200,
  });

  errorRate.add(listRes.status !== 200);
  apiLatency.add(listRes.timings.duration);

  sleep(1); // 模拟用户思考时间

  // 模拟下单接口
  const orderRes = http.post(
    `${BASE_URL}/api/v1/orders`,
    JSON.stringify({ productId: 'prod-001', quantity: 1 }),
    {
      headers: { 'Content-Type': 'application/json' },
      tags: { endpoint: 'create_order' },
    }
  );

  check(orderRes, {
    'order status 201': (r) => r.status === 201,
    'order latency < 500ms': (r) => r.timings.duration < 500,
  });

  errorRate.add(orderRes.status !== 201);
  sleep(0.5);
}
```

执行命令（输出到 Prometheus）：

```bash
k6 run \
  --out experimental-prometheus-rw \
  -e K6_PROMETHEUS_RW_SERVER_URL=http://prometheus-pushgateway:9091/metrics/job/k6 \
  -e TARGET_URL=http://api-server:8080 \
  load-test.js
```

---

## 系统瓶颈定位

压测过程中，监控体系要和压测工具同步运行。这里推荐使用 **USE 方法**（由 Netflix 的 Brendan Gregg 提出）作为瓶颈诊断框架：

- **Utilization（利用率）**：资源被使用的比例，例如 CPU 利用率 85%
- **Saturation（饱和度）**：资源等待队列的长度，例如 CPU 的 run queue 长度、磁盘 IO 等待数
- **Errors（错误数）**：设备级或系统级的错误，例如网卡丢包率、磁盘 IO 错误

USE 方法的价值在于：它强迫你对每一类资源都完整地回答这三个问题，而不是只看利用率，忽视了饱和度信号。

### 四种典型瓶颈特征

**CPU 瓶颈**的压测特征：QPS 增长停止的同时，CPU 利用率接近 100%，延迟线性增长。此时 CPU run queue（`node_schedstat_waiting_seconds`）会持续增大。解法：水平扩容，或优化热点代码路径。

**内存瓶颈**的压测特征：QPS 增长到某个点时延迟突然飙升，JVM 类服务表现为 GC 停顿指标（`jvm_gc_pause_seconds`）暴涨，Go 服务表现为 `go_gc_duration_seconds` 增大。解法：增大实例内存规格，或优化对象分配。

**IO 瓶颈**的压测特征：CPU 不高，但延迟持续增大，磁盘 `await` 时间（`node_disk_io_time_seconds_total`）显著上升。数据库场景中还会看到慢查询数量增加。解法：使用 SSD，优化 SQL 减少全表扫描，或增加读缓存。

**网络瓶颈**的压测特征：应用层指标看起来正常，但下游服务超时率增加，网卡带宽使用率接近上限（`node_network_transmit_bytes_total`）。解法：检查大对象传输是否可以压缩，或升级网络规格。

:::warning 压测中最容易忽视的瓶颈
数据库连接池是最常见的被忽视的瓶颈点。压测时应用层 CPU 和内存都正常，但 P99 延迟突然升高——很可能是数据库连接池已满，请求在等待连接。通过 `hikaricp_connections_pending`（HikariCP）或 `pg_stat_activity_count`（PostgreSQL）可以快速确认。
:::

### 找到拐点

阶梯压测中，拐点是容量规划的关键数据。拐点的判断标准是：**在某个 VU 档位，QPS 增长幅度明显小于 VU 增长幅度，同时 P99 延迟开始非线性增长**。

例如：从 200 VU 到 500 VU，QPS 从 800 增长到 1200（增幅 50%），但 P99 延迟从 80ms 增长到 350ms（增幅 337%）。这个区间就是系统的拐点区域，说明系统已经进入饱和状态，500 VU 对应的 QPS 是不可持续的。

**可持续的单实例峰值 QPS**，应该取拐点前最后一个稳定档位的 QPS 值，通常是延迟刚开始加速上涨的那个点。

---

## 容量模型建立

有了压测数据，就可以建立容量模型，回答"我需要多少实例"这个核心问题。

### 基本容量公式

```
所需实例数 = ceil(目标 QPS × (1 + 安全余量) / 单实例可持续 QPS)
```

以具体数值说明：

- 大促预期峰值 QPS：10,000
- 安全余量：30%（应对预测误差和突发）
- 压测得到的单实例可持续 QPS：800

```
所需实例数 = ceil(10,000 × 1.3 / 800) = ceil(16.25) = 17 个实例
```

:::tip 安全余量的选取
安全余量需要根据流量的可预测性来设定。大促流量较为可预测，30% 余量通常足够。对于突发性流量（如病毒式传播的内容），建议预留 50%～100% 的余量，并配合弹性扩容策略。
:::

### 数据库容量的特殊性

数据库不像无状态应用可以随意水平扩展，其容量规划有两个特殊约束：

**连接数上限**：MySQL 的 `max_connections` 默认 151，PostgreSQL 默认 100。当应用实例水平扩展时，数据库连接数线性增加，很容易触发上限。假设每个应用实例连接池大小为 10，当应用扩展到 20 个实例时，数据库就需要承受 200 个连接，超过默认上限。应对策略是引入连接代理（如 PgBouncer、ProxySQL），在连接层做复用。

**Amdahl 定律的影响**：数据库写操作因为锁竞争，并发扩展能力远不如读操作。当写 QPS 达到某个值后，继续增加连接数反而会因为锁竞争导致吞吐量下降。这也是读写分离和分库分表的根本原因。

---

## 需求预测与弹性策略

容量规划不是一次性工作，而是需要持续跟踪流量增长趋势并动态调整。

### 趋势分析

业务流量通常呈现两种模式叠加：

- **长期线性增长**：用户规模扩大带来的基础流量增长
- **周期性波动**：每日的早晚高峰、每周的工作日高峰、每年的节假日大促

Prometheus 的 `predict_linear` 函数可以基于历史数据做线性增长预测：

```promql
predict_linear(
  rate(http_requests_total{job="api-server"}[1h])[7d:1h],
  30 * 24 * 3600  -- 预测 30 天后的 QPS
)
```

这个查询基于过去 7 天的 QPS 增长趋势，预测 30 天后的 QPS 值。结合容量公式，就能提前计算需要在何时扩容。

### Kubernetes HPA 与 KEDA

**HPA（Horizontal Pod Autoscaler）** 是 Kubernetes 原生的弹性扩缩机制，基于 CPU 或内存利用率触发扩缩：

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-server-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  minReplicas: 3
  maxReplicas: 50
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60  # 目标 CPU 利用率 60%
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60  # 扩容稳定窗口 60 秒
      policies:
        - type: Pods
          value: 5          # 每次最多扩容 5 个 Pod
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300 # 缩容稳定窗口 5 分钟，避免抖动
```

**KEDA（Kubernetes Event-Driven Autoscaling）** 扩展了 HPA 的触发维度，支持基于外部指标扩缩，例如 Kafka Topic 的消息积压量：

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: kafka-consumer-scaler
spec:
  scaleTargetRef:
    name: order-consumer
  minReplicaCount: 2
  maxReplicaCount: 30
  triggers:
    - type: kafka
      metadata:
        bootstrapServers: kafka-broker:9092
        consumerGroup: order-processing-group
        topic: order-events
        lagThreshold: "1000"  # 积压超过 1000 条时触发扩容
```

KEDA 适合消费型服务：消息积压是比 CPU 更直接的负载指标。当下游消费速度跟不上上游生产速度时，KEDA 能更快速、更精准地触发扩容。

### 预留容量 vs 弹性扩容的成本权衡

| 策略 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| 全量预留 | 响应最快，零风险 | 资源利用率低，成本高 | 核心支付链路、极端低容忍场景 |
| 弹性扩容 | 成本最优 | 扩容有时延（30s～2min） | 非核心服务、流量平滑增长场景 |
| 预留基础量 + 弹性补充 | 平衡成本和响应速度 | 需要精细配置 | **推荐策略**，适合大多数场景 |

**推荐策略**：保持正常负载 150% 的基础副本数（预留余量应对小波动），超出部分通过 HPA 弹性扩容。这样既避免了扩容时延的风险，又不需要预留大促级别的全量资源。

---

## 大促容量规划实战

以电商大促场景为例，完整的容量规划时间线如下：

**T-30 天（压测与基准确认）**

完成单服务的阶梯压测，得到每个核心服务的单实例可持续 QPS。基于业务预期峰值 QPS 和容量公式，计算各服务的目标实例数。此时还需要确认数据库连接数上限是否会成为瓶颈，以及缓存容量是否足够承载高峰期的读请求量。

**T-7 天（集群扩容与联调验证）**

按照计算结果完成扩容，对扩容后的集群进行混合场景压测（模拟大促流量比例），验证扩容方案的正确性。同时验证 HPA 和 KEDA 策略的配置是否符合预期，确保在极端流量下弹性策略能正确触发。

**T-1 天（预热与最终检查）**

缓存预热是大促前最关键的操作。冷启动状态下，所有请求都会打到数据库，这会在大促开始瞬间造成数据库压力尖峰。提前将热点数据写入 Redis，将缓存命中率拉到正常水位。同时检查 Kubernetes 节点是否有足够的 Pending 容量支撑 HPA 扩容。

:::warning 大促当天最容易翻车的点
大促开始前 5 分钟，通常会有用户提前刷页面造成流量提前冲击。如果 HPA 的 `stabilizationWindowSeconds` 设置过长（如默认 5 分钟），这波流量可能在 HPA 尚未响应时就压垮服务。建议大促期间临时将 `scaleUp.stabilizationWindowSeconds` 设置为 30 秒。
:::

**大促中（实时监控与快速响应）**

核心监控看板需要持续关注四项指标：QPS 趋势（是否符合预期曲线）、P99 延迟（是否在 SLO 范围内）、错误率（是否出现异常错误类型）、各服务副本数（HPA 是否正常工作）。提前制定好降级预案：如果数据库压力过大，哪些非核心功能可以关闭；如果某个服务完全不可用，限流策略如何触发。

**大促后（缩容与复盘）**

大促结束后不要立刻缩容，等待流量回落到正常水位（通常 2～4 小时后）再逐步缩容，避免尾部流量导致服务抖动。复盘时重点关注：实际 QPS 与预测 QPS 的偏差、哪个瓶颈点最先触发、哪个监控告警最先发出信号，将这些数据沉淀为下次容量规划的输入。

---

## 小结

- **基线是容量规划的起点**：没有过去 30 天的 QPS、延迟、资源利用率数据，任何容量估算都是猜测
- **压测找到真实容量上限**：阶梯压测 + USE 方法，可以精确定位单实例可持续 QPS 和瓶颈所在
- **容量公式简单但有效**：`实例数 = ceil(目标 QPS × (1 + 安全余量) / 单实例 QPS)`，数据库连接数是额外约束
- **弹性扩缩是容量规划的保险**：HPA 应对计算资源压力，KEDA 应对消息积压，预留基础量 + 弹性补充是推荐策略
- **大促容量规划是一个流程**：T-30、T-7、T-1、大促中、大促后，每个阶段有明确的操作清单，不能临时抱佛脚

---

## 常见问题

### Q1：压测环境和生产环境差异很大，压测数据还有参考价值吗？

压测数据的可信度取决于环境相似度，但即使环境有差异，压测仍然有三个核心价值：第一，压测揭示的**瓶颈类型**（CPU、IO、连接数）通常在生产中同样存在，只是触发阈值不同；第二，**单实例 QPS 的相对值**是可以换算的，例如压测环境的实例规格是生产的一半，那么可持续 QPS 也大约是一半；第三，**系统行为模式**（延迟在拐点后的增长曲线）可以帮助预判生产中的劣化特征。最理想的做法是全链路压测（流量引流到隔离的生产副本），但代价较高，适合核心交易链路的大促前验证。

### Q2：HPA 的 CPU 目标利用率设为多少合适？

通常设置为 **60%～70%**，而非更高的值，原因有两个：一是 HPA 的评估周期默认 15 秒，触发扩容后新 Pod 就绪需要额外 30 秒到 2 分钟，这段时间内旧 Pod 需要独立承受全部流量，如果目标利用率设置过高，扩容触发时旧 Pod 可能已经处于饱和状态；二是 CPU 利用率是采样值，存在平滑延迟，真实的瞬时峰值往往比采样值高 20%～30%。将目标设为 60%，意味着实际触发扩容时 CPU 大约在 65%～75% 之间，留有足够的缓冲空间完成扩容。

### Q3：如何区分慢查询导致的高延迟和容量不足导致的高延迟？

两者的核心区别在于**负载相关性**：容量不足导致的高延迟会随 QPS 增长而加剧，且在 QPS 回落后延迟也会恢复；慢查询导致的高延迟在低负载下同样存在，且通常集中在特定接口。诊断方法：在 Grafana 中按接口分组观察 P99 延迟，如果某个接口的延迟明显高于其他接口，且与负载增长不完全相关，大概率是慢查询。用 `EXPLAIN ANALYZE` 验证可疑 SQL，结合数据库的 `slow_query_log` 确认。

### Q4：Kubernetes 节点资源不足时，HPA 扩容出来的 Pod 会 Pending，如何提前应对？

这个问题的根本是**节点容量规划**需要先于 Pod 容量规划。有三种应对策略：一是使用 **Cluster Autoscaler**，让 Kubernetes 在 Pod Pending 时自动添加节点（适合公有云环境）；二是**预留 Pending 节点池**，预先准备一批处于 cordon 状态的节点，大促前手动解除 cordon；三是提前**手动扩展节点**，在 T-7 天完成节点扩容，确保大促前集群已经有足够的空闲容量支撑 HPA 扩容。混合部署场景（On-Premises + 公有云弹性节点）是大型电商常用的成本优化方案。

### Q5：容量规划中，如何估算缓存（Redis）的容量需求？

Redis 容量规划需要关注两个维度：**内存容量**和**连接数/QPS 上限**。内存容量 = 热点数据量 × (1 + 增长余量)，热点数据量可以通过生产 Redis 的 `used_memory` 指标获取，再根据业务增长预期乘以系数。Redis 单线程处理命令，其 QPS 上限通常在 10 万～20 万之间（取决于命令复杂度），可以通过 `redis-benchmark` 在目标机型上实测。特别需要注意的是大 Key 问题：一个存储了数万元素的 Hash 或 Set，在执行全量操作时会导致 Redis 阻塞，这在压测中很难复现，需要在代码审查和 `redis-cli --bigkeys` 扫描中提前发现。
