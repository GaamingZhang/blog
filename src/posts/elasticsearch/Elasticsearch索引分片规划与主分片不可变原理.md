---
date: 2026-02-11
author: Gaaming Zhang
isOriginal: false
article: true
category:
  - Elasticsearch
tag:
  - Elasticsearch
  - ClaudeCode
---

# Elasticsearch 索引分片规划与主分片不可变原理

## 问题的起点

想象这样一个场景：你的团队在设计 ES 索引时，凭经验设置了 5 个主分片，系统平稳运行了半年。随着业务增长，单个索引的数据量从 50GB 膨胀到了 500GB，每个分片高达 100GB，查询延迟开始爬升，分片恢复也变得异常缓慢。你打开 ES 文档，准备把主分片数从 5 改到 20——结果发现，**主分片数是只读的，无法修改**。

这不是 ES 的设计缺陷，而是一个经过深思熟虑的设计决策，背后有严密的数学逻辑。理解这个逻辑，不仅能让你明白为什么不能改，更能帮助你在设计之初就把分片数规划正确。

本文从路由算法出发，逐步展开分片规划的完整方法论。

---

## 分片的本质：一个完整的 Lucene 索引

在深入路由算法之前，先建立对分片的准确认知。

ES 的一个"索引"在逻辑上是一个整体，但在物理上，它被切分为若干个**分片（Shard）**。每个分片本质上是一个独立的、完整的 Lucene 索引实例，拥有自己的内存 Buffer、Translog、Segment 文件和元数据。

```
ES Index（逻辑整体）
  ├── Shard 0（完整的 Lucene 实例）
  ├── Shard 1（完整的 Lucene 实例）
  └── Shard 2（完整的 Lucene 实例）
        │
        └── Replica Shard（主分片的完整副本）
```

分片分为两种：

- **主分片（Primary Shard）**：负责接受写入请求，是数据的权威来源。索引创建时确定数量，之后不可变更。
- **副本分片（Replica Shard）**：主分片的完整拷贝，服务读请求，提供高可用。数量可以动态修改。

分片是 ES **水平扩展的基本单位**。如果一个索引只有一个主分片，所有写入都集中在一个节点上，无论集群有多少个节点，写入都无法水平扩展。相反，10 个主分片分布在 10 个节点上，写入请求可以并行地打到所有节点。

分片也是**并行查询的基本单位**。查询时，协调节点将请求广播到所有分片，每个分片独立执行并返回局部结果，协调节点再汇总。分片数越多，并行度越高——但超过节点数后，多余的分片只能堆叠在同一个节点上，反而引入额外开销。

---

## 路由算法：主分片数量不可变的根本原因

这是整篇文章最核心的部分。

### 路由公式

当一个文档被写入或查询时，ES 需要知道它在哪个分片上。ES 使用以下公式完成这个计算：

```
shard_num = hash(_routing) % number_of_primary_shards
```

- `_routing` 默认是文档的 `_id`，也可以手动指定。
- `hash()` 是 ES 内部使用的 MurmurHash3，对同一个字符串始终返回相同的整数。
- `number_of_primary_shards` 是索引创建时设定的主分片数。

举一个具体例子：假设主分片数为 5，文档 ID 为 `"user_001"`，`hash("user_001")` 返回 `12345678`：

```
shard_num = 12345678 % 5 = 3
```

这条文档被路由到分片 3。无论写入还是查询，只要 `_id` 是 `"user_001"`，公式都会指向分片 3——这保证了**写入和读取的一致性**，不需要维护任何文档到分片的映射表。

### 如果主分片数改变，会发生什么

假设上面的例子中，我们把主分片数从 5 改为 10，同一条文档：

```
shard_num = 12345678 % 10 = 8
```

路由结果从分片 3 变成了分片 8。**但这条文档的数据还在分片 3 上**，没有人通知它搬家。此后，查询这条文档会去分片 8 找，永远找不到；写入同一 `_id` 的更新操作也会错误地写入分片 8，产生重复数据。

这不是某一条文档的问题，而是索引中**所有已有文档**的路由结果全部失效。整个索引的数据分布和路由计算从此脱节，索引将完全不可用。

这就是为什么 ES 在设计上直接封锁了对 `number_of_primary_shards` 的修改——一旦允许修改，后果是灾难性的，没有任何自动修复的可能。

### 与分布式系统中其他取模方案的对比

熟悉分布式系统的工程师可能会联想到**一致性哈希**。一致性哈希的优势在于，当节点数量增减时，只有 `1/n` 的数据需要迁移，而简单取模哈希在节点数变化时几乎所有数据都需要重新分布。

ES 选择了简单取模而非一致性哈希，原因是：

- **路由计算极为简单**，不需要维护哈希环结构，无额外内存开销。
- **ES 的扩容方式不是"增加分片"，而是"增加节点 + 副本"**。主分片数固定，但可以把这些分片重新分布到新加入的节点上（Shard Rebalancing），这个过程不改变路由算法，数据不需要重新分布。
- **真正需要改变分片数时**，ES 提供了显式的 Reindex、Split、Shrink 操作，而不是悄悄改变路由规则。

MySQL 分库分表同样面临这个问题：取模分表后，一旦表数量改变，几乎所有数据都要迁移，因此分表数量也需要预先规划好（通常选择 2 的幂次，便于后续翻倍扩容）。Kafka 的 partition 数也是类似的约束——修改 partition 数后，已有消息不会重新分布，只有新消息按新规则路由，这会破坏按 key 分区的语义。

### 副本分片为什么可以动态修改

理解了路由算法，副本可以动态调整就很好解释了：副本分片**不参与路由计算**。

路由公式只用到 `number_of_primary_shards`，副本数不在公式里。增加或删除副本，只是给主分片创建或销毁拷贝，不改变任何文档的"家"在哪里。增加副本的代价是存储翻倍和同步开销，但对数据定位没有任何影响。

```json
// 动态修改副本数，对路由完全透明
PUT /my_index/_settings
{
  "index.number_of_replicas": 2
}
```

---

## 主分片数量规划方法论

既然主分片数一旦确定就无法修改，规划就必须在创建索引之前做好。以下是一套可落地的方法论。

### 单个分片的合理大小

ES 官方给出的经验值是：**单个分片的大小控制在 10GB 到 50GB 之间**。

| 场景 | 推荐单分片大小 | 说明 |
|------|--------------|------|
| 搜索场景（商品、文档） | 10GB ～ 50GB | 查询延迟敏感，分片不宜过大 |
| 日志场景（ELK） | 20GB ～ 80GB | 写入量大，对查询延迟要求宽松 |
| 归档/冷数据 | 50GB ～ 100GB | 很少查询，优先节省资源 |

**分片过小的代价**：

- 每个分片是独立的 Lucene 实例，有固定的元数据开销（内存中的 Segment 元数据、索引元数据）。分片数量太多，集群的元数据总量膨胀，Master 节点压力增大。
- 每次搜索都要广播到所有分片，分片太多意味着协调节点要汇总的结果集更大，网络和 CPU 开销更高。
- ES 官方给出了一个经验比例：每 GB 堆内存不超过 20 个分片（包括主分片和副本）。

**分片过大的代价**：

- 分片恢复（节点重启或故障转移）需要搬运整个分片的数据，100GB 的分片在恢复时会长时间占用 I/O 和网络资源。
- 单个查询需要在一个更大的 Segment 集合上执行，延迟更高。
- Segment Merge 的代价更大，后台 I/O 压力更高。

### 估算公式

```
主分片数 = ceil(预期总数据量 / 单分片目标大小) × 增长系数
```

- **预期总数据量**：根据数据保留策略估算，而不是当前数据量。若数据保留 6 个月，估算 6 个月后的量。
- **单分片目标大小**：搜索场景取 30GB，日志场景取 50GB。
- **增长系数**：建议 1.2 ～ 1.5，预留 20%～50% 的增长空间。

例如，一个电商商品索引，预计 3 年内商品总量在 2000 万到 3000 万之间，每条文档平均 2KB，总数据量约 60GB，目标单分片 30GB，增长系数 1.3：

```
主分片数 = ceil(60 / 30) × 1.3 = 2 × 1.3 ≈ 3（取整数，选 3 或 5）
```

这里建议选 **5** 而不是 3，原因在下一小节解释。

### 与节点数的配合

分片数和节点数的配合决定了数据分布是否均匀。最理想的状态是：**每个节点上的分片数相等**。

规则：分片总数（主分片 + 副本分片）应该能被节点数整除。

```
分片总数 = 主分片数 × (1 + 副本数)
分片总数 % 数据节点数 = 0   ← 理想状态
```

假设有 3 个数据节点，副本数为 1：

- 主分片数 = 3 → 总分片 = 6 → 6 / 3 = 2，每节点 2 个分片，均匀。
- 主分片数 = 5 → 总分片 = 10 → 10 / 3 = 3.33，不能整除，有 1 个节点多 1 个分片，略不均匀。
- 主分片数 = 6 → 总分片 = 12 → 12 / 3 = 4，每节点 4 个分片，均匀。

这就是为什么上面的例子选 5 而不是 3——如果集群有 5 个节点，5 个主分片可以做到每节点 1 个主分片，充分利用集群并行能力。

**单节点分片数量上限**的经验公式：

```
单节点最大分片数 = 节点堆内存(GB) × 20
```

一个 32GB 堆内存的节点，最多承载约 640 个分片（包含主分片和副本）。超过这个上限，JVM GC 压力和元数据管理开销会显著增加。

### 不同场景的分片策略

**小型索引（数据量 < 5GB）**：

直接使用 1 个主分片。分片过多对小索引毫无收益，只增加开销。

**时序日志索引（ELK 日志管道）**：

不要在单个大索引上堆积数据。使用 **ILM（索引生命周期管理）** 配合 Rollover，每天或每 50GB 自动滚动生成新索引。每个滚动生成的索引使用 1 ～ 2 个主分片，通过别名对外暴露统一的访问入口。

这是最重要的一条建议：**时序数据永远用滚动索引，而不是一个大索引加大量分片**。

```
logs-2026.02.01（1 主分片）→ logs-2026.02.02（1 主分片）→ ...
对外只暴露 logs-write（写别名）和 logs-read（读别名）
```

**大型搜索索引（数据量 50GB ～ 1TB）**：

按估算公式计算主分片数，同时确保分片总数与节点数的配合。副本数根据读写比例和可用性要求设置，通常 1 ～ 2 个。

---

## 分片规划失误后的补救方案

即使规划时做了充分估算，现实中仍然会遇到需要调整分片数的场景。ES 提供了三种官方方案，各有适用场景和限制。

### 方案一：Reindex API

最通用的方案。新建一个主分片数正确的目标索引，将原索引的数据全量迁移过去，再切换别名。

```
原索引（5 主分片）→ 新建索引（20 主分片）→ Reindex 迁移数据 → 切换别名
```

**操作要点**：

1. 新建目标索引，配置好正确的 `mappings` 和 `settings`。
2. 执行 Reindex，建议加 `"conflicts": "proceed"` 避免版本冲突中断任务。
3. 如果原索引仍在写入，Reindex 完成后需要再次同步增量数据（可用 `version_type: external` 或时间戳过滤）。
4. 切换别名：原子地将读写别名从原索引切换到新索引。
5. 验证完成后删除原索引。

```json
POST /_reindex
{
  "source": { "index": "my_index_old" },
  "dest":   { "index": "my_index_new" }
}
```

Reindex 的主要代价是时间和 I/O——对于 TB 级索引，迁移可能需要数小时。过程中通常需要降低写入 `refresh_interval` 并暂时把目标索引副本设为 0 以提升迁移速度。

### 方案二：Split Index API

将原索引的主分片数翻倍（或翻为整数倍）。**只能扩，不能缩**，且目标分片数必须是原分片数的整数倍。

前提条件：原索引必须设置为只读（`blocks.write: true`），且每个主分片只有一个副本分布在特定节点上。

```json
// 先设为只读
PUT /my_index/_settings
{ "index.blocks.write": true }

// 从 5 分片扩展到 10 分片（只能是整数倍）
POST /my_index/_split/my_index_new
{
  "settings": {
    "index.number_of_shards": 10
  }
}
```

Split 的机制是：在原分片节点上直接硬链接文件，速度比 Reindex 快得多，几乎不消耗额外磁盘 I/O。但只读期间无法写入数据，且只能倍增，无法任意调整。

### 方案三：Shrink Index API

将分片数缩减到原来的约数（必须能整除）。**只能缩，不能扩**，且前提是所有主分片必须先迁移到同一个节点上。

```json
// 先把所有主分片路由到同一个节点
PUT /my_index/_settings
{
  "index.routing.allocation.require._name": "node-1",
  "index.blocks.write": true
}

// 从 10 分片缩减到 2 分片
POST /my_index/_shrink/my_index_shrunken
{
  "settings": {
    "index.number_of_shards": 2
  }
}
```

Shrink 的使用场景相对有限，通常用于历史冷数据的整合降本，而不是生产索引的日常调整。

### 三种方案对比

| 方案 | 方向 | 速度 | 写入中断 | 适用场景 |
|------|------|------|---------|---------|
| Reindex | 任意调整 | 慢（全量迁移） | 需要管理增量 | 最通用，分片数需要大幅调整 |
| Split | 只能扩（整数倍） | 快（硬链接） | 只读期间 | 分片数需要翻倍，数据量不大 |
| Shrink | 只能缩（整数因子） | 快 | 迁移 + 只读期间 | 合并历史冷索引 |

---

## ILM 与分片规划的结合

对于时序数据，正确的分片规划方式不是在单个索引上设置很多分片，而是结合 **ILM（Index Lifecycle Management）** 做滚动索引。

### 滚动索引的核心逻辑

```
写入别名（logs-write） → 始终指向当前活跃索引
读取别名（logs-read）  → 覆盖所有历史索引

logs-000001（已满，进入 Warm 阶段）
logs-000002（已满，进入 Warm 阶段）
logs-000003（当前活跃，接受写入）  ← logs-write 指向这里
```

每个滚动生成的索引可以单独设置较小的主分片数（通常 1 ～ 2 个），整体查询并发通过大量历史索引并行搜索来提升，而不是依赖单索引的大分片数。

### Hot-Warm-Cold 架构中的分片考量

ILM 的 Hot-Warm-Cold 分层对分片规划有直接影响：

```
Hot 阶段（SSD 节点）：
  └── 活跃写入索引，1-2 主分片，1 副本

Warm 阶段（HDD 节点）：
  └── 只读历史索引，可执行 Shrink 合并为 1 主分片，降低元数据开销
  └── Forcemerge 为 1 Segment，最大化查询效率

Cold 阶段（对象存储 / Searchable Snapshots）：
  └── 极少访问，Searchable Snapshot 挂载，几乎不占用本地磁盘
```

Warm 阶段的 Shrink 操作是 ILM 中最有价值的优化之一：把大量历史日志索引从 2 个主分片合并为 1 个，集群元数据减半，Master 节点压力大幅下降。

---

## 生产规划检查清单

在创建索引之前，逐项确认以下问题：

**数据量估算**

- [ ] 明确数据保留周期（3 个月 / 6 个月 / 1 年）
- [ ] 估算保留周期内的总数据量（基于当前增长速率 × 周期 × 1.3 增长系数）
- [ ] 确定单分片目标大小（搜索场景 ≤ 50GB，日志场景 ≤ 80GB）

**索引类型判断**

- [ ] 如果是时序数据 → 优先使用滚动索引 + ILM，而非大分片数索引
- [ ] 如果是静态搜索索引 → 按估算公式计算主分片数

**分片数与节点数配合**

- [ ] 确认 `(主分片数 × (1 + 副本数)) % 数据节点数 = 0` 或接近整除
- [ ] 确认单节点分片数 ≤ 节点堆内存(GB) × 20

**别名规划**

- [ ] 写入通过别名进行，不直接对索引名写入
- [ ] 读取通过别名进行，便于后续 Reindex 无缝切换

**常见错误**

- 把分片数等于节点数当成"最优"：分片数应由数据量决定，而不是节点数
- 对小索引（< 5GB）设置过多分片：1 个就够了
- 不使用别名直接写入索引：一旦需要 Reindex，写入切换会有中断风险
- 时序日志使用单一大索引：应该用滚动索引

---

## 小结

- **主分片数不可变的根本原因**是路由算法：`shard_num = hash(_routing) % number_of_primary_shards`。改变主分片数会使所有已有文档的路由结果失效，导致数据无法定位。
- **副本分片可以动态修改**，因为它不参与路由计算，只是主分片的完整拷贝。
- **分片规划的核心思路**：根据预期数据量除以单分片目标大小得到主分片数，同时确保分片总数与节点数配合均匀，并预留 20%～30% 的增长空间。
- **时序数据永远用滚动索引**，而不是一个大索引配大量分片，结合 ILM 的 Hot-Warm-Cold 分层可以进一步在查询性能和存储成本之间取得平衡。
- **规划失误的补救**：Reindex 最通用（任意调整，但耗时），Split 扩分片快（只能整数倍增），Shrink 缩分片（需迁移到单节点，适合冷数据整合）。

---

## 常见问题

### Q1：创建索引时如果没有指定主分片数，默认是多少？会有问题吗？

ES 7.0 之前默认主分片数为 5，7.0 之后改为 **1**。这个改变是有意为之：官方认为大多数中小索引根本不需要多个分片，默认 5 个分片对小索引是浪费，且产生大量不必要的元数据开销。

对于生产环境，建议始终显式设置主分片数，而不依赖默认值。原因是：你对自己数据量的了解比 ES 的默认值更准确，任何"我以为默认是合理的"都可能是后期踩坑的来源。

### Q2：自定义 `_routing` 时，分片数规划需要注意什么？

自定义 `_routing`（如按租户 ID 路由）会打破默认的均匀分布。如果某些 routing key 对应的数据量远大于其他 key，可能导致某些分片远比其他分片大——这被称为**数据倾斜（Data Skew）**。

规划建议：
1. 分析 routing key 的基数和数据分布，确保 key 的数量远大于分片数。
2. 如果 key 的基数较少（如只有几十个租户），考虑使用 `routing_partition_size` 让每个 routing key 对应多个分片，避免数据全部集中在一个分片上。
3. 定期通过 `_cat/shards` 监控各分片大小，及时发现倾斜问题。

### Q3：Reindex 过程中如果原索引仍在持续写入，如何保证数据不丢失？

Reindex 本身只是一次全量快照迁移，不能实时同步增量数据。处理持续写入的标准方案是：

1. 记录 Reindex 开始的时间戳 `T_start`。
2. 执行全量 Reindex。
3. Reindex 完成后，补跑增量：通过时间范围过滤（`range query on @timestamp`），将 `T_start` 之后写入的文档再次 Reindex 到新索引。
4. 在低峰期，执行别名原子切换，同时将写入流量切到新索引。
5. 切换后新增量的数据差距通常只有秒级，可以接受。

如果业务对停机窗口零容忍，也可以在写入层面实现**双写**：应用程序同时向新旧索引写入，等数据完全一致后再切换读流量，最后关闭对旧索引的写入。

### Q4：为什么说分片数等于节点数不是最优方案？

这是一个非常普遍的误解。"分片数等于节点数"能保证每个节点有且仅有一个主分片，看起来分布均匀，但实际上忽略了数据量：

- 如果数据量只有 10GB，3 个节点 3 个分片，每个分片 3.3GB，单分片太小，元数据开销比数据本身还大。
- 如果数据量是 300GB，3 个节点 3 个分片，每个分片 100GB，超过了推荐上限，查询和恢复都会变慢。

分片数应由**数据量**决定，节点数决定的是**分片如何分布**。正确的顺序是：先根据数据量算出需要多少分片，再根据节点数调整到一个能整除的值，而不是反过来。

### Q5：`_forcemerge` 对分片规划有什么影响，什么时候应该执行？

`_forcemerge` 将分片内的多个 Segment 合并为更少的 Segment（通常是 1 个），它不改变分片数，只影响分片内部的存储结构。合并为 1 个 Segment 后，搜索时不再需要遍历多个 Segment 文件，查询性能最优。

**应该执行的场景**：

- 确定不再有新数据写入的历史索引（如按天滚动的日志索引，昨天的索引今天肯定不会再写入）。
- 在 ILM 的 Warm 或 Cold 阶段自动触发，对只读历史数据效果最好。

**不应该执行的场景**：

- 仍在持续写入的活跃索引。`_forcemerge` 产生的大 Segment 会立刻被新写入的小 Segment 打破，Merge 工作白做，反而消耗了大量 I/O。
- 集群负载高峰期。Forcemerge 是重 I/O 操作，应在业务低峰期执行，并设置低优先级或限速。

## 参考资源

- [Elasticsearch 分片规划指南](https://www.elastic.co/guide/en/elasticsearch/reference/current/size-your-shards.html)
- [索引设置官方文档](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html)
- [Split 和 Shrink API](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-split-index.html)
