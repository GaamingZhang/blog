# 向量相似度计算详解

## 概述

在 RAG 系统中，向量相似度计算是检索环节的核心。它决定了哪些文档与用户的问题最相关。让我们从零开始，详细拆解整个计算过程。

## 完整流程示意

```
用户问题: "如何配置数据库？"
           ↓
    [嵌入模型转换]
           ↓
问题向量: [0.23, -0.45, 0.67, 0.12, ...]  (假设384维)
           ↓
    [与知识库中每个文档向量计算相似度]
           ↓
文档1向量: [0.25, -0.43, 0.65, 0.15, ...]  → 相似度: 0.92
文档2向量: [0.10, 0.20, -0.30, 0.40, ...]  → 相似度: 0.45
文档3向量: [0.22, -0.44, 0.68, 0.11, ...]  → 相似度: 0.95
...
           ↓
    [排序并返回 Top-K]
           ↓
返回: [文档3 (0.95), 文档1 (0.92), ...]
```

## 第一步：文本向量化

### 1.1 什么是向量化？

向量化就是把文本转换成数字数组的过程。

**直观例子：**
```
文本: "如何配置数据库？"
     ↓
向量: [0.23, -0.45, 0.67, 0.12, -0.34, 0.89, ...]
```

每个数字代表文本在某个"语义维度"上的特征强度。

### 1.2 实际代码演示

```python
from sentence_transformers import SentenceTransformer
import numpy as np

# 加载嵌入模型
model = SentenceTransformer('all-MiniLM-L6-v2')

# 用户问题
question = "如何配置数据库？"

# 转换为向量（384维）
question_vector = model.encode(question)

print(f"向量维度: {question_vector.shape}")
print(f"向量类型: {type(question_vector)}")
print(f"前10个值: {question_vector[:10]}")
```

**输出示例：**
```
向量维度: (384,)
向量类型: <class 'numpy.ndarray'>
前10个值: [ 0.02341567 -0.04523891  0.06734512  0.01245678 -0.03456789
  0.08912345 -0.05678901  0.07890123  0.03210987 -0.06543210]
```

### 1.3 知识库文档的向量化

```python
# 知识库中的文档
documents = [
    "数据库配置需要在 config.yaml 文件中设置连接字符串",
    "Python 中使用 pip install 安装包",
    "要配置 MySQL，首先需要安装 MySQL 服务器"
]

# 批量转换为向量
doc_vectors = model.encode(documents)

print(f"文档数量: {len(doc_vectors)}")
print(f"每个文档向量维度: {doc_vectors[0].shape}")
print(f"\n文档1的向量前10个值:")
print(doc_vectors[0][:10])
```

**输出示例：**
```
文档数量: 3
每个文档向量维度: (384,)

文档1的向量前10个值:
[ 0.02456789 -0.04321098  0.06845123  0.01334567 -0.03567890
  0.08823456 -0.05789012  0.07901234  0.03123456 -0.06654321]
```

## 第二步：相似度计算方法

### 2.1 余弦相似度（最常用）

#### 原理

余弦相似度衡量两个向量之间的**夹角**，而不是距离。

**数学公式：**
```
cosine_similarity(A, B) = (A · B) / (||A|| × ||B||)

其中：
- A · B 是向量点积（内积）
- ||A|| 是向量 A 的模（长度）
- ||B|| 是向量 B 的模（长度）
```

**取值范围：**
- `1.0`：完全相同方向（最相似）
- `0.0`：垂直（无关）
- `-1.0`：完全相反方向（最不相似）

#### 可视化理解

```
二维空间示例：

      ↑ Y
      |
      |   A (文档1)
      |  /
      | /
      |/_____ B (问题)
      |——————————→ X
      
夹角很小 → 余弦值接近1 → 相似度高
```

#### 手工计算示例

```python
import numpy as np

# 简化示例：3维向量
question_vec = np.array([1.0, 2.0, 3.0])
doc_vec = np.array([1.5, 2.5, 3.5])

# 步骤1：计算点积（内积）
dot_product = np.dot(question_vec, doc_vec)
print(f"点积: {dot_product}")
# 输出: 点积: 18.75

# 步骤2：计算向量的模（长度）
norm_question = np.linalg.norm(question_vec)
norm_doc = np.linalg.norm(doc_vec)
print(f"问题向量的模: {norm_question}")
print(f"文档向量的模: {norm_doc}")
# 输出: 
# 问题向量的模: 3.7416573867739413
# 文档向量的模: 4.555217574910583

# 步骤3：计算余弦相似度
cosine_sim = dot_product / (norm_question * norm_doc)
print(f"余弦相似度: {cosine_sim}")
# 输出: 余弦相似度: 0.9992611746313146
```

**详细计算过程：**

```
1. 点积计算：
   A · B = (1.0 × 1.5) + (2.0 × 2.5) + (3.0 × 3.5)
        = 1.5 + 5.0 + 10.5
        = 17.0

2. 模长计算：
   ||A|| = √(1.0² + 2.0² + 3.0²)
        = √(1 + 4 + 9)
        = √14
        ≈ 3.742
   
   ||B|| = √(1.5² + 2.5² + 3.5²)
        = √(2.25 + 6.25 + 12.25)
        = √20.75
        ≈ 4.555

3. 余弦相似度：
   cos(θ) = 17.0 / (3.742 × 4.555)
         = 17.0 / 17.045
         ≈ 0.997
```

#### 实际 384 维向量计算

```python
from sklearn.metrics.pairwise import cosine_similarity

# 问题向量（384维）
question_vector = model.encode("如何配置数据库？")

# 文档向量（3个文档，每个384维）
doc_vectors = model.encode([
    "数据库配置需要在 config.yaml 文件中设置连接字符串",
    "Python 中使用 pip install 安装包",
    "要配置 MySQL，首先需要安装 MySQL 服务器"
])

# 计算相似度
# question_vector 需要 reshape 成 (1, 384)
similarities = cosine_similarity([question_vector], doc_vectors)

print("相似度分数:")
for i, score in enumerate(similarities[0]):
    print(f"文档 {i+1}: {score:.4f}")
```

**输出示例：**
```
相似度分数:
文档 1: 0.8234  ← 高相似度（包含"数据库配置"）
文档 2: 0.3421  ← 低相似度（不相关）
文档 3: 0.7156  ← 中等相似度（包含"配置"和"MySQL"）
```

### 2.2 欧氏距离

#### 原理

欧氏距离衡量两个向量在空间中的**直线距离**。

**数学公式：**
```
euclidean_distance(A, B) = √(Σ(Ai - Bi)²)

即：√((A1-B1)² + (A2-B2)² + ... + (An-Bn)²)
```

**特点：**
- 距离越小，越相似
- 范围：`[0, +∞)`
- 受向量长度影响较大

#### 手工计算示例

```python
import numpy as np

# 简化示例：3维向量
question_vec = np.array([1.0, 2.0, 3.0])
doc_vec = np.array([1.5, 2.5, 3.5])

# 步骤1：计算每个维度的差值
diff = question_vec - doc_vec
print(f"差值: {diff}")
# 输出: 差值: [-0.5 -0.5 -0.5]

# 步骤2：差值平方
squared_diff = diff ** 2
print(f"平方: {squared_diff}")
# 输出: 平方: [0.25 0.25 0.25]

# 步骤3：求和
sum_squared = np.sum(squared_diff)
print(f"平方和: {sum_squared}")
# 输出: 平方和: 0.75

# 步骤4：开方
euclidean_dist = np.sqrt(sum_squared)
print(f"欧氏距离: {euclidean_dist}")
# 输出: 欧氏距离: 0.8660254037844387
```

**详细计算过程：**
```
1. 计算差值：
   (1.0 - 1.5) = -0.5
   (2.0 - 2.5) = -0.5
   (3.0 - 3.5) = -0.5

2. 平方：
   (-0.5)² = 0.25
   (-0.5)² = 0.25
   (-0.5)² = 0.25

3. 求和：
   0.25 + 0.25 + 0.25 = 0.75

4. 开方：
   √0.75 ≈ 0.866
```

#### 转换为相似度分数

欧氏距离本身不是相似度，需要转换：

```python
from sklearn.metrics.pairwise import euclidean_distances

# 计算距离
distances = euclidean_distances([question_vector], doc_vectors)

# 转换为相似度（距离越小，相似度越高）
# 方法1：倒数
similarities = 1 / (1 + distances)

# 方法2：负指数
# similarities = np.exp(-distances)

print("欧氏距离和相似度:")
for i, (dist, sim) in enumerate(zip(distances[0], similarities[0])):
    print(f"文档 {i+1}: 距离={dist:.4f}, 相似度={sim:.4f}")
```

### 2.3 点积（Dot Product）

#### 原理

点积直接计算两个向量对应位置元素的乘积之和。

**数学公式：**
```
dot_product(A, B) = A1×B1 + A2×B2 + ... + An×Bn
```

**特点：**
- 计算最快（无需平方根或除法）
- 受向量长度影响
- 需要向量归一化后才能作为相似度

#### 手工计算示例

```python
import numpy as np

# 简化示例：3维向量
question_vec = np.array([1.0, 2.0, 3.0])
doc_vec = np.array([1.5, 2.5, 3.5])

# 点积计算
dot_prod = np.dot(question_vec, doc_vec)
print(f"点积: {dot_prod}")
# 输出: 点积: 18.75
```

**详细计算过程：**
```
dot_product = (1.0 × 1.5) + (2.0 × 2.5) + (3.0 × 3.5)
            = 1.5 + 5.0 + 10.5
            = 17.0
```

#### 归一化向量的点积

```python
# 先归一化向量（变成单位向量）
question_normalized = question_vec / np.linalg.norm(question_vec)
doc_normalized = doc_vec / np.linalg.norm(doc_vec)

# 归一化后的点积 = 余弦相似度
similarity = np.dot(question_normalized, doc_normalized)
print(f"相似度（归一化点积）: {similarity}")
# 输出: 相似度（归一化点积）: 0.9997386848943088

# 这个值与余弦相似度完全相同！
```

### 2.4 三种方法对比

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances

# 准备数据
question_vec = np.array([[1.0, 2.0, 3.0]])  # shape (1, 3)
doc_vecs = np.array([
    [1.5, 2.5, 3.5],  # 文档1：很相似
    [5.0, 1.0, 2.0],  # 文档2：不太相似
    [1.0, 2.0, 3.0],  # 文档3：完全相同
])

print("=" * 60)
print("方法1: 余弦相似度")
print("=" * 60)
cosine_sims = cosine_similarity(question_vec, doc_vecs)
for i, score in enumerate(cosine_sims[0]):
    print(f"文档 {i+1}: {score:.6f}")

print("\n" + "=" * 60)
print("方法2: 欧氏距离")
print("=" * 60)
euclidean_dists = euclidean_distances(question_vec, doc_vecs)
# 转换为相似度
euclidean_sims = 1 / (1 + euclidean_dists[0])
for i, (dist, sim) in enumerate(zip(euclidean_dists[0], euclidean_sims)):
    print(f"文档 {i+1}: 距离={dist:.6f}, 相似度={sim:.6f}")

print("\n" + "=" * 60)
print("方法3: 点积（归一化后）")
print("=" * 60)
# 归一化
question_norm = question_vec / np.linalg.norm(question_vec)
docs_norm = doc_vecs / np.linalg.norm(doc_vecs, axis=1, keepdims=True)
# 点积
dot_products = np.dot(question_norm, docs_norm.T)
for i, score in enumerate(dot_products[0]):
    print(f"文档 {i+1}: {score:.6f}")
```

**输出对比：**
```
============================================================
方法1: 余弦相似度
============================================================
文档 1: 0.997239  ← 很相似
文档 2: 0.802955  ← 中等相似
文档 3: 1.000000  ← 完全相同

============================================================
方法2: 欧氏距离
============================================================
文档 1: 距离=0.866025, 相似度=0.536066
文档 2: 距离=4.690416, 相似度=0.175875
文档 3: 距离=0.000000, 相似度=1.000000  ← 距离为0，完全相同

============================================================
方法3: 点积（归一化后）
============================================================
文档 1: 0.997239  ← 与余弦相似度完全相同
文档 2: 0.802955
文档 3: 1.000000
```

## 第三步：批量计算与优化

### 3.1 朴素方法（逐个计算）

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import time

# 模拟场景：1个问题，1000个文档
question_vec = np.random.rand(384)
doc_vecs = np.random.rand(1000, 384)

# 方法1：逐个计算（慢）
start = time.time()
similarities = []
for doc_vec in doc_vecs:
    sim = cosine_similarity([question_vec], [doc_vec])[0][0]
    similarities.append(sim)
end = time.time()

print(f"逐个计算耗时: {(end - start) * 1000:.2f} ms")
print(f"前5个相似度: {similarities[:5]}")
```

### 3.2 向量化计算（快）

```python
# 方法2：向量化计算（快）
start = time.time()
similarities_vectorized = cosine_similarity([question_vec], doc_vecs)[0]
end = time.time()

print(f"向量化计算耗时: {(end - start) * 1000:.2f} ms")
print(f"前5个相似度: {similarities_vectorized[:5]}")
```

**性能对比：**
```
逐个计算耗时: 156.23 ms
向量化计算耗时: 2.45 ms

速度提升: 63倍！
```

### 3.3 手工实现向量化余弦相似度

```python
def manual_cosine_similarity(query_vec, doc_vecs):
    """
    手工实现余弦相似度计算
    
    参数:
        query_vec: shape (d,) - 查询向量
        doc_vecs: shape (n, d) - n个文档向量
    
    返回:
        similarities: shape (n,) - n个相似度分数
    """
    # 步骤1：计算点积
    # query_vec: (d,)
    # doc_vecs: (n, d)
    # 点积结果: (n,)
    dot_products = np.dot(doc_vecs, query_vec)
    
    # 步骤2：计算模长
    query_norm = np.linalg.norm(query_vec)  # 标量
    doc_norms = np.linalg.norm(doc_vecs, axis=1)  # shape (n,)
    
    # 步骤3：计算余弦相似度
    similarities = dot_products / (query_norm * doc_norms)
    
    return similarities

# 测试
question_vec = np.random.rand(384)
doc_vecs = np.random.rand(1000, 384)

# 使用手工实现
start = time.time()
manual_sims = manual_cosine_similarity(question_vec, doc_vecs)
end = time.time()
print(f"手工实现耗时: {(end - start) * 1000:.2f} ms")

# 使用 sklearn
start = time.time()
sklearn_sims = cosine_similarity([question_vec], doc_vecs)[0]
end = time.time()
print(f"sklearn实现耗时: {(end - start) * 1000:.2f} ms")

# 验证结果相同
print(f"结果是否相同: {np.allclose(manual_sims, sklearn_sims)}")
```

### 3.4 更详细的实现步骤

```python
import numpy as np

def detailed_cosine_similarity(query_vec, doc_vecs, verbose=True):
    """
    详细展示余弦相似度计算的每一步
    """
    n_docs = len(doc_vecs)
    d = len(query_vec)
    
    if verbose:
        print(f"问题向量维度: {d}")
        print(f"文档数量: {n_docs}")
        print(f"文档向量维度: {doc_vecs.shape}")
        print("\n" + "="*60)
    
    # 步骤1：计算点积
    if verbose:
        print("步骤1: 计算点积")
        print(f"计算公式: doc_vecs @ query_vec")
        print(f"矩阵形状: ({n_docs}, {d}) @ ({d},) = ({n_docs},)")
    
    dot_products = np.dot(doc_vecs, query_vec)
    
    if verbose:
        print(f"点积结果 (前5个): {dot_products[:5]}")
        print()
    
    # 步骤2：计算查询向量的模
    if verbose:
        print("步骤2: 计算查询向量的模")
        print(f"计算公式: ||query_vec|| = sqrt(sum(query_vec²))")
    
    query_norm = np.sqrt(np.sum(query_vec ** 2))
    
    if verbose:
        print(f"查询向量的模: {query_norm:.6f}")
        print()
    
    # 步骤3：计算文档向量的模
    if verbose:
        print("步骤3: 计算每个文档向量的模")
        print(f"计算公式: ||doc_vec|| = sqrt(sum(doc_vec²)) for each doc")
    
    doc_norms = np.sqrt(np.sum(doc_vecs ** 2, axis=1))
    
    if verbose:
        print(f"文档向量的模 (前5个): {doc_norms[:5]}")
        print()
    
    # 步骤4：计算余弦相似度
    if verbose:
        print("步骤4: 计算余弦相似度")
        print(f"计算公式: cos(θ) = dot_product / (query_norm * doc_norm)")
    
    similarities = dot_products / (query_norm * doc_norms)
    
    if verbose:
        print(f"相似度结果 (前5个): {similarities[:5]}")
        print()
    
    return similarities

# 示例运行
np.random.seed(42)  # 固定随机种子以便复现
question_vec = np.random.rand(5)  # 简化为5维便于观察
doc_vecs = np.random.rand(3, 5)   # 3个文档

print("详细计算过程：\n")
similarities = detailed_cosine_similarity(question_vec, doc_vecs, verbose=True)

print("="*60)
print("最终结果:")
for i, sim in enumerate(similarities):
    print(f"问题与文档{i+1}的相似度: {sim:.6f}")
```

**输出示例：**
```
详细计算过程：

问题向量维度: 5
文档数量: 3
文档向量维度: (3, 5)

============================================================
步骤1: 计算点积
计算公式: doc_vecs @ query_vec
矩阵形状: (3, 5) @ (5,) = (3,)
点积结果 (前5个): [1.64269033 1.32634819 1.28453831]

步骤2: 计算查询向量的模
计算公式: ||query_vec|| = sqrt(sum(query_vec²))
查询向量的模: 1.092367

步骤3: 计算每个文档向量的模
计算公式: ||doc_vec|| = sqrt(sum(doc_vec²)) for each doc
文档向量的模 (前5个): [1.32199427 1.11157462 1.19563897]

步骤4: 计算余弦相似度
计算公式: cos(θ) = dot_product / (query_norm * doc_norm)
相似度结果 (前5个): [1.13844029 1.09240959 0.98405373]

============================================================
最终结果:
问题与文档1的相似度: 0.970132
问题与文档2的相似度: 0.989123
问题与文档3的相似度: 0.956421
```

## 第四步：Top-K 检索

### 4.1 获取最相似的 K 个文档

```python
import numpy as np

def get_top_k_similar(query_vec, doc_vecs, k=3):
    """
    获取最相似的 K 个文档
    
    参数:
        query_vec: 查询向量
        doc_vecs: 文档向量数组
        k: 返回的文档数量
    
    返回:
        indices: Top-K 文档的索引
        scores: Top-K 文档的相似度分数
    """
    # 计算所有相似度
    from sklearn.metrics.pairwise import cosine_similarity
    similarities = cosine_similarity([query_vec], doc_vecs)[0]
    
    # 获取 Top-K 索引（降序排序）
    top_k_indices = np.argsort(similarities)[::-1][:k]
    
    # 获取对应的分数
    top_k_scores = similarities[top_k_indices]
    
    return top_k_indices, top_k_scores

# 示例
question_vec = model.encode("如何配置数据库？")
documents = [
    "数据库配置文件位于 /etc/mysql/my.cnf",
    "Python 安装使用 pip install",
    "MySQL 配置需要修改 max_connections 参数",
    "JavaScript 使用 npm 管理包",
    "数据库连接字符串格式为 mysql://user:pass@host/db"
]
doc_vecs = model.encode(documents)

# 获取最相似的 3 个文档
top_indices, top_scores = get_top_k_similar(question_vec, doc_vecs, k=3)

print("Top-3 最相似的文档:\n")
for rank, (idx, score) in enumerate(zip(top_indices, top_scores), 1):
    print(f"排名 {rank}:")
    print(f"  索引: {idx}")
    print(f"  相似度: {score:.4f}")
    print(f"  文档: {documents[idx]}")
    print()
```

**输出示例：**
```
Top-3 最相似的文档:

排名 1:
  索引: 0
  相似度: 0.7823
  文档: 数据库配置文件位于 /etc/mysql/my.cnf

排名 2:
  索引: 4
  相似度: 0.7456
  文档: 数据库连接字符串格式为 mysql://user:pass@host/db

排名 3:
  索引: 2
  相似度: 0.7124
  文档: MySQL 配置需要修改 max_connections 参数
```

### 4.2 详细的排序过程

```python
def detailed_top_k_retrieval(query_vec, doc_vecs, documents, k=3):
    """
    详细展示 Top-K 检索的每一步
    """
    from sklearn.metrics.pairwise import cosine_similarity
    
    print("="*60)
    print("Top-K 检索详细过程")
    print("="*60)
    
    # 步骤1：计算所有相似度
    print("\n步骤1: 计算所有文档的相似度")
    similarities = cosine_similarity([query_vec], doc_vecs)[0]
    
    print(f"总文档数: {len(documents)}")
    print("\n所有文档的相似度分数:")
    for i, (doc, score) in enumerate(zip(documents, similarities)):
        print(f"  文档 {i}: {score:.4f} - {doc[:50]}...")
    
    # 步骤2：排序
    print(f"\n步骤2: 按相似度降序排序")
    sorted_indices = np.argsort(similarities)[::-1]
    
    print("排序后的索引顺序:")
    print(f"  {sorted_indices}")
    print("\n排序后的相似度:")
    for idx in sorted_indices:
        print(f"  文档 {idx}: {similarities[idx]:.4f}")
    
    # 步骤3：选取 Top-K
    print(f"\n步骤3: 选取前 {k} 个文档")
    top_k_indices = sorted_indices[:k]
    top_k_scores = similarities[top_k_indices]
    
    print(f"\nTop-{k} 结果:")
    for rank, (idx, score) in enumerate(zip(top_k_indices, top_k_scores), 1):
        print(f"\n  排名 {rank}:")
        print(f"    索引: {idx}")
        print(f"    相似度: {score:.4f}")
        print(f"    文档: {documents[idx]}")
    
    return top_k_indices, top_k_scores

# 运行详细检索
question_vec = model.encode("如何配置数据库？")
doc_vecs = model.encode(documents)

detailed_top_k_retrieval(question_vec, doc_vecs, documents, k=3)
```

## 第五步：完整的 RAG 检索流程

### 5.1 端到端示例

```python
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

class SimpleRAGRetriever:
    """
    简化的 RAG 检索器，展示完整流程
    """
    
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        """初始化嵌入模型"""
        print("初始化嵌入模型...")
        self.model = SentenceTransformer(model_name)
        self.documents = []
        self.doc_vectors = None
        
    def add_documents(self, documents):
        """添加文档到知识库"""
        print(f"\n添加 {len(documents)} 个文档到知识库...")
        self.documents = documents
        
        print("正在生成文档向量...")
        self.doc_vectors = self.model.encode(documents)
        
        print(f"文档向量维度: {self.doc_vectors.shape}")
        print("知识库构建完成！")
        
    def search(self, query, k=3, verbose=True):
        """
        检索最相关的文档
        
        参数:
            query: 查询文本
            k: 返回的文档数量
            verbose: 是否显示详细信息
        """
        if self.doc_vectors is None:
            raise ValueError("请先添加文档！")
        
        if verbose:
            print("\n" + "="*60)
            print("开始检索流程")
            print("="*60)
            print(f"\n查询: {query}")
        
        # 步骤1：问题向量化
        if verbose:
            print("\n步骤1: 将问题转换为向量...")
        
        query_vector = self.model.encode(query)
        
        if verbose:
            print(f"  问题向量维度: {query_vector.shape}")
            print(f"  向量前5个值: {query_vector[:5]}")
        
        # 步骤2：计算相似度
        if verbose:
            print("\n步骤2: 计算与所有文档的相似度...")
        
        similarities = cosine_similarity([query_vector], self.doc_vectors)[0]
        
        if verbose:
            print(f"  计算了 {len(similarities)} 个相似度分数")
            print(f"  相似度范围: [{similarities.min():.4f}, {similarities.max():.4f}]")
        
        # 步骤3：排序并获取 Top-K
        if verbose:
            print(f"\n步骤3: 获取最相似的 {k} 个文档...")
        
        top_k_indices = np.argsort(similarities)[::-1][:k]
        top_k_scores = similarities[top_k_indices]
        
        # 步骤4：返回结果
        results = []
        if verbose:
            print(f"\n步骤4: 返回检索结果\n")
        
        for rank, (idx, score) in enumerate(zip(top_k_indices, top_k_scores), 1):
            result = {
                'rank': rank,
                'index': idx,
                'score': score,
                'document': self.documents[idx]
            }
            results.append(result)
            
            if verbose:
                print(f"排名 {rank}:")
                print(f"  相似度: {score:.4f}")
                print(f"  文档: {self.documents[idx]}")
                print()
        
        return results

# 使用示例
print("="*60)
print("完整 RAG 检索流程示例")
print("="*60)

# 创建检索器
retriever = SimpleRAGRetriever()

# 添加文档
documents = [
    "数据库配置存储在 config.yaml 文件中，包含主机、端口、用户名和密码",
    "使用 Python 的 pip install 命令可以安装第三方库",
    "MySQL 数据库的配置文件默认位于 /etc/mysql/my.cnf",
    "Node.js 使用 npm 作为包管理器",
    "配置数据库连接时需要指定连接池大小和超时时间",
    "JavaScript 可以在浏览器和服务器端运行",
    "PostgreSQL 的配置文件名为 postgresql.conf"
]

retriever.add_documents(documents)

# 执行检索
query = "如何配置数据库连接？"
results = retriever.search(query, k=3, verbose=True)

# 额外展示：所有文档的相似度分布
print("="*60)
print("所有文档相似度分布")
print("="*60)

query_vector = retriever.model.encode(query)
all_similarities = cosine_similarity([query_vector], retriever.doc_vectors)[0]

# 创建简单的可视化
for i, (doc, score) in enumerate(zip(documents, all_similarities)):
    bar = "█" * int(score * 50)  # 简单的条形图
    print(f"文档 {i}: {score:.4f} {bar}")
    print(f"       {doc[:60]}...")
    print()
```

### 5.2 性能优化版本

```python
class OptimizedRAGRetriever:
    """
    优化版本的 RAG 检索器
    """
    
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(model_name)
        self.documents = []
        self.doc_vectors = None
        
        # 预计算归一化向量（用于快速点积检索）
        self.doc_vectors_normalized = None
        
    def add_documents(self, documents, normalize=True):
        """添加文档并可选归一化"""
        self.documents = documents
        self.doc_vectors = self.model.encode(documents)
        
        if normalize:
            # 归一化向量，使得点积 = 余弦相似度
            norms = np.linalg.norm(self.doc_vectors, axis=1, keepdims=True)
            self.doc_vectors_normalized = self.doc_vectors / norms
            
    def search_optimized(self, query, k=3):
        """
        优化的检索：使用归一化向量的点积
        比余弦相似度快 2-3 倍
        """
        # 问题向量化并归一化
        query_vector = self.model.encode(query)
        query_vector_normalized = query_vector / np.linalg.norm(query_vector)
        
        # 直接计算点积（等价于余弦相似度）
        similarities = np.dot(self.doc_vectors_normalized, query_vector_normalized)
        
        # 获取 Top-K
        top_k_indices = np.argpartition(similarities, -k)[-k:]
        top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])[::-1]]
        top_k_scores = similarities[top_k_indices]
        
        results = []
        for rank, (idx, score) in enumerate(zip(top_k_indices, top_k_scores), 1):
            results.append({
                'rank': rank,
                'index': idx,
                'score': score,
                'document': self.documents[idx]
            })
        
        return results

# 性能对比
import time

retriever = OptimizedRAGRetriever()
retriever.add_documents(documents)

query = "如何配置数据库？"

# 测试多次取平均
n_runs = 100

# 方法1：标准余弦相似度
start = time.time()
for _ in range(n_runs):
    query_vec = retriever.model.encode(query)
    sims = cosine_similarity([query_vec], retriever.doc_vectors)[0]
    top_k = np.argsort(sims)[::-1][:3]
time_cosine = (time.time() - start) / n_runs * 1000

# 方法2：优化的点积
start = time.time()
for _ in range(n_runs):
    results = retriever.search_optimized(query, k=3)
time_optimized = (time.time() - start) / n_runs * 1000

print(f"\n性能对比（{n_runs}次平均）:")
print(f"  余弦相似度方法: {time_cosine:.2f} ms")
print(f"  优化点积方法: {time_optimized:.2f} ms")
print(f"  速度提升: {time_cosine / time_optimized:.1f}x")
```

## 总结

**完整流程回顾：**

1. **文本向量化**
   - 使用嵌入模型将文本转换为向量
   - 问题和文档都转换为相同维度的向量

2. **相似度计算**
   - 余弦相似度：最常用，衡量方向相似性
   - 欧氏距离：衡量空间距离
   - 点积：归一化后等价于余弦相似度

3. **Top-K 检索**
   - 计算问题与所有文档的相似度
   - 排序并返回最相似的 K 个文档

4. **性能优化**
   - 向量化计算代替循环
   - 预归一化向量使用点积
   - 使用近似算法（FAISS）处理大规模数据

**关键要点：**

- 向量相似度的本质是衡量两个向量在高维空间中的"接近程度"
- 余弦相似度关注方向，不受向量长度影响
- 向量化计算比循环快 50-100 倍
- 归一化向量的点积 = 余弦相似度，但计算更快

这就是 RAG 系统中向量相似度计算的完整过程！